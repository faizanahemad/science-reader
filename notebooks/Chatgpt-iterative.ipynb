{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c05dbc8",
   "metadata": {},
   "source": [
    "We are a research team that are using gpt-3.5 API to do question generation and question answering. We are also performing complex aspect based document summarization. We intend to publish our work if completed in upcoming EMNLP and then launch it to wider userbase. Current user base is a small alpha test user group of 20 people including our own team of 8. We hit rate limits frequently on token count based rate limit on gpt-3.5-turbo while multiple concurrent users are using our system as well as when we run automated quality tests. We have been asking our users to create separate accounts and upload their own API keys to our service for usage but this is not a scalable and appealing method to our users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e285bb9a",
   "metadata": {},
   "source": [
    "Planning and Re-planning helps improve LLMs. Planning stage can be done by an expensive LLM but the middle execution stages can be handled by cheaper models.\n",
    "\n",
    "- Better ways of asking\n",
    "- Plan and replan\n",
    "- break into smaller part and write programs for smaller parts.\n",
    "- Better memory and retrieval\n",
    "- Critic\n",
    "- Are the tools provided even enough to answer the request. feasibility.\n",
    "- chatgpt UI uses toolformer approach while we can't use that, unless we generate few tokens at a time.\n",
    "- Incorporate examples in some steps\n",
    "- Can it handle generic NLG tasks by searching examples from internet, search examples, or search prompts, and do better ICL.\n",
    "- Tasks it must handle\n",
    "    - Planning\n",
    "    - Question Answering\n",
    "    - Maths\n",
    "    - Multi-Hop QnA\n",
    "    - Document and PDF QnA\n",
    "    - Summarise a website or website QnA\n",
    "    - Large book QnA\n",
    "    - Long form writing - To generate longer sequences we can use sequential multiple agents, with about 1000 word summary of previous writing as input, and another 1000 words as any additional context.\n",
    "    - Multi-modal with Image-to-text and text-to-image\n",
    "    \n",
    "    \n",
    "- Async Agents\n",
    "    - Agents whose results are asynchronously used and if not available within certain time limit, the results are skipped.\n",
    "    \n",
    "Why not Langchain\n",
    "- Langchain makes agents and tools use only text input and output.\n",
    "- Langchain agents don't plan, they go step by step asking what should be my next step. We suggest planning and re-planning and making a plan before starting.\n",
    "- We believe that asking gpt4 to write code for its plan is a better approach than the output parsing and stitching that langchain does, langchain essentially hides the execution but we don't want that.\n",
    "- Langchain's output parsing and stitching is useful for less powerful agents.\n",
    "- we want our execution chain to look like a DAG while langchain's execution chain looks like a linear path. Although a DAG structure can be converted into a linear path as well, but it may optimal and parallizable to have a DAG path."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89ffc47",
   "metadata": {},
   "source": [
    "I want to build two plugins \n",
    "1. for literature survey: Given a search phrase / link to arxiv paper / research description, find relevant papers using web-search, then use a pdf to text engine and read each paper (in context of our current work), and give out a literature review summary with relevant citations. One key aspect is to be able to do BFS on the paper citation graph with iterative deepening, and spawning gpt4 agents to read each paper, find the relevant context and keep it in summary for using in our overall output text.\n",
    "2. Think before you reply plugin: Given a question, this plugin uses a four step approach to provide a better answer. step-1: plan- \"to answer this question or user request what do we need to keep in our context, or how can we improve our context, can we web search (if yes write down the search terms), or devise a set of multiple steps to follow before answering, if yes, enumerate the steps\" step-2: plan following from step-1, step-3: generate multiple answers with reasoning using context developed in step-1 & step-2 with a critic step that criticises each answer and rewrites them to improve each answer separately, and finally step-4: fuse all generated answers, along with the question in context to get a single good answer.\n",
    "3. For Generic paid API access: Give LM ability to use any paid API by a plugin, first LM can ask what available APIs exist and then on basis of available APIs (and their cost) chatgpt can decide which API to use for generating a better response. A key challenge will be using paid APIs and how user's can authenticate to paid API providers.\n",
    "\n",
    "Other Links\n",
    "- https://portal.azure.com/#home\n",
    "\n",
    "Princliple:\n",
    "- We make simple reusable components, any new component we make, we subclass from langchain Basetool class\n",
    "- WE combine them to get higher order functionality.\n",
    "\n",
    "Important LangChain classes and links\n",
    "We can use Prompt Tempates, Chat Memory, Vector Stores, Agents and Tools. \n",
    "\n",
    "- [OpenAI class](https://python.langchain.com/en/latest/reference/modules/llms.html#langchain.llms.OpenAI)\n",
    "- [VectorStoreMemory](https://python.langchain.com/en/latest/modules/memory/types/vectorstore_retriever_memory.html)\n",
    "- [Custom Memory Class](https://python.langchain.com/en/latest/modules/memory/examples/custom_memory.html)\n",
    "- [Making Custom Tools](https://python.langchain.com/en/latest/modules/agents/tools/custom_tools.html)\n",
    "- [Bing Search](https://python.langchain.com/en/latest/modules/agents/tools/examples/bing_search.html) and its tool class [BingSearchResults](https://python.langchain.com/en/latest/_modules/langchain/tools/bing_search/tool.html#BingSearchResults)\n",
    "- https://python.langchain.com/en/latest/reference/modules/tools.html#langchain.tools.DuckDuckGoSearchResults and https://python.langchain.com/en/latest/reference/modules/utilities.html#langchain.utilities.DuckDuckGoSearchAPIWrapper\n",
    "\n",
    "\n",
    "Agents we would need:\n",
    "\n",
    "- Composer: Compose multile modules as a DAG [Chain](https://python.langchain.com/en/latest/modules/chains/getting_started.html)\n",
    "- Contextual reader: Given a long text, and a context, read the page contextually and gather information relevant to the context.\n",
    "- Vector Search agent:\n",
    "- Text search agent:\n",
    "- Pdf reader agent: \n",
    "- Link follower or crawler agent:\n",
    "    - Get page links\n",
    "    - Recursively apply page reader and link getter.\n",
    "- Fuse information: Given context, and information from two or more agents, fuses the information from the sources into one cohesive information.\n",
    "- Keep History Agent: Keeps simple summary of existing conversation, as well as L1, L2, L3 cache style summary, as well as TOC based, text search based, vector search based search over full conversation.\n",
    "\n",
    "- Planning and Instruction Agent: Remembers the user's instruction, creates and improves plan based on existing information and context, decides what next steps to take.\n",
    "    - Let model know what tools it can use, their descriptions, then ask it to plan.\n",
    "    - Model comes up with a plan - you ask model - lets sya step-x fails or doesn't provide expected results. what can be done instead.\n",
    "    - plan improvement by self-critic\n",
    "    - lets evaluate our plan and progress step by step and see if we can improve it.\n",
    "    - User query Intent detection: based on query intent we will decide what type of text/web filtering is needed, as well as reduce number of calls for other calls. This can be part of planning.\n",
    "    - based on our plan, we want to reduce both input and output size of every subsequent module to keep things fast.\n",
    "\n",
    "- Document reader: Given a long text document, this breaks it into chunks before feeding to the API, generates hierarchical TOC based, text reverse index BM25 based, vector based, and simple summary.\n",
    "- Index Agent: Implements functionality for Document Reader, Keep History etc.\n",
    "- Web Search Agent: Uses gs-api or duckduckgo, with Link follower or crawler agent, Contextual page reader, to search for information from the web.\n",
    "- Prompt Engineer agent: Depending on the specific use case, this agent stores preformed prompts, as well as develops new prompts for better model performance.\n",
    "- Critic Agent: Given a context and some response, or context and some intermediate information. It generates criticsm. \"Given the below user request and gathered context, is it enough to answer the user question throughly and correctly?\" [Constitutional Chain](https://python.langchain.com/en/latest/reference/modules/chains.html#langchain.chains.ConstitutionalChain)\n",
    "- Evaluate Response Agent: Depends on Critic Agent, Given two or more responses and a context, it uses predefined factors or comes up with its own factors and then provides an evaluation of the responses and a final best response from the two, on basis of the evaluation factors.\n",
    "- Text Cleaner Agent: Given text composed of html, and other gibberish, get cleaned text. Caveat, the text might contain code blocks which should not be skipped.\n",
    "- Speech to Text.\n",
    "- Image to Descriptive Text\n",
    "- Get page images\n",
    "- Get page tables\n",
    "\n",
    "User interface agents: This agents try to solve the final user problem and usually have some hand crafting.\n",
    "12. Code writer Agent: Writes code for requested functionality, by using other agents.\n",
    "13. Literature Survey agent:\n",
    "14. Open Domain QnA agent:\n",
    "15. Open ended text generation agent: when remaining agents don't fit.\n",
    "\n",
    "\n",
    "\n",
    "Evaluation\n",
    "- On Gpt4 and Bard related evaluation tools\n",
    "- On other tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afb178a",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e1ccf6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-13T16:30:49.800911Z",
     "start_time": "2023-05-13T16:30:41.449122Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install duckduckgo-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "98e030af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T10:18:04.003582Z",
     "start_time": "2023-05-25T10:18:03.919939Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import re\n",
    "from pprint import pprint\n",
    "import time\n",
    "import concurrent.futures\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from copy import deepcopy, copy\n",
    "import requests\n",
    "import tempfile\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import requests\n",
    "import dill\n",
    "import os\n",
    "\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "pd.set_option('max_colwidth', 800)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d30831",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T11:53:49.177752Z",
     "start_time": "2023-05-24T11:53:46.920408Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "openai_embed = OpenAIEmbeddings(openai_api_key= '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', model='text-embedding-ada-002')\n",
    "\n",
    "embeds = openai_embed.embed_documents(documents)\n",
    "type(embeds)\n",
    "np.matmul(np.array(embeds), np.array(embeds).T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d516b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-23T03:55:06.938079Z",
     "start_time": "2023-05-23T03:55:03.574752Z"
    }
   },
   "outputs": [],
   "source": [
    "import ai21\n",
    "ai21.api_key = \"4U9o26ovfFLLFS7q10So3euEQkuCPJqv\"\n",
    "def call_ai21(text, num_tokens=4000, temperature=0.7):\n",
    "    response_grande = ai21.Completion.execute(\n",
    "          model=\"j2-jumbo-instruct\",\n",
    "          prompt=text,\n",
    "          numResults=1,\n",
    "          maxTokens=num_tokens,\n",
    "          temperature=temperature,\n",
    "          topKReturn=0,\n",
    "          topP=1,\n",
    "          stopSequences=[\"##\"]\n",
    "    )\n",
    "    result = response_grande[\"completions\"][0][\"data\"][\"text\"]\n",
    "    return result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "793e5c5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-12T15:44:42.121359Z",
     "start_time": "2023-06-12T15:44:41.170868Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whisper-1\n",
      "babbage\n",
      "davinci\n",
      "text-davinci-edit-001\n",
      "babbage-code-search-code\n",
      "text-similarity-babbage-001\n",
      "code-davinci-edit-001\n",
      "text-davinci-001\n",
      "ada\n",
      "babbage-code-search-text\n",
      "babbage-similarity\n",
      "code-search-babbage-text-001\n",
      "text-curie-001\n",
      "code-search-babbage-code-001\n",
      "text-ada-001\n",
      "text-similarity-ada-001\n",
      "curie-instruct-beta\n",
      "ada-code-search-code\n",
      "ada-similarity\n",
      "code-search-ada-text-001\n",
      "text-search-ada-query-001\n",
      "davinci-search-document\n",
      "ada-code-search-text\n",
      "text-search-ada-doc-001\n",
      "davinci-instruct-beta\n",
      "text-similarity-curie-001\n",
      "code-search-ada-code-001\n",
      "ada-search-query\n",
      "text-search-davinci-query-001\n",
      "curie-search-query\n",
      "davinci-search-query\n",
      "babbage-search-document\n",
      "ada-search-document\n",
      "text-search-curie-query-001\n",
      "text-search-babbage-doc-001\n",
      "curie-search-document\n",
      "text-search-curie-doc-001\n",
      "babbage-search-query\n",
      "text-babbage-001\n",
      "text-search-davinci-doc-001\n",
      "text-embedding-ada-002\n",
      "text-search-babbage-query-001\n",
      "curie-similarity\n",
      "curie\n",
      "text-similarity-davinci-001\n",
      "text-davinci-002\n",
      "gpt-4-0314\n",
      "gpt-3.5-turbo\n",
      "text-davinci-003\n",
      "davinci-similarity\n",
      "gpt-4\n",
      "gpt-3.5-turbo-0301\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import tiktoken\n",
    "\n",
    "# Set your OpenAI API key\n",
    "openai.api_key = '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'\n",
    "\n",
    "\n",
    "model_lst = openai.Model.list()\n",
    "for model in model_lst['data']:\n",
    "    print(model['id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f7495b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T09:25:05.898952Z",
     "start_time": "2023-05-25T09:25:05.883966Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.agents import Tool\n",
    "from langchain.tools import BaseTool\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.text_splitter import NLTKTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.llms import GPT4All\n",
    "from llama_index.node_parser.simple import SimpleNodeParser\n",
    "from llama_index.langchain_helpers.text_splitter import TokenTextSplitter\n",
    "from llama_index import (\n",
    "    GPTVectorStoreIndex, \n",
    "    LangchainEmbedding, \n",
    "    LLMPredictor, \n",
    "    ServiceContext, \n",
    "    StorageContext, \n",
    "    download_loader,\n",
    "    PromptHelper\n",
    ")\n",
    "from llama_index import SimpleDirectoryReader, LangchainEmbedding, GPTListIndex, PromptHelper\n",
    "from llama_index import LLMPredictor, ServiceContext\n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "\n",
    "from langchain.utilities import SerpAPIWrapper\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from typing import Optional, Type\n",
    "from langchain.callbacks.manager import AsyncCallbackManagerForToolRun, CallbackManagerForToolRun\n",
    "from langchain.tools import DuckDuckGoSearchRun\n",
    "from langchain.utilities import BingSearchAPIWrapper, DuckDuckGoSearchAPIWrapper\n",
    "from langchain.tools import DuckDuckGoSearchResults\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "# openai = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.7, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d43d66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T11:54:54.439382Z",
     "start_time": "2023-05-24T11:54:53.289769Z"
    }
   },
   "outputs": [],
   "source": [
    "os.environ[\"BING_SUBSCRIPTION_KEY\"] = \"4cf7467058dc49b9b98e8147a4ae7bd5\"\n",
    "os.environ[\"BING_SEARCH_URL\"] = \"https://api.bing.microsoft.com/v7.0/search\"\n",
    "\n",
    "\n",
    "search = BingSearchAPIWrapper(k=1)\n",
    "search.results(\"python chatgpt\", 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2f65f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T09:24:41.195398Z",
     "start_time": "2023-05-24T09:24:41.169613Z"
    }
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import threading\n",
    "from playwright.async_api import async_playwright\n",
    "    \n",
    "class RunThread(threading.Thread):\n",
    "    def __init__(self, func, args, kwargs):\n",
    "        \"\"\"\n",
    "        https://stackoverflow.com/questions/55409641/asyncio-run-cannot-be-called-from-a-running-event-loop-when-using-jupyter-no\n",
    "        \"\"\"\n",
    "        self.func = func\n",
    "        self.args = args\n",
    "        self.kwargs = kwargs\n",
    "        self.result = None\n",
    "        super().__init__()\n",
    "\n",
    "    def run(self):\n",
    "        self.result = asyncio.run(self.func(*self.args, **self.kwargs))\n",
    "\n",
    "def run_async(func, *args, **kwargs):\n",
    "    try:\n",
    "        loop = asyncio.get_running_loop()\n",
    "    except RuntimeError:\n",
    "        loop = None\n",
    "    if loop and loop.is_running():\n",
    "        thread = RunThread(func, args, kwargs)\n",
    "        thread.start()\n",
    "        thread.join()\n",
    "        return thread.result\n",
    "    else:\n",
    "        return asyncio.run(func(*args, **kwargs))\n",
    "    \n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def call_api_parallel(api_calls, fn, max_workers=4):\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit tasks and collect Future objects\n",
    "        futures = [executor.submit(fn, **api_call) for api_call in api_calls]\n",
    "\n",
    "        # Collect results in order of input tasks\n",
    "        results = [future.result() for future in futures]\n",
    "    return results\n",
    "\n",
    "def call_api_parallel_multi_fn(api_calls, fns):\n",
    "    assert len(api_calls) == len(fns)\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        # Submit tasks and collect Future objects\n",
    "        futures = [executor.submit(fn, **api_call) for fn, api_call in zip(fns, api_calls)]\n",
    "\n",
    "        # Collect results in order of input tasks\n",
    "        results = [future.result() for future in futures]\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7fa050",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T18:15:32.295222Z",
     "start_time": "2023-05-21T18:15:31.279522Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain import OpenAI, ConversationChain\n",
    "\n",
    "llm = OpenAI(temperature=0.1)\n",
    "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
    "print(llm(text))\n",
    "\n",
    "# tools = load_tools([\"wikipedia\", \"llm-math\"], llm=llm)\n",
    "# agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "# agent.run(\"What is the temperature in Bangalore in degree celcius divided by 2\")\n",
    "\n",
    "# conversation = ConversationChain(llm=llm, verbose=True)\n",
    "# output = conversation.predict(input=\"Hi there!\")\n",
    "# print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e83db4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T08:11:27.976706Z",
     "start_time": "2023-05-24T08:11:21.295268Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def round_robin(arr):\n",
    "    while True:\n",
    "        for item in arr:\n",
    "            yield item\n",
    "\n",
    "\n",
    "class CallGpt:\n",
    "    def __init__(self, ):\n",
    "        self.easy_models = round_robin([\n",
    "            \"text-davinci-003\", \"gpt-3.5-turbo\", \"gpt-3.5-turbo-0301\", \"text-davinci-003\", \n",
    "            \"text-davinci-002\", \n",
    "#             \"davinci-instruct-beta:2.0.0\", \n",
    "#             \"text-davinci-001\"\n",
    "                                       ])\n",
    "        self.completion_models = [\n",
    "            \"text-davinci-003\",\"text-davinci-002\", \"davinci-instruct-beta:2.0.0\", \"text-davinci-001\"\n",
    "        ]\n",
    "        self.turbo_models = round_robin([\"gpt-3.5-turbo\", \"gpt-3.5-turbo-0301\"])\n",
    "        self.hard_models = round_robin([\"gpt-4\", \"gpt-4-0314\"])\n",
    "\n",
    "        self.system = \"You are a helpful assistant. Please respond to the user request while following their instructions.\"\n",
    "        import tiktoken\n",
    "        self.easy_enc = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "        self.hard_enc = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "    def get_easy_call(self):\n",
    "        @retry(wait=wait_random_exponential(min=15, max=60), stop=stop_after_attempt(3))\n",
    "        def call(text, temperature=0.7, num_tokens=None):\n",
    "            easy_model = next(self.easy_models)\n",
    "            input_len = len(self.easy_enc.encode(self.system +\" \\n \" + text))\n",
    "            if easy_model in self.completion_models:\n",
    "                completions = openai.Completion.create(\n",
    "                    engine=easy_model,\n",
    "                    prompt=self.system +\" \\n \" + text,\n",
    "                    temperature=temperature,\n",
    "                    max_tokens = 3000,\n",
    "                )\n",
    "                message = completions.choices[0].text\n",
    "                finish_reason = completions.choices[0].finish_reason\n",
    "#                 if finish_reason != 'stop':\n",
    "#                     print(easy_model + \" \" + str(input_len) + \" \" + str(len(self.easy_enc.encode(message))) + \" \" + \" \"+ finish_reason + \" \" + message + \" \\n\")\n",
    "                assert finish_reason == 'stop'\n",
    "            else:\n",
    "                response = openai.ChatCompletion.create(\n",
    "                    model=easy_model,\n",
    "                    messages=[\n",
    "                            {\"role\": \"system\", \"content\": self.system},\n",
    "                            {\"role\": \"user\", \"content\": text},\n",
    "                        ],\n",
    "                        temperature=temperature\n",
    "                    )\n",
    "                message = response['choices'][0]['message']['content']\n",
    "                finish_reason = response['choices'][0]['finish_reason']\n",
    "#                 print(easy_model + \" \" + str(len(self.easy_enc.encode(self.system +\" \\n \" + text))) + \" \" + str(len(self.easy_enc.encode(message))) + \" \" + \" \"+ finish_reason + \" \" + message + \" \\n\")\n",
    "                assert finish_reason == 'stop'\n",
    "                \n",
    "            return message\n",
    "        return call\n",
    "    def get_turbo_call(self):\n",
    "        @retry(wait=wait_random_exponential(min=25, max=60), stop=stop_after_attempt(3))\n",
    "        def call(text, temperature=0.7, num_tokens=None):\n",
    "            easy_model = next(self.turbo_models)\n",
    "            input_len = len(self.easy_enc.encode(self.system +\" \\n \" + text))\n",
    "            \n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=easy_model,\n",
    "                messages=[\n",
    "                        {\"role\": \"system\", \"content\": self.system},\n",
    "                        {\"role\": \"user\", \"content\": text},\n",
    "                    ],\n",
    "                    temperature=temperature\n",
    "                )\n",
    "            message = response['choices'][0]['message']['content']\n",
    "            finish_reason = response['choices'][0]['finish_reason']\n",
    "#             print(easy_model + \" \" + str(len(self.easy_enc.encode(self.system +\" \\n \" + text))) + \" \" + str(len(self.easy_enc.encode(message))) + \" \" + \" \"+ finish_reason + \" \" + message + \" \\n\")\n",
    "            assert finish_reason == 'stop'\n",
    "            return message\n",
    "        return call\n",
    "    def get_hard_call(self):\n",
    "        @retry(wait=wait_random_exponential(min=25, max=60), stop=stop_after_attempt(3))\n",
    "        def call(text, temperature=0.1, num_tokens=None):\n",
    "            model = next(self.hard_models)\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                        {\"role\": \"system\", \"content\": self.system},\n",
    "                        {\"role\": \"user\", \"content\": text},\n",
    "                    ],\n",
    "                    temperature=temperature\n",
    "                )\n",
    "            assert response['choices'][0]['finish_reason'] == 'stop'\n",
    "            return response['choices'][0]['message']['content']\n",
    "        return call\n",
    "     \n",
    "callGpt = CallGpt()\n",
    "callGpt.get_easy_call()(\"Hi gpt! Howdy\")\n",
    "# callGpt.get_hard_call()(\"Hi gpt! Howdy\")\n",
    "# callGpt.get_turbo_call()(\"Hi gpt! Howdy\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f88634c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T18:16:26.885063Z",
     "start_time": "2023-05-21T18:16:26.876642Z"
    }
   },
   "outputs": [],
   "source": [
    "def chunk_text_langchain(text, chunk_size=3400):\n",
    "    text_splitter = TokenTextSplitter(chunk_size=chunk_size, chunk_overlap=100)\n",
    "    texts = text_splitter.split_text(text)\n",
    "    for t in texts:\n",
    "        yield t\n",
    "        \n",
    "def split_text(text):\n",
    "    # Split the text by spaces, newlines, and HTML tags\n",
    "    chunks = re.split(r'( |\\n|<[^>]+>)', text)\n",
    "    \n",
    "    # Find the middle index\n",
    "    middle = len(chunks) // 2\n",
    "\n",
    "    # Split the chunks into two halves\n",
    "    first_half = ''.join(chunks[:min(middle+100, len(chunks)-1)])\n",
    "    second_half = ''.join(chunks[max(0, middle-100):])\n",
    "    \n",
    "    yield first_half\n",
    "    yield second_half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3714014e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T09:23:36.629587Z",
     "start_time": "2023-05-25T09:23:36.593194Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_nested(val, nesting = -5): \n",
    "    if isinstance(val, dict): \n",
    "        print('') \n",
    "        nesting += 5 \n",
    "        print(nesting * ' ', end='') \n",
    "        print(type(val)) \n",
    "        for k in val: \n",
    "            print(nesting * ' ', end='') \n",
    "            print(k, end=':') \n",
    "            print_nested(val[k],nesting) \n",
    "    elif isinstance(val, (tuple, list)) and len(val) > 0 and isinstance(val[0], (dict, tuple, list)):\n",
    "        nesting += 5\n",
    "        print('') \n",
    "        print(nesting * ' ', end='') \n",
    "        print(type(val), end=\":\")\n",
    "        print_nested(val[0], nesting) \n",
    "    else:\n",
    "        print(type(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df6bd5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T18:16:26.902928Z",
     "start_time": "2023-05-21T18:16:26.887635Z"
    }
   },
   "outputs": [],
   "source": [
    "class AddAttribute:\n",
    "    def __init__(self, attribute, value):\n",
    "        self.attribute = attribute\n",
    "        self.value = value\n",
    "\n",
    "    def __call__(self, func):\n",
    "        setattr(func, self.attribute, self.value)\n",
    "        return func\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a928bf58",
   "metadata": {},
   "source": [
    "# Make Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc46c7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T18:16:27.487440Z",
     "start_time": "2023-05-21T18:16:27.454381Z"
    }
   },
   "outputs": [],
   "source": [
    "@AddAttribute('name', \"MathTool\")\n",
    "@AddAttribute('description', \"\"\"\n",
    "MathTool:\n",
    "    This tool takes a numeric expression as a string and provides the output for it.\n",
    "\n",
    "    Input params/args: \n",
    "        num_expr (str): numeric expression to evaluate\n",
    "\n",
    "    Returns: \n",
    "        str: evaluated expression answer\n",
    "\n",
    "    Usage:\n",
    "        `answer=MathTool(num_expr=\"2*3\") # Expected answer = 6, # This tool needs no initialization`\n",
    "\n",
    "    \"\"\")\n",
    "def MathTool(num_expr: str):\n",
    "    math_tool = load_tools([\"llm-math\"], llm=llm)[0]\n",
    "    return math_tool._run(num_expr).replace(\"Answer: \", \"\")\n",
    "\n",
    "\n",
    "@AddAttribute('name', \"WikipediaTool\")\n",
    "@AddAttribute('description', \"\"\"\n",
    "WikipediaTool:\n",
    "    This tool takes a phrase or key words and searches them over wikipedia, returns results from wikipedia as a str.\n",
    "\n",
    "    Input params/args: \n",
    "        search_phrase (str): phrase to search over on wikipedia\n",
    "\n",
    "    Returns: \n",
    "        str: searched paragraph on basis of search_phrase from wikipedia\n",
    "\n",
    "    Usage:\n",
    "        `answer=WikipediaTool(search_phrase=\"phrase to search\") # This tool needs no initialization`\n",
    "\n",
    "    \"\"\")\n",
    "def WikipediaTool(search_phrase: str):\n",
    "    tool = load_tools([\"wikipedia\"], llm=llm)[0]\n",
    "    return tool._run(search_phrase)\n",
    "\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "\n",
    "@AddAttribute('name', \"TextLengthCheck\")\n",
    "@AddAttribute('description', \"\"\"\n",
    "TextLengthCheck:\n",
    "    Checks if the token count of the given `text_document` is smaller or lesser than the `threshold`.\n",
    "\n",
    "    Input params/args: \n",
    "        text_document (str): document to verify if its length or word count or token count is less than threshold.\n",
    "        threshold (int): Token count, text_document token count is below this then returns True\n",
    "\n",
    "    Returns: \n",
    "        bool: whether length or token count is less than given threshold.\n",
    "\n",
    "    Usage:\n",
    "        `length_valid = TextLengthCheck(text_document=\"document to check length\") # This tool needs no initialization`\n",
    "        `less_than_ten = TextLengthCheck(text_document=\"document to check length\", threshold=10)`\n",
    "\n",
    "    \"\"\")\n",
    "def TextLengthCheck(text_document: str, threshold: int=3400):\n",
    "    return len(enc.encode(text_document)) < threshold\n",
    "\n",
    "@AddAttribute('name', \"Search\")\n",
    "@AddAttribute('description', \"\"\"\n",
    "Search:\n",
    "    This tool takes a search phrase, performs search over a web search engine and returns a list of urls for the search.\n",
    "\n",
    "    Input params/args: \n",
    "        search_phrase (str): phrase or keywords to search over the web/internet.\n",
    "        top_n (int): Number of webpages or results to return from search. Default is 5.\n",
    "\n",
    "    Returns: \n",
    "        List[str]: List of webpage urls for given search_phrase, List length same as top_n input parameter.\n",
    "\n",
    "    Usage:\n",
    "        `web_url_list = Search(search_phrase=\"phrase to search\") # This tool needs no initialization`\n",
    "        \n",
    "    Alternative Usage:\n",
    "        `web_url_list = Search(search_phrase=\"phrase to search\", top_n=20) # Get a custom number of results\n",
    "\n",
    "    \"\"\")\n",
    "def Search(search_phrase: str, top_n: int=5):\n",
    "    return [r[\"link\"] for r in  BingSearchAPIWrapper().results(search_phrase, top_n)]\n",
    "\n",
    "@AddAttribute('name', \"ChunkText\")\n",
    "@AddAttribute('description', \"\"\"\n",
    "ChunkText:\n",
    "    This tool takes a text document and chunks it into given chunk size lengths, then returns a list of strings as chunked sub-documents.\n",
    "\n",
    "    Input params/args: \n",
    "        text_document (str): document to create chunks from.\n",
    "        chunk_size (int): Size of each chunk. Default is 3400, smaller chunk sizes are needed if downstream systems throw length error or token limit exceeded errors.\n",
    "\n",
    "    Returns: \n",
    "        List[str]: text_chunks\n",
    "\n",
    "    Usage:\n",
    "        `text_chunks = ChunkText(text_document=\"document to chunk\") # This tool needs no initialization`\n",
    "        \n",
    "    Alternative Usage:\n",
    "        `text_chunks = ChunkText(text_document=\"document to chunk\", chunk_size=1800) # Smaller chunk size, more chunks, but avoid token limit exceeded or length errors.\n",
    "\n",
    "    \"\"\")\n",
    "def ChunkText(text_document: str, chunk_size: int=3400, chunk_overlap:int=100):\n",
    "    text_splitter = TokenTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    return text_splitter.split_text(text_document)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a47e38c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T18:16:27.505282Z",
     "start_time": "2023-05-21T18:16:27.489005Z"
    }
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "tlc_gpt3 = partial(TextLengthCheck, threshold=1700)\n",
    "tlc_gpt4 = partial(TextLengthCheck, threshold=3400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f371f137",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T18:16:27.567346Z",
     "start_time": "2023-05-21T18:16:27.507025Z"
    },
    "code_folding": [
     131
    ]
   },
   "outputs": [],
   "source": [
    "class Summarizer:\n",
    "    def __init__(self):\n",
    "        self.name = \"Summariser\"\n",
    "        self.description = \"\"\"\n",
    "Summarizer:\n",
    "    This tool takes a text document and summarizes it into a shorter version while preserving the main points and context. Useful when the document is too long and needs to be shortened before further processing.\n",
    "\n",
    "    Input params/args: \n",
    "        text_document (str): document to summarize.\n",
    "\n",
    "    Returns: \n",
    "        str: summarized_document.\n",
    "\n",
    "    Usage:\n",
    "        `summary = Summarizer()(text_document=\"document to summarize\") # Note: this tool needs to be initialized first.`\n",
    "    \"\"\"\n",
    "        self.prompt = PromptTemplate(\n",
    "            input_variables=[\"document\"],\n",
    "            template=\"\"\" \n",
    "Summarize the document below into a shorter version (by eliminating repeatation, by paraphrasing etc.) while preserving the main points and context, do not miss any important details.\n",
    "Document is given below:\n",
    "{document}\n",
    "\"\"\",\n",
    "        )\n",
    "    def __call__(self, text_document):\n",
    "        prompt = self.prompt.format(document=text_document)\n",
    "        return callGpt.get_easy_call()(prompt, temperature=0.7)\n",
    "    \n",
    "class ReduceRepeatTool:\n",
    "    def __init__(self):\n",
    "        self.name = \"ReduceRepeatTool\"\n",
    "        self.description = \"\"\"       \n",
    "ReduceRepeatTool:\n",
    "    This tool takes a text document reduces repeated content in the document. Useful when document has a lot of repeated content or ideas which can be mentioned in a shorter version.\n",
    "\n",
    "    Input params/args: \n",
    "        text_document (str): document to summarize.\n",
    "\n",
    "    Returns: \n",
    "        str: non_repeat_document.\n",
    "\n",
    "    Usage:\n",
    "        `non_repeat_document = ReduceRepeatTool()(text_document=\"document to to reduce repeats\") # Note: this tool needs to be initialized first.`\n",
    "        \n",
    "    \"\"\"\n",
    "        self.prompt = PromptTemplate(\n",
    "            input_variables=[\"document\"],\n",
    "            template=\"\"\" \n",
    "Reduce repeated content in the document given. Some ideas or phrases or points are repeated with minor variation, please remove them by paraphrasing the document into a shorter version, while preserving the main points and context, do not miss any important details.\n",
    "Document is given below:\n",
    "{document}\n",
    "\"\"\",\n",
    "        )\n",
    "    def __call__(self, text_document):\n",
    "        prompt = self.prompt.format(document=text_document)\n",
    "        return callGpt.get_turbo_call()(prompt, temperature=0.4)\n",
    "\n",
    "def combine_array_two_at_a_time(array):\n",
    "    result = []\n",
    "    if len(array) % 2 == 1:\n",
    "        array.append('')\n",
    "    for i in range(0, len(array), 2):\n",
    "        result.append(array[i] + ' ' + array[i+1])\n",
    "    return result\n",
    "\n",
    "def concat_array_two_at_a_time(array):\n",
    "    result = []\n",
    "    if len(array) % 2 == 1:\n",
    "        array.append('')\n",
    "    for i in range(0, len(array), 2):\n",
    "        result.append([array[i],array[i+1]])\n",
    "    return result\n",
    "\n",
    "\n",
    "def process_text(text, chunk_size, my_function):\n",
    "    # Split the text into chunks\n",
    "    chunks = list(chunk_text_langchain(text, chunk_size))\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        # Use the executor to apply my_function to each chunk\n",
    "        futures = [executor.submit(my_function, chunk) for chunk in chunks]\n",
    "\n",
    "    # Get the results from the futures\n",
    "    results = [future.result() for future in futures]\n",
    "    tlc = partial(TextLengthCheck, threshold=1800)\n",
    "    summariser = Summarizer()\n",
    "    while len(results) > 1:\n",
    "        results = [r if tlc(r) else summariser(r) for r in results]\n",
    "        results = combine_array_two_at_a_time(results)\n",
    "    results = [r if tlc(r) else summariser(r) for r in results]\n",
    "    results = ReduceRepeatTool()(' '.join(results))\n",
    "    \n",
    "    return results\n",
    "\n",
    "async def get_url_content(url):\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch()\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(url)\n",
    "        title = await page.title()\n",
    "        page_content = await page.content()\n",
    "        # TODO: get rendered body\n",
    "        page_content = await page.evaluate(\"\"\"\n",
    "        (() => document.body.innerText)()\n",
    "        \"\"\")\n",
    "        await browser.close()\n",
    "        return {\"title\": title, \"page_content\": page_content}\n",
    "\n",
    "\n",
    "class Cleaner:\n",
    "    def __init__(self, model=\"gpt-3.5-turbo\", prompt=None, context=None):\n",
    "        self.instruction = \"\"\"\n",
    "You will be given unclean text fragments from web scraping a url.\n",
    "Your goal is to return cleaned text without html tags and other irrelevant content (including code exception stack traces). \n",
    "If you are given a user request, instruction or query, then use that as well in filtering the information and return information relevant to the user query or instruction.\n",
    "just extract relevant information if user query is given (Try to answer mostly in bullet points in this case.) else return cleaned text..\n",
    "No creativity needed here.\n",
    "Some context about the source document and user query is provided next, use the user query if provided and give very concise succint response.\n",
    "        \"\"\" if prompt is None else prompt\n",
    "        self.clean_now_follows = \"\\nActual text to be cleaned follows: \\n\"\n",
    "        self.prompt = (self.instruction + \" \" + (context if context is not None else \"\") + \" \" + self.clean_now_follows) if prompt is None else prompt\n",
    "        \n",
    "    def clean_one(self, string, model=None):\n",
    "        return callGpt.get_easy_call()(self.prompt + string, temperature=0.1)\n",
    "    \n",
    "    def clean_one_with_exception(self, string):\n",
    "        try:\n",
    "            cleaned_text = self.clean_one(string)\n",
    "            return cleaned_text\n",
    "        except Exception as e:\n",
    "            exp_str = str(e)\n",
    "            too_long = \"maximum context length\" in exp_str and \"your messages resulted in\" in exp_str\n",
    "            if too_long:\n",
    "                return \" \".join([self.clean_one_with_exception(st) for st in split_text(string)])\n",
    "            raise e\n",
    "                \n",
    "    def __call__(self, string, chunk_size=3400):\n",
    "        from functools import partial\n",
    "#         cleaners = [self.clean_one_with_exception for m in self.models]\n",
    "#         cleaners = round_robin(cleaners)\n",
    "        # TODO: Cleaner's next chunks could use short summary from the previous chunks to model understand page context.\n",
    "#         return process_text_round_robin(string, chunk_size, cleaners)\n",
    "        return process_text(string, chunk_size, self.clean_one_with_exception)\n",
    "\n",
    "class GetWebPage:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.name = \"GetWebPage\"\n",
    "        self.description = \"\"\"\n",
    "GetWebPage:\n",
    "    This tool takes a url link to a webpage and returns cleaned text content of that Page. Useful if you want to visit a page and get it's content. Optionally it can also take a user context or instruction and give only relevant parts of the page for the provided context.\n",
    "\n",
    "    Input params/args: \n",
    "        url (str): url of page to visit\n",
    "        context (str): user query/instructions/context about what to look for in this webpage\n",
    "\n",
    "    Returns: \n",
    "        str: page_content\n",
    "\n",
    "    Usage:\n",
    "        `page_content = GetWebPage()(url=\"url to visit\", context=\"user query or page reading instructions\") # Note: this tool needs to be initialized first.`\n",
    "\n",
    "    \"\"\"\n",
    "    def __call__(self, url, context=None):\n",
    "        page_items = run_async(get_url_content, url)\n",
    "#         clean_title = Cleaner(context=f\"Some context about the source, url: {url}, title: {page_items['title']}\")(page_items[\"title\"])\n",
    "        if not isinstance(page_items, dict):\n",
    "            print(f\"url: {url}, title: None, content: None\")\n",
    "            return f\"url: {url}, title: None, content: None\"\n",
    "        page_content = page_items[\"page_content\"]\n",
    "        if not isinstance(page_content, str):\n",
    "            print(f\"url: {url}, title: {page_items['title']}, content: None\")\n",
    "            return f\"url: {url}, title: {page_items['title']}, content: None\"\n",
    "        page_content = Cleaner(context=f\"\\n\\n url: {url}, title: {page_items['title']}\" + (f\"user query or context: {context}\" if context is not None else \"\"))(page_content, \n",
    "        chunk_size=768)\n",
    "        return f\"url: {url}, title: {page_items['title']}, content: {page_content}\"\n",
    "    def _run(self, url: str, run_manager: Optional[CallbackManagerForToolRun] = None) -> str:\n",
    "        \"\"\"Use the tool.\"\"\"\n",
    "        return self.__call__(url)\n",
    "    async def _arun(self, query: str, run_manager: Optional[AsyncCallbackManagerForToolRun] = None) -> str:\n",
    "        \"\"\"Use the tool asynchronously.\"\"\"\n",
    "        raise NotImplementedError(\"custom_search does not support async\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45545a33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T18:20:49.793886Z",
     "start_time": "2023-05-21T18:20:30.512057Z"
    }
   },
   "outputs": [],
   "source": [
    "class PDFReaderTool:\n",
    "    def __init__(self,):\n",
    "        self.reader = download_loader(\"PyMuPDFReader\")\n",
    "        \n",
    "    def __call__(self, url):\n",
    "        with tempfile.NamedTemporaryFile(dir=os.getcwd(), delete=False) as temp_file:\n",
    "            response = requests.get(url)\n",
    "            temp_file.write(response.content)\n",
    "\n",
    "\n",
    "            documents = self.reader().load(file_path=temp_file.name, metadata=True)\n",
    "        for doc in documents:\n",
    "            doc.text = doc.text.decode()\n",
    "        doc_text = \" \".join([doc.text for doc in documents])\n",
    "        return doc_text\n",
    "    \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "9cdf3897",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T12:41:37.765675Z",
     "start_time": "2023-05-25T12:41:37.737297Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class LongSummarizer:\n",
    "    def __init__(self, chunk_size=1000):\n",
    "        self.chunk_size=1000\n",
    "        self.name = \"LongSummarizer\"\n",
    "        self.description = \"\"\"\n",
    "LongSummarizer:\n",
    "    This tool takes a text document breaks it into chunks, then summarizes the chunks and creates chunk level and document level summary.\n",
    "\n",
    "    Input params/args: \n",
    "        long_document (str): document to summarize.\n",
    "\n",
    "    Returns: \n",
    "        str: summarized_document.\n",
    "\n",
    "    Usage:\n",
    "        `summarized_document = LongSummarizer()(text_document=\"Long document to summarize\") # Note: this tool needs to be initialized first.`\n",
    "    \"\"\"\n",
    "        self.info_dict = dict(title={\"information_request\": \"Write a one line title describing the section.\", \n",
    "                                     \"information\":\"title\"}, \n",
    "                             summary={\"information_request\": \"Write a thorough and detailed summary for the current document fragment using the fragment text and `summary_till_now` as guide. Capture all details of this fragment.\", \n",
    "                                      \"information\":\"summary\"},\n",
    "                             summary_till_now={\"information_request\": \"Write a coherent and detailed running summary using the provided 'summary till now' and the current fragment of document. Capture all details of what we have read till now including details of this part in 'summary_till_now'.\", \n",
    "                                               \"information\":\"summary till now\"},\n",
    "                             questions_and_answers={\"information_request\": \"Generate multiple relevant questions and their answers which can be answered from this fragment with each question and answer in a different line. Don't do numbering of questions and answers\", \n",
    "                                                    \"information\":\"Generated Questions and Answers\"})\n",
    "        self.prompt = PromptTemplate(\n",
    "            input_variables=[\"summary_till_now\", \"document\", \"information_request\", \"information\"],\n",
    "            template=\"\"\" \n",
    "Given below text is small part/fragment text of a larger document that we are reading sequentially.\n",
    "\"{document}\"\n",
    "\n",
    "Note: This is a raw unformatted document which may contain irrelevant text/syntax or other noise. Ignore what you consider noise or irrelevant.\n",
    "\n",
    "\n",
    "You are also given a summary of what we have read till now (summary till now excluding this fragment text) from the larger document below:\n",
    "{summary_till_now}\n",
    "\n",
    "\n",
    "Based on the fragment of document and previous summary above provided you need to:\n",
    "{information_request}\n",
    "\n",
    "\n",
    "{information}:\n",
    "\n",
    "\"\"\",\n",
    "        )\n",
    "    def __call__(self, long_document):\n",
    "        \n",
    "        chunks = ChunkText(long_document, self.chunk_size)\n",
    "        running_summary = \"No summary yet, this is the first fragment.\"\n",
    "        chunk_questions = []\n",
    "        chunked_summary = []\n",
    "        title = []\n",
    "        \n",
    "        for cnk in tqdm(chunks):\n",
    "            rdict = dict()\n",
    "            keys, prompts = [], []\n",
    "            for k, v in self.info_dict.items():\n",
    "                prompt = self.prompt.format(document=cnk, summary_till_now=running_summary, \n",
    "                                            information_request=v[\"information_request\"], information=v[\"information\"])\n",
    "#                 print(prompt, \"\\n\", \"=\"*80)\n",
    "                \n",
    "                prompts.append(prompt)\n",
    "                keys.append(k)\n",
    "            calls = [{\"text\": p, \"temperature\": 0.7, \"num_tokens\": 512} for p in prompts]\n",
    "            fns = [callGpt.get_turbo_call() if k==\"summary_till_now\" else call_ai21 for k in keys]\n",
    "#             resp = call_api_parallel(calls, call_ai21)\n",
    "            resp = call_api_parallel_multi_fn(calls, fns)\n",
    "            resp = dict(zip(keys, resp))\n",
    "            pprint(resp)\n",
    "            chunked_summary.append(resp[\"summary\"])\n",
    "            qna_success = False\n",
    "            qna_resp = resp[\"questions_and_answers\"]\n",
    "            while not qna_success:\n",
    "                qna = [qa.strip() for qa in qna_resp.split(\"\\n\") if len(qa.strip()) > 0]\n",
    "                # any line with less than 3 words.\n",
    "                qna = [qa for qa in qna if len(qa.split())>2]\n",
    "                qna_success = len(qna) % 2 == 0\n",
    "                if not qna_success:\n",
    "                    qna_call = calls[keys.index(\"questions_and_answers\")]\n",
    "                    qna_resp = call_ai21(**qna_call)\n",
    "                    \n",
    "            \n",
    "            qna = concat_array_two_at_a_time(qna)\n",
    "            chunk_questions.append(qna)\n",
    "            running_summary = resp[\"summary_till_now\"]\n",
    "            title.append(resp[\"title\"])\n",
    "        full_length_summary = \" \".join(chunked_summary)        \n",
    "        return dict(full_length_summary=full_length_summary, title=title, chunked_summary=chunked_summary, chunks=chunks, running_summary=running_summary, chunk_questions=chunk_questions)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f49aaf2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T17:49:12.244298Z",
     "start_time": "2023-05-24T17:49:11.990143Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "chunks = ChunkText(doc_text, 500)\n",
    "\n",
    "def create_index_faiss(chunks, embed_model, doc_id=None):\n",
    "    if doc_id is None:\n",
    "        doc_id = [\"\"] * len(chunks)\n",
    "    elif isinstance(doc_id, (str, int)):\n",
    "        doc_id = [doc_id] * len(chunks)\n",
    "    else:\n",
    "        assert len(doc_id) == len(chunks) and isinstance(doc_id, (list, tuple))\n",
    "        doc_id = [int(d) for d in doc_id]\n",
    "    chunks = [Document(page_content=c, metadata={\"order\": i}) for i, c in enumerate(chunks)]\n",
    "    for ix, chunk in enumerate(chunks):\n",
    "        chunk.metadata[\"next\"] = None if ix == len(chunks)-1 else chunks[ix + 1]\n",
    "        chunk.metadata[\"previous\"] = None if ix == 0 else chunks[ix - 1]\n",
    "        chunk.metadata[\"doc_id\"] = doc_id[ix]\n",
    "    db = FAISS.from_documents(chunks, embed_model)\n",
    "    return db\n",
    "\n",
    "# db = FAISS.from_documents(chunks, openai_embed)\n",
    "# docs = db.similarity_search(\"What performance advantage does this method provide?\")\n",
    "# print(len(docs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0611a50b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T16:55:18.166300Z",
     "start_time": "2023-05-22T16:55:18.147361Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from llama_index.data_structs.node import Node, DocumentRelationship\n",
    "from llama_index import LangchainEmbedding, ServiceContext, Document\n",
    "from llama_index import GPTTreeIndex, SimpleDirectoryReader\n",
    "from IPython.display import Markdown\n",
    "\n",
    "\n",
    "def create_index_with_nodes(chunks, embed_model, doc_id=None, ):\n",
    "    embed_model = LangchainEmbedding(embed_model)\n",
    "    prompt_helper = PromptHelper(max_input_size=4000, num_output=1, max_chunk_overlap=-1000)\n",
    "    llm_predictor = LLMPredictor(llm=OpenAI(temperature=0, model_name=\"gpt-3.5-turbo\", max_tokens=4000))\n",
    "    service_context = ServiceContext.from_defaults(\n",
    "        llm_predictor=llm_predictor,\n",
    "        embed_model=embed_model,\n",
    "        prompt_helper=prompt_helper,\n",
    "    )\n",
    "    nodes = []\n",
    "    if doc_id is None:\n",
    "        doc_id = [\"\"] * len(chunks)\n",
    "    elif isinstance(doc_id, str):\n",
    "        doc_id = [doc_id] * len(chunks)\n",
    "    else:\n",
    "        assert len(doc_id) == len(chunks) and isinstance(doc_id, (list, tuple))\n",
    "        doc_id = [str(d)+\"_\" for d in doc_id]\n",
    "    for ix, nt in enumerate(chunks):\n",
    "        node = Node(text=nt, doc_id= doc_id[ix] + str(ix))\n",
    "        if len(nodes)>1:\n",
    "            node.relationships[DocumentRelationship.PREVIOUS] = nodes[ix -1].get_doc_id()\n",
    "        nodes.append(node)\n",
    "\n",
    "    for ix, node in enumerate(nodes):\n",
    "        if ix + 1 < len(nodes):\n",
    "            node.relationships[DocumentRelationship.NEXT] = nodes[ix+1].get_doc_id()\n",
    "    \n",
    "    index = GPTVectorStoreIndex.from_documents(nodes, service_context=service_context)\n",
    "    return index\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9cc9a1",
   "metadata": {},
   "source": [
    "# Create Paper Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecea588",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T08:40:32.561319Z",
     "start_time": "2023-05-25T08:40:32.540620Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# key takeaways and reusable insights\n",
    "class DeepReader:\n",
    "    def __init__(self, chunk_size=3000):\n",
    "        self.chunk_size=3000\n",
    "        self.name = \"DeepReader\"\n",
    "        self.description = \"\"\"\n",
    "DeepReader:\n",
    "    This tool reads the paper in depth.\n",
    "\n",
    "    Input params/args: \n",
    "        long_document (str): document to generate questions.\n",
    "        full_summary_doc (dict): summary dict from LongSummarizer\n",
    "\n",
    "    Returns: \n",
    "        Dict[str, str]: deep reading dict\n",
    "\n",
    "    Usage:\n",
    "        `deep_read = DeepReader()(long_document=\"long text document\", full_summary_doc=<full_summary from LongSummarizer>) # Note: this tool needs to be initialized first.`\n",
    "    \"\"\"\n",
    "        self.answer_format = \"\"\"\n",
    "{\n",
    "    \"methodology\": \"\",\n",
    "    \"previous_literature_and_differentiation\": \"\",\n",
    "    \"experiments_and_evaluation\": \"\",\n",
    "    \"results_and_comparison\": \"\",\n",
    "    \"limitations_and_future_work\": \"\"\n",
    "}\n",
    "        \"\"\"\n",
    "        self.prompt = PromptTemplate(\n",
    "            input_variables=[\"full_summary\", \"document\", \"answer_format\", \"previous_information\"],\n",
    "            template=\"\"\"\n",
    "We are reading a paper / document in parts and trying to various aspects of the work.\n",
    "Given below text is small part/fragment text of the larger document that we are reading sequentially.\n",
    "\"{document}\"\n",
    "\n",
    "Note: This is a raw unformatted document which may contain irrelevant text/syntax or other noise. Ignore what you consider noise or irrelevant.\n",
    "\n",
    "\n",
    "You are given a summary of the full larger document below:\n",
    "\"{full_summary}\"\n",
    "\n",
    "\n",
    "Based on these information you are requested to provide the below information (or add more to already gathered \"previous sections information\") to help understand this work/document better.\n",
    "\n",
    "- Motivation and Methodology (in \"methodology\" field)\n",
    "    - What do the authors do in this overall work (i.e. their methodology) with details from this part/fragment \n",
    "    - Detailed methodology and approach described in this work.\n",
    "    - what problem they address ?\n",
    "    - how they solve the problem in details?\n",
    "    - Why do they solve this problem?\n",
    "    - what is their justification in doing it? Why do they use this method? \n",
    "    - Any insights from their methods\n",
    "\n",
    "- Previous Literature and Background work (in \"previous_literature_and_differentiation\" field)\n",
    "    - what previous literature is referred to?\n",
    "    - how their work is different from previous literature?\n",
    "    \n",
    "- Experiments and Evaluation (in \"experiments_and_evaluation\" field)\n",
    "    - How is the proposed method/idea evaluated\n",
    "    - on what datasets do they evaluate\n",
    "    - What experiments are performed?\n",
    "    - Are there any experiments with surprising insights?\n",
    "    - Any other surprising experiments or insights\n",
    "    \n",
    "- Results (in \"results_and_comparison\" field)\n",
    "    - What results do they get and \n",
    "    - how does this method perform compared to other methods?\n",
    "    \n",
    "- Limitations (in \"limitations_and_future_work\" field)\n",
    "    - What are the limitations of this method, \n",
    "    - where does this method fail? \n",
    "    - What are some further future research opportunities for this domain as a follow up to this method?\n",
    "\n",
    "Guidelines:\n",
    "- We may have some of these fields from previous sections as well. Don't repeat the information we already have in your output. \n",
    "- If there is new information for these fields from the current section, then just output the new information from this section to the respective field.\n",
    "- Be concise but detailed and informative, capture all information, don't use platitudes.\n",
    "- Provide detailed answers for each field with as much information as possible. Don't miss out any information.\n",
    "- Avoid phrases like \"This fragment serves as\", \"This fragment highlights\", \"This fragment demonstrates\", \"In this fragment, \", \"In this work\" etc in your output. We are reading the larger document in small parts due to system limitation but all the output will be concatenated together before showing to the user, hence these phrases serve no purporse.\n",
    "\n",
    "If this is not the first section, then from previous sections we have gathered the below \"previous sections information\":\n",
    "\"{previous_information}\"\n",
    "\n",
    "In case this is first section, then \"previous sections information\" will be empty else it will be in the output python dictionary format.\n",
    "\n",
    "\n",
    "Your answer/output must only be a python dictionary only of below format:\n",
    "\n",
    "{answer_format}\n",
    "\n",
    "\"\"\",\n",
    "        )\n",
    "    def __call__(self, long_document, full_summary):\n",
    "        \n",
    "        chunks = ChunkText(long_document, self.chunk_size)\n",
    "        fsum = full_summary[\"running_summary\"]\n",
    "        all_sections_data = []\n",
    "        prev = {}\n",
    "        for chunk in tqdm(chunks):\n",
    "            prompt = self.prompt.format(full_summary=fsum, document=chunk, answer_format=self.answer_format, previous_information=str(prev))\n",
    "            answer = callGpt.get_hard_call()(text=prompt, temperature=0.9)\n",
    "            print(answer)\n",
    "            answer = eval(answer)\n",
    "            prev = answer\n",
    "            all_sections_data.append(answer)\n",
    "            prev = {k: \" \".join([d[k] for d in all_sections_data]).strip() for k in list(all_sections_data[0].keys())}\n",
    "            \n",
    "        return all_sections_data\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f2925e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T06:33:40.149260Z",
     "start_time": "2023-05-25T06:33:40.101381Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357820e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T09:36:53.154736Z",
     "start_time": "2023-05-24T09:36:53.129013Z"
    }
   },
   "outputs": [],
   "source": [
    "# prev sec, # next sec, # this sec. Full summary, chunked summary (\" \".join(full_summary[\"chunked_summary\"]))\n",
    "# Questions which will help a scientist understand this work\n",
    "# Questions which will help a scientist to use this research for their own stuff.\n",
    "# Thought provoking questions\n",
    "# Criticisms and short comings and debatable decisions made in this work\n",
    "# Cross section questions\n",
    "\n",
    "class QuestionAnswerGenerator:\n",
    "    def __init__(self, chunk_size=1000):\n",
    "        self.chunk_size=1000\n",
    "        self.name = \"QuestionAnswerGenerator\"\n",
    "        self.description = \"\"\"\n",
    "QuestionAnswerGenerator:\n",
    "    This tool generates questions for chunks of the text document.\n",
    "\n",
    "    Input params/args: \n",
    "        long_document (str): document to generate questions.\n",
    "        full_summary_doc (dict): summary dict from LongSummarizer\n",
    "\n",
    "    Returns: \n",
    "        List[Tuple[str]]: questions and answers for each chunk.\n",
    "\n",
    "    Usage:\n",
    "        `questions_and_answers = QuestionAnswerGenerator()(long_document=\"long text document\", full_summary_doc=<full_summary from LongSummarizer>) # Note: this tool needs to be initialized first.`\n",
    "    \"\"\"\n",
    "        self.prompt = PromptTemplate(\n",
    "            input_variables=[\"full_summary\", \"current_section\", \"next_section\", \"previous_section\",  \n",
    "                             \"previous_questions_and_answers\"],\n",
    "            template=\"\"\" \n",
    "We want to generate questions and answers from current given section/fragment/chunk of a larger work document. \n",
    "The summary of the larger document is given below:\n",
    "{full_summary}\n",
    "\n",
    "The previous section is given below:\n",
    "\"{previous_section}\"\n",
    "\n",
    "The current section/fragment is given below:\n",
    "\"{current_section}\"\n",
    "\n",
    "The next section is given below:\n",
    "\"{next_section}\"\n",
    "\n",
    "\n",
    "We also provide a set of questions and answers we had earlier created from the current section below:\n",
    "{previous_questions_and_answers}\n",
    "\n",
    "Based on these information you are requested to generate further questions and answers to help understand this work deeper.\n",
    "Generate the following type of questions and their answers.\n",
    "- Complex Questions and answers which help in deeply understanding this work.\n",
    "- Questions which will help a scientist/researcher understand this work.\n",
    "- Questions which will help a scientist to use this work document for their own stuff.\n",
    "- Thought provoking questions which can be opinionated\n",
    "- Questions which require knowledge of multiple sections and reading of whole document.\n",
    "- Questions regarding criticisms and short comings and debatable decisions made in this work.\n",
    "\n",
    "Guidelines to follow:\n",
    "- Ensure that your answers are long, elaborate, detailed and provide deeper insights. \n",
    "- Ensure that your questions are complex and difficult to answer.\n",
    "- No short answers.\n",
    "- Generate each question and answer in a separate line.\n",
    "\n",
    "Questions and Answers:\n",
    "\n",
    "\"\"\",\n",
    "        )\n",
    "    def __call__(self, long_document, full_summary):\n",
    "        \n",
    "        chunks = full_summary[\"chunks\"]\n",
    "        fsum = full_summary[\"running_summary\"]\n",
    "        long_sum = full_summary[\"full_length_summary\"] # \"expanded_summary\"\n",
    "        previous_questions_and_answers = full_summary[\"chunk_questions\"]\n",
    "        \n",
    "        prompts = []\n",
    "        for ix, (pqa, chunk) in enumerate(zip(previous_questions_and_answers, chunks)):\n",
    "            previous_section = \"No previous section. We are on first section.\" if ix == 0 else chunks[ix - 1]\n",
    "            next_section = \"No next section. We are on last section.\" if ix == len(chunks)-1 else chunks[ix + 1]\n",
    "            prompt = self.prompt.format(full_summary=fsum, current_section=chunk, \n",
    "                                        previous_section=previous_section, next_section=next_section, \n",
    "                                        previous_questions_and_answers=pqa)\n",
    "            prompts.append(prompt)\n",
    "        def printed_gpt_call(**kwargs):\n",
    "            rsp = callGpt.get_turbo_call()(**kwargs)\n",
    "#             print(rsp)\n",
    "            return rsp\n",
    "        calls = [{\"text\": p, \"temperature\": 0.7, \"num_tokens\": 4000} for p in prompts]\n",
    "        responses = call_api_parallel(calls, printed_gpt_call, max_workers=2)\n",
    "        chunk_questions = []\n",
    "        for ix, (qna_resp, call) in enumerate(zip(responses, calls)):\n",
    "            qna_success = False\n",
    "            while not qna_success:\n",
    "                qna = [qa.strip() for qa in qna_resp.split(\"\\n\") if len(qa.strip()) > 0]\n",
    "                # any line with less than 3 words.\n",
    "                qna = [qa for qa in qna if len(qa.split())>2]\n",
    "                print(qna)\n",
    "                qna_success = len(qna) % 2 == 0\n",
    "                if not qna_success:\n",
    "                    qna_resp = callGpt.get_turbo_call()(**call)\n",
    "            qna = concat_array_two_at_a_time(qna)\n",
    "            chunk_questions.append(qna)\n",
    "        return chunk_questions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1677d564",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-23T09:48:47.641044Z",
     "start_time": "2023-05-23T09:43:54.691108Z"
    }
   },
   "outputs": [],
   "source": [
    "doc_text = PDFReaderTool()(\"https://arxiv.org/pdf/2305.10429.pdf\")\n",
    "full_summary = LongSummarizer()(doc_text)\n",
    "pprint(full_summary)\n",
    "\n",
    "dpr = DeepReader()(doc_text, full_summary)\n",
    "dpr = {k: \" \".join([d[k] for d in dpr]).strip() for k in list(dpr[0].keys())}\n",
    "pprint(dpr)\n",
    "\n",
    "qa_generations = QuestionAnswerGenerator()(doc_text, full_summary)\n",
    "pprint(qa_generations)\n",
    "\n",
    "full_summary[\"detailed_qna\"] = qa_generations\n",
    "full_summary[\"deep_reader_details\"] = dpr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a378f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T10:03:55.420769Z",
     "start_time": "2023-05-25T10:03:55.402118Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01cad50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T06:36:01.899107Z",
     "start_time": "2023-05-25T06:36:01.890475Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295e5f3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d05442",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T07:00:59.624780Z",
     "start_time": "2023-05-25T06:38:11.197232Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "doc_text = PDFReaderTool()(\"https://vcai.mpi-inf.mpg.de/projects/DragGAN/data/paper.pdf\")\n",
    "full_summary_dgn = LongSummarizer()(doc_text)\n",
    "pprint(full_summary_dgn)\n",
    "\n",
    "\n",
    "dpr = DeepReader()(doc_text, full_summary_dgn)\n",
    "dpr = {k: \" \".join([d[k] for d in dpr]).strip() for k in list(dpr[0].keys())}\n",
    "pprint(dpr)\n",
    "\n",
    "qa_generations = QuestionAnswerGenerator()(doc_text, full_summary_dgn)\n",
    "pprint(qa_generations)\n",
    "\n",
    "full_summary_dgn[\"detailed_qna\"] = qa_generations\n",
    "full_summary_dgn[\"deep_reader_details\"] = dpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae7d0c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T09:16:04.625846Z",
     "start_time": "2023-05-25T09:16:04.619254Z"
    }
   },
   "outputs": [],
   "source": [
    "dpr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa635c41",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Create Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "08a5f2c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T10:28:20.008942Z",
     "start_time": "2023-05-25T10:28:19.954961Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# One or more doc/pdf reader\n",
    "\n",
    "# Literature survey tool\n",
    "\n",
    "# Generate longer text by doing lookups\n",
    "\n",
    "# Running Summarize a Long Document\n",
    "\n",
    "# Generate long answers for a query on a document by multiple look-ups and long generation\n",
    "\n",
    "# Generate long answers for a query/request from multiple supported documents (multi-pdf) by multiple look-ups and long generation\n",
    "\n",
    "# Analyse multiple documents together, with ability to generate cross document questions and answer by seeing across documents.\n",
    "\n",
    "# To give better answers -> Refine the answer by adding more details as well as explaining jargons and side concerns\n",
    "\n",
    "# Save user asked questions\n",
    "\n",
    "# In PDF reader UX user should be able to select a page or section and ask to summarize it contextually with knowledge of remaining doc.\n",
    "\n",
    "# Streaming typing\n",
    "\n",
    "class MultiDocIndex:\n",
    "    pass\n",
    "\n",
    "import mmh3\n",
    "\n",
    "class DocIndex:\n",
    "    def __init__(self, doc_source, doc_filetype, doc_type, doc_text, full_summary):\n",
    "        \n",
    "        self.result_cutoff = 2\n",
    "        self.doc_id = str(mmh3.hash(doc_source + doc_filetype + doc_type, signed=False))\n",
    "        self.doc_source = doc_source\n",
    "        self.doc_filetype = doc_filetype\n",
    "        self.doc_type = doc_type\n",
    "        \n",
    "        if full_summary is None:\n",
    "            if doc_filetype == \"pdf\" and \"http\" in doc_source and doc_text is None:\n",
    "                doc_text = PDFReaderTool()(doc_source)\n",
    "            assert doc_text\n",
    "            lsum = self.long_summarizer(doc_text)\n",
    "        else:\n",
    "            lsum = full_summary\n",
    "        self.doc_data = lsum\n",
    "        \n",
    "        self.small_chunks = ChunkText(doc_text, 256, 50)\n",
    "        self.raw_texts = lsum['chunks']\n",
    "        self.running_summary = lsum[\"running_summary\"]\n",
    "        self.summaries_with_previous_context = lsum['chunked_summary']\n",
    "        self.chunk_titles = lsum[\"title\"]\n",
    "        self.chunk_questions_and_answers = lsum[\"chunk_questions\"]\n",
    "        self.detailed_qna = lsum[\"detailed_qna\"]\n",
    "        \n",
    "        \n",
    "        \n",
    "        dqna = ([qa[0]+\"\\n\"+ qa[1] for qsec in lsum[\"detailed_qna\"] for qa in qsec]) + ([k + \"\\n\" + v for k, v in lsum[\"deep_reader_details\"].items()])\n",
    "        self.dqna_index = create_index_faiss(dqna, openai_embed, doc_id=self.doc_id, )\n",
    "        \n",
    "        # cqna\n",
    "        self.answered_questions = []\n",
    "        \n",
    "        sec_ids, qna = zip(*[[ix, str(ix)+\"\\n\"+qa[0]+\"\\n\"+ qa[1]] for ix, qsec in enumerate(lsum[\"chunk_questions\"]) for qa in qsec])\n",
    "        print(len(sec_ids), len(qna), type(sec_ids))\n",
    "        assert len(sec_ids) == len(qna)\n",
    "        self.qna_index = create_index_faiss(qna, openai_embed, doc_id=sec_ids, )\n",
    "        \n",
    "        # Use a tree index on the full document.\n",
    "        \n",
    "        self.raw_index = create_index_faiss(self.raw_texts, openai_embed,  )\n",
    "        self.summary_index = create_index_faiss(self.summaries_with_previous_context, openai_embed, )\n",
    "        self.small_chunk_index = create_index_faiss(self.small_chunks, openai_embed,  )\n",
    "        \n",
    "        \n",
    "        self.previous_qna_history = []\n",
    "        \n",
    "        self.followup = PromptTemplate(\n",
    "            input_variables=[\"followup\", \"query\", \"answer\", \"fragment\", \"summary\", \"full_summary\", \"questions_answers\", \"answer_format\"],\n",
    "            template=\"\"\"\n",
    "Our user had previously asked us a query to answer from a large document. \n",
    "We had answered the question and now the user has asked a followup question.\n",
    "You will help answer the followup question or information request from context (text chunks of larger document) you are provided. \n",
    "The provided context is part/fragment of a larger main document.\n",
    "\n",
    "Answer the followup question or information request below:\n",
    "\n",
    "{followup}\n",
    "\n",
    "You are given the summary of the larger main document below:\n",
    "\n",
    "{full_summary}\n",
    "\n",
    "You are given few text chunks from within the document to answer this question as below:\n",
    "\n",
    "{fragment}\n",
    "\n",
    "Next, You are given few question and answer pairs from the document below:\n",
    "\n",
    "{questions_answers}\n",
    "\n",
    "You are also given summarized text chunks of certain parts of document below:\n",
    "\n",
    "{summary}\n",
    "\n",
    "Finally, the previous question and its answer is provided below:\n",
    "\n",
    "{query}\n",
    "\n",
    "{answer}\n",
    "\n",
    "Answer elaborately providing as much detail as possible. Remember followup questions (our current scenario) are seeking information different from \"previous question and its answer\" but information in similar area.\n",
    "\n",
    "Provide the below details in your response:\n",
    "- elaborate answer\n",
    "- Is the followup question in search engine friendly format, reply in \"needs_reformulation\" (especially if understanding the followup question requires knowledge of previous question and answer). In this case reformulate the followup question into search friendly self-contained format or good search phrase in \"reformulated_query\".\n",
    "- further questions: what other questions we can ask to answer better (questions which elaborate the asked question/provided answer)\n",
    "- short answer summary: short summary of answer\n",
    "- whether to read next and previous fragment\n",
    "- related questions: related questions which may be similar to this question\n",
    "\n",
    "Your answer must be a python dictionary only of below format:\n",
    "\n",
    "{answer_format}\n",
    "\n",
    "\n",
    "\"\"\",\n",
    "        )\n",
    "        self.followup_response_format = \"\"\"\n",
    "{\n",
    "    \"answer\": <elaborate answer to the followup question>, \n",
    "    \"needs_reformulation\": <bool, whether followup question needs to be rewritten for text search>,\n",
    "    \"reformulated_query\": <str, rewritten followup query, if above needs_reformulation is True>,\n",
    "    \"further_questions\": <List[str], what other questions we can ask to answer better (questions which elaborate the asked question/provided answer)>,\n",
    "    \"short_answer_summary\": <short summary of answer>,\n",
    "    \"read_next_fragment\": <bool, should we read the next fragment from document to improve the answer>,\n",
    "    \"read_previous_fragment\": <bool, should we read the next previous from document to improve the answer>,\n",
    "    \"related_questions\": <List[str], related questions which may be similar to this question (not questions which elaborate the asked question/provided answer)>\n",
    "}\n",
    "        \"\"\"\n",
    "        \n",
    "        self.short_answer_prompt = PromptTemplate(\n",
    "            input_variables=[\"query\", \"fragment\", \"summary\", \"questions_answers\", \"full_summary\", \"answer_format\"],\n",
    "            template=\"\"\"\n",
    "You will help answer a question or information request from context (text chunks of larger document) you are provided. \n",
    "The provided context is part/fragment of a larger main document.\n",
    "\n",
    "Answer the question or information request below:\n",
    "\n",
    "{query}\n",
    "\n",
    "You are given the summary of the larger main document below:\n",
    "\n",
    "{full_summary}\n",
    "\n",
    "You are given few text chunks from within the document to answer this question as below:\n",
    "\n",
    "{fragment}\n",
    "\n",
    "Next, You are given few question and answer pairs from the document below:\n",
    "\n",
    "{questions_answers}\n",
    "\n",
    "You are also given summarized text chunks of certain parts of document below:\n",
    "\n",
    "{summary}\n",
    "\n",
    "Answer elaborately providing as much detail as possible.\n",
    "\n",
    "Provide the below details in your response:\n",
    "- elaborate answer\n",
    "- further questions: what other questions we can ask to answer better (questions which elaborate the asked question/provided answer)\n",
    "- short answer summary: short summary of answer\n",
    "- whether to read next and previous fragment\n",
    "- related questions: related questions which may be similar to this question\n",
    "\n",
    "Your answer must be a python dictionary only of below format:\n",
    "\n",
    "{answer_format}\n",
    "\n",
    "\"\"\",\n",
    "        )\n",
    "        self.short_query_response_format = \"\"\"\n",
    "{\n",
    "    \"answer\": <elaborate answer to the question>, \n",
    "    \"further_questions\": <List[str], what other questions we can ask to answer better (questions which elaborate the asked question/provided answer)>,\n",
    "    \"short_answer_summary\": <short summary of answer>,\n",
    "    \"read_next_fragment\": <bool, should we read the next fragment from document to improve the answer>,\n",
    "    \"read_previous_fragment\": <bool, should we read the next previous from document to improve the answer>,\n",
    "    \"related_questions\": <List[str], related questions which may be similar to this question (not questions which elaborate the asked question/provided answer)>\n",
    "}\n",
    "        \"\"\"\n",
    "        \n",
    "        self.long_query_response_format = \"\"\"\n",
    "    {\n",
    "        \"answer\": <further elaborated and extended answer to the question based on previous answer with much more details and wide coverage>, \n",
    "        \"further_questions\": <List[str], what other questions we can ask to answer better (questions which elaborate the asked question/provided answer)>,\n",
    "        \"short_answer_summary\": <short summary of answer>,\n",
    "        \"related_questions\": <List[str], related questions which may be similar to this question (not questions which elaborate the asked question/provided answer)>\n",
    "    }\n",
    "            \"\"\"\n",
    "\n",
    "        self.long_answer_query = PromptTemplate(\n",
    "                input_variables=[\"query\", \"previous_answer\", \"more_questions_and_answers\", \"full_summary\", \"answer_format\"],\n",
    "                template=\"\"\"\n",
    "You will help write a long and extended answer (building and expanding on previous answer and provinding much more details and wide coverage) for a question or information request below:\n",
    "\n",
    "The question or information request is given below:\n",
    "\n",
    "{query}\n",
    "\n",
    "Previous answer for this question is given below:\n",
    "{previous_answer}\n",
    "\n",
    "\n",
    "You are given the summary of the larger main document below:\n",
    "\n",
    "{full_summary}\n",
    "\n",
    "\n",
    "\n",
    "You are also given few related questions and answers which you can use as additional information as below:\n",
    "\n",
    "{more_questions_and_answers}\n",
    "\n",
    "\n",
    "\n",
    "Answer elaborately providing as much detail as possible.\n",
    "\n",
    "Provide the below details in your response:\n",
    "- elaborate and expanded answer which provides more details than previous answer while keeping all details from previous answer. \n",
    "- To write an extended and further elaborated answer think of multiple different angles that this question can be thought from. \n",
    "- Add clarifications and answers to other questions from \"related questions and answers\" into this as well. Provide details on jargons and difficult to understand terms in your \"answer\" as well.\n",
    "- use details from \"few related questions and answers\" and add them in your \"answer\", you can cover side details in your answer which do not directly relate to the main answer as well. \n",
    "- when looking at \"few related questions and answers\" add more details and elaborations to this \"answer\" from \"few related questions and answers\".\n",
    "- Try to answer any question that may arise from the answer you write in the answer itself.\n",
    "- further questions: what other questions we can ask to answer better (questions which elaborate the asked question/provided answer)\n",
    "- short answer summary: short summary of answer\n",
    "- related questions: related questions which may be similar to this question\n",
    "\n",
    "Your answer must be a python dictionary only of below format:\n",
    "\n",
    "{answer_format}\n",
    "\n",
    "    \"\"\",\n",
    "            )\n",
    "    def save_local(self, folder):\n",
    "        import dill\n",
    "        doc_id = self.doc_id\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "        with open(os.path.join(folder, f\"{doc_id}.index\"), \"wb\") as f:\n",
    "            dill.dump(self, f)\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_local(folder, filename):\n",
    "        import dill\n",
    "        with open(os.path.join(folder, filename + \".index\"), \"rb\") as f:\n",
    "            return dill.load(f)\n",
    "        \n",
    "    \n",
    "    def get_short_answer(self, query):\n",
    "        qna_nodes = self.qna_index.similarity_search(query, k=self.result_cutoff)\n",
    "        raw_nodes = self.raw_index.similarity_search(query, k=self.result_cutoff)\n",
    "        small_chunk_nodes = self.small_chunk_index.similarity_search(query, k=self.result_cutoff*2)\n",
    "        dqna_nodes = self.dqna_index.similarity_search(query, k=self.result_cutoff*2)\n",
    "        raw_node_ids = [n.metadata[\"order\"] for n in raw_nodes]\n",
    "        qna_doc_ids = [n.metadata[\"doc_id\"] for n in qna_nodes]\n",
    "        additional_qna_doc_ids = [n for n in qna_doc_ids if n not in raw_node_ids]\n",
    "\n",
    "        additional_text_qna = \"\\n\".join([self.summaries_with_previous_context[int(i)] for i in additional_qna_doc_ids])\n",
    "        summary_nodes = self.summary_index.similarity_search(query, k=self.result_cutoff)\n",
    "        summary_text = \"\\n\".join([n.page_content for n in summary_nodes]) + \"\\n\" + additional_text_qna\n",
    "        qna_text = \"\\n\".join([n.page_content for n in list(qna_nodes + dqna_nodes)])\n",
    "        raw_text = \"\\n\".join([n.page_content for n in raw_nodes] + [n.page_content for n in small_chunk_nodes])\n",
    "        \n",
    "        prompt = self.short_answer_prompt.format(query=query, fragment=raw_text, summary=summary_text, \n",
    "                                        questions_answers=qna_text, full_summary=self.running_summary,\n",
    "                                        answer_format=self.short_query_response_format)\n",
    "        answers = eval(callGpt.get_hard_call()(prompt, temperature=0.7))\n",
    "        answers[\"raw_node_ids\"] = raw_node_ids\n",
    "        answers[\"query\"] = query\n",
    "        return answers\n",
    "    \n",
    "    def add_more_details_to_answer(self, query, previous_answer=None, \n",
    "                        further_questions_answers=None, \n",
    "                        related_questions_answers=None, more_answers=None):\n",
    "        # Give out similar questions and answers\n",
    "        if previous_answer is None:\n",
    "            previous_answer = self.get_short_answer(query)\n",
    "            pprint(previous_answer)\n",
    "        \n",
    "        if further_questions_answers is None:\n",
    "            further_questions = previous_answer[\"further_questions\"]\n",
    "            further_questions_answers = []\n",
    "            for qs in further_questions:\n",
    "                qs = self.get_short_answer(qs)\n",
    "                further_questions_answers.append(qs)\n",
    "            pprint(further_questions_answers)\n",
    "        \n",
    "        if related_questions_answers is None:\n",
    "            related_questions = previous_answer[\"related_questions\"]\n",
    "            related_questions_answers = []\n",
    "            for qs in related_questions:\n",
    "                qs = self.get_short_answer(qs)\n",
    "                related_questions_answers.append(qs)\n",
    "            pprint(related_questions_answers) \n",
    "        \n",
    "        raw_nodes = self.raw_texts\n",
    "        if more_answers is None:\n",
    "            more_answers = []\n",
    "            for rnid in previous_answer[\"raw_node_ids\"]:\n",
    "                rnid = int(rnid)\n",
    "                if rnid - 1 >= 0:\n",
    "                    more_answers.append(ContextualAnswer()(query, raw_nodes[rnid - 1]))\n",
    "                if (rnid + 1) < len(raw_nodes):\n",
    "                    more_answers.append(ContextualAnswer()(query, raw_nodes[rnid + 1]))\n",
    "            print(len(more_answers))\n",
    "            pprint(more_answers)\n",
    "        \n",
    "        more_questions_and_answers = [eqs[\"query\"] + \"\\n\" + eqs[\"answer\"] for eqs in (further_questions_answers + related_questions_answers)]\n",
    "        more_questions_and_answers = \"\\n\\n\".join(more_questions_and_answers)\n",
    "#         pprint(more_questions_and_answers)    \n",
    "        more_answers = \"\\n\\n\".join(more_answers)\n",
    "                \n",
    "        prompt = self.long_answer_query.format(query=query, previous_answer=previous_answer[\"answer\"] + \"\\n\" + more_answers, \n",
    "                                              more_questions_and_answers=more_questions_and_answers, full_summary=self.running_summary, \n",
    "                                              answer_format=self.long_query_response_format)\n",
    "        answers = eval(callGpt.get_hard_call()(prompt, temperature=0.9))\n",
    "        answers[\"query\"] = query\n",
    "        answers[\"more_answers\"] = more_answers\n",
    "        answers[\"further_questions_answers\"] = further_questions_answers\n",
    "        answers[\"related_questions_answers\"] = related_questions_answers\n",
    "        answers[\"previous_answer\"] = previous_answer\n",
    "        return answers\n",
    "        \n",
    "    def save_answer(self, query, answer):\n",
    "        pass\n",
    "    \n",
    "    def get_short_info(self):\n",
    "        return dict(doc_id=self.doc_id, source=self.doc_source, summary=self.running_summary)\n",
    "    \n",
    "    def get_all_details(self):\n",
    "        return dict(doc_id=self.doc_id, source=self.doc_source, summary=self.running_summary, details=self.doc_data)\n",
    "    \n",
    "    def ask_follow_up(self, query, previous_answer, previous_long_answer=None):\n",
    "        qna_nodes = self.qna_index.similarity_search(query, k=self.result_cutoff)\n",
    "        raw_nodes = self.raw_index.similarity_search(query, k=self.result_cutoff)\n",
    "        small_chunk_nodes = self.small_chunk_index.similarity_search(query, k=self.result_cutoff*2)\n",
    "        dqna_nodes = self.dqna_index.similarity_search(query, k=self.result_cutoff)\n",
    "        raw_node_ids = [n.metadata[\"order\"] for n in raw_nodes]\n",
    "        summary_nodes = self.summary_index.similarity_search(query, k=self.result_cutoff)\n",
    "        summary_text = \"\\n\".join([n.page_content for n in summary_nodes])\n",
    "        qna_text = \"\\n\".join([n.page_content for n in list(qna_nodes + dqna_nodes)])\n",
    "        raw_text = \"\\n\".join([n.page_content for n in raw_nodes] + [n.page_content for n in small_chunk_nodes])\n",
    "        answer=previous_answer[\"answer\"] + \"\\n\" + (previous_answer[\"parent\"][\"answer\"] if \"parent\" in previous_answer else \"\")\n",
    "        prompt = self.followup.format(followup=query, query=previous_answer[\"query\"], \n",
    "                                      answer=answer, summary=summary_text, \n",
    "                                      fragment=raw_text,\n",
    "                                      full_summary=self.running_summary, questions_answers=qna_text, \n",
    "                                      answer_format=self.followup_response_format)\n",
    "        answers = eval(callGpt.get_hard_call()(prompt, temperature=0.7))\n",
    "        answers[\"raw_node_ids\"] = raw_node_ids\n",
    "        answers[\"query\"] = query\n",
    "        answers[\"parent\"] = previous_answer\n",
    "        \n",
    "        if answers[\"needs_reformulation\"] and len(answers[\"reformulated_query\"].strip().split()) > 0 and \"parent\" not in previous_answer:\n",
    "            reformed = self.ask_follow_up(answers[\"reformulated_query\"], answers)\n",
    "            reformed[\"parent\"] = answers\n",
    "            return reformed\n",
    "        \n",
    "        return answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "b7e5e873",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T12:42:26.372458Z",
     "start_time": "2023-05-25T12:42:26.354555Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def create_document_index(pdf_url)->DocIndex:\n",
    "    doc_text = PDFReaderTool()(pdf_url)\n",
    "    full_summary = LongSummarizer()(doc_text)\n",
    "\n",
    "\n",
    "\n",
    "    dpr = DeepReader()(doc_text, full_summary)\n",
    "    dpr = {k: \" \".join([d[k] for d in dpr]).strip() for k in list(dpr[0].keys())}\n",
    "\n",
    "\n",
    "    qa_generations = QuestionAnswerGenerator()(doc_text, full_summary)\n",
    "\n",
    "\n",
    "    full_summary[\"detailed_qna\"] = qa_generations\n",
    "    full_summary[\"deep_reader_details\"] = dpr\n",
    "    doc_index = DocIndex(pdf_url, \n",
    "                \"pdf\", \n",
    "                \"scientific_article\", doc_text, full_summary)\n",
    "    return doc_index\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "6f916b66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T15:44:41.681234Z",
     "start_time": "2023-05-25T14:57:43.032376Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|████████▍                                                                                                                                               | 1/18 [00:18<05:16, 18.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'questions_and_answers': 'Question 1:\\n'\n",
      "                          'What is DragGAN?\\n'\n",
      "                          'Answer 1:\\n'\n",
      "                          'DragGAN is a user-interactive manipulation approach '\n",
      "                          'that allows users to drag any points of the image '\n",
      "                          'to precisely reach target points.\\n'\n",
      "                          '\\n'\n",
      "                          'Question 2:\\n'\n",
      "                          'How does DragGAN achieve precise control over where '\n",
      "                          'pixels go?\\n'\n",
      "                          'Answer 2:\\n'\n",
      "                          'DragGAN achieves precise control over where pixels '\n",
      "                          'go by using two main components: a feature-based '\n",
      "                          'motion supervision that drives the handle point to '\n",
      "                          'move towards the target position, and a new point '\n",
      "                          'tracking approach that leverages the discriminative '\n",
      "                          'generator features to keep localizing the position '\n",
      "                          'of the handle points.\\n'\n",
      "                          '\\n'\n",
      "                          'Question 3:\\n'\n",
      "                          'How does DragGAN compare to prior approaches in the '\n",
      "                          'tasks of image manipulation and point tracking?\\n'\n",
      "                          'Answer 3:\\n'\n",
      "                          'DragGAN demonstrates an advantage over prior '\n",
      "                          'approaches in the tasks of image manipulation and '\n",
      "                          'point tracking. It is able to generate realistic '\n",
      "                          'outputs for challenging scenarios such as '\n",
      "                          'hallucinating occluded content and deforming shapes '\n",
      "                          \"that consistently follow the object's rigidity.\",\n",
      " 'summary': 'DragGAN allows users to \"drag\" the content of any GAN-generated '\n",
      "            'images. Users only need to click a few handle points (red) and '\n",
      "            'target points (blue) on the image, and our approach will move the '\n",
      "            'handle points to precisely reach their corresponding target '\n",
      "            'points. Users can optionally draw a mask of the flexible region '\n",
      "            '(brighter area), keeping the rest of the image fixed. This '\n",
      "            'flexible point-based manipulation enables control of many spatial '\n",
      "            'attributes like pose, shape, expression, and layout across '\n",
      "            'diverse object categories.',\n",
      " 'summary_till_now': 'No summary yet, this is the first fragment.\\n'\n",
      "                     '\\n'\n",
      "                     'Current fragment:\\n'\n",
      "                     'The current fragment discusses a new approach called '\n",
      "                     'DragGAN that allows users to manipulate GAN-generated '\n",
      "                     'images by \"dragging\" any points of the image to '\n",
      "                     'precisely reach target points in a user-interactive '\n",
      "                     'manner. This approach enables control over spatial '\n",
      "                     'attributes such as pose, shape, expression, and layout '\n",
      "                     'across diverse object categories. DragGAN consists of '\n",
      "                     'two main components: a feature-based motion supervision '\n",
      "                     'and a new point tracking approach. Through DragGAN, '\n",
      "                     'anyone can deform an image with precise control over '\n",
      "                     'where pixels go, producing realistic outputs even for '\n",
      "                     'challenging scenarios. The article also showcases the '\n",
      "                     'manipulation of real images through GAN inversion.',\n",
      " 'title': 'Drag Your GAN: Interactive Point-based Manipulation on the '\n",
      "          'Generative Image Manifold'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|████████████████▉                                                                                                                                       | 2/18 [00:33<04:21, 16.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'questions_and_answers': 'Q: What are the main components of DragGAN?\\n'\n",
      "                          'A: DragGAN consists of two major components: a '\n",
      "                          'feature-based motion supervision and a new point '\n",
      "                          'tracking approach.\\n'\n",
      "                          '\\n'\n",
      "                          'Q: How does DragGAN allow users to control diverse '\n",
      "                          'spatial attributes?\\n'\n",
      "                          'A: DragGAN allows users to click any number of '\n",
      "                          'handle points and target points on the image and '\n",
      "                          'the goal is to drive the handle points to reach '\n",
      "                          'their corresponding target points.\\n'\n",
      "                          '\\n'\n",
      "                          'Q: How does DragGAN allow users to control '\n",
      "                          'different spatial attributes?\\n'\n",
      "                          'A: DragGAN allows users to control diverse spatial '\n",
      "                          'attributes including position, pose, shape, '\n",
      "                          'expression, and layout of the generated objects or '\n",
      "                          'animals.\\n'\n",
      "                          '\\n'\n",
      "                          'Q: How does DragGAN achieve control over spatial '\n",
      "                          'attributes with high precision?\\n'\n",
      "                          'A: DragGAN allows users to click any number of '\n",
      "                          'handle points and target points on the image and '\n",
      "                          'the goal is to drive the handle points to reach '\n",
      "                          'their corresponding target points.',\n",
      " 'summary': 'The DragGAN approach allows users to click any number of handle '\n",
      "            'points and target points on an image, and the goal is to drive '\n",
      "            'the handle points to reach their corresponding target points. '\n",
      "            'This approach provides control over diverse spatial attributes '\n",
      "            'and is agnostic to object categories. DragGAN consists of two '\n",
      "            'main components: a feature-based motion supervision and a new '\n",
      "            'point tracking approach. Through DragGAN, anyone can deform an '\n",
      "            'image with precise control over where pixels go, producing '\n",
      "            'realistic outputs even for challenging scenarios. The approach '\n",
      "            'with the closest setting to ours is \"text-guided image '\n",
      "            'synthesis.\"',\n",
      " 'summary_till_now': 'There is no summary till now as this is the first '\n",
      "                     'fragment of the larger document. The current fragment '\n",
      "                     'discusses a new approach called DragGAN, which enables '\n",
      "                     'users to manipulate GAN-generated images by \"dragging\" '\n",
      "                     'any points of the image to reach target points in a '\n",
      "                     'precise and user-interactive manner. This approach '\n",
      "                     'allows for control over spatial attributes such as pose, '\n",
      "                     'shape, expression, and layout across diverse object '\n",
      "                     'categories. DragGAN consists of two main components: a '\n",
      "                     'feature-based motion supervision and a new point '\n",
      "                     'tracking approach. The article also showcases the '\n",
      "                     'manipulation of real images through GAN inversion.',\n",
      " 'title': '\"DragGAN: Interactive Point-based Manipulation on the Generative '\n",
      "          'Image Manifold\"'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|█████████████████████████▎                                                                                                                              | 3/18 [01:36<09:25, 37.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'questions_and_answers': '\\n'\n",
      "                          'Q1. How does DragGAN achieve precise point '\n",
      "                          'tracking?\\n'\n",
      "                          'A1. DragGAN achieves precise point tracking by '\n",
      "                          'optimizing a shifted feature patch loss to optimize '\n",
      "                          'the latent code. Nearest neighbor search is then '\n",
      "                          'used to track the handle points by shifting them '\n",
      "                          'closer to the target points.\\n'\n",
      "                          '\\n'\n",
      "                          '\\n'\n",
      "                          'Q2. How does DragGAN achieve efficient '\n",
      "                          'manipulation?\\n'\n",
      "                          'A2. DragGAN achieves efficient manipulation by '\n",
      "                          'using a single RTX 3090 GPU for most cases, '\n",
      "                          'allowing for live, interactive editing sessions.\\n'\n",
      "                          '\\n'\n",
      "                          '\\n'\n",
      "                          'Q3. How does DragGAN achieve diverse manipulation '\n",
      "                          'effects?\\n'\n",
      "                          'A3. DragGAN achieves diverse manipulation effects '\n",
      "                          'by allowing users to control diverse spatial '\n",
      "                          'attributes and by deforming the image on the '\n",
      "                          'learned manifold of a GAN, which tends to obey the '\n",
      "                          'underlying object structures.\\n'\n",
      "                          '\\n'\n",
      "                          '\\n'\n",
      "                          'Q4. How does DragGAN handle more than one point?\\n'\n",
      "                          'A4. DragGAN handles more than one point by using a '\n",
      "                          'shifted feature patch loss that optimizes the '\n",
      "                          'latent code for each handle point.\\n'\n",
      "                          '\\n'\n",
      "                          '\\n'\n",
      "                          'Q5. How does DragGAN compare to '\n",
      "                          'UserControllableLT?\\n'\n",
      "                          'A5. DragGAN outperforms UserControllableLT by '\n",
      "                          'handling more than one point with precise position '\n",
      "                          'control and by allowing users to optionally draw a '\n",
      "                          'region of interest to perform region-specific '\n",
      "                          'editing.',\n",
      " 'summary': 'DragGAN is a new technique for interactive point-based '\n",
      "            'manipulation of GAN-generated images, allowing users to click any '\n",
      "            'number of handle points and target points on an image and '\n",
      "            'precisely control the spatial attributes of the image. It '\n",
      "            'consists of two main components: a feature-based motion '\n",
      "            'supervision and a new point tracking approach. The technique is '\n",
      "            'built on the key insight that the feature space of a GAN is '\n",
      "            'sufficiently discriminative to enable both motion supervision and '\n",
      "            'precise point tracking. DragGAN allows users to optionally draw a '\n",
      "            'region of interest in an image to perform region-specific '\n",
      "            'editing. It is built on GAN inversion techniques and outperforms '\n",
      "            'existing point tracking approaches for GAN-generated frames.',\n",
      " 'summary_till_now': 'There is no summary till now as this is the first '\n",
      "                     'fragment of the larger document. \\n'\n",
      "                     '\\n'\n",
      "                     'Current fragment:\\n'\n",
      "                     '\\n'\n",
      "                     'The current fragment discusses a new approach called '\n",
      "                     'DragGAN, which enables users to manipulate GAN-generated '\n",
      "                     'images by \"dragging\" any points of the image to reach '\n",
      "                     'target points in a precise and user-interactive manner. '\n",
      "                     'This approach allows for control over spatial attributes '\n",
      "                     'such as pose, shape, expression, and layout across '\n",
      "                     'diverse object categories. DragGAN consists of two main '\n",
      "                     'components: a feature-based motion supervision and a new '\n",
      "                     'point tracking approach. The article also showcases the '\n",
      "                     'manipulation of real images through GAN inversion.\\n'\n",
      "                     '\\n'\n",
      "                     'To achieve interactive point-based manipulation, DragGAN '\n",
      "                     'addresses two sub-problems, including supervising the '\n",
      "                     'handle points to move towards the targets and tracking '\n",
      "                     'the handle points so that their positions are known at '\n",
      "                     'each editing step. The technique is built on the key '\n",
      "                     'insight that the feature space of a GAN is sufficiently '\n",
      "                     'discriminative to enable both motion supervision and '\n",
      "                     'precise point tracking. The motion supervision is '\n",
      "                     'achieved via a shifted feature patch loss that optimizes '\n",
      "                     'the latent code. Each optimization step leads to the '\n",
      "                     'handle points shifting closer to the targets; thus point '\n",
      "                     'tracking is then performed through nearest neighbor '\n",
      "                     'search in the feature space. This optimization process '\n",
      "                     'is repeated until the handle points reach the targets. '\n",
      "                     'DragGAN also allows users to optionally draw a region of '\n",
      "                     'interest to perform region-specific editing. \\n'\n",
      "                     '\\n'\n",
      "                     'The article also compares DragGAN with '\n",
      "                     'UserControllableLT, which also studies dragging-based '\n",
      "                     'manipulation. However, DragGAN handles more than one '\n",
      "                     'point with precise position control, which enables much '\n",
      "                     'more diverse and accurate image manipulation. The '\n",
      "                     'article showcases the evaluation of DragGAN on diverse '\n",
      "                     'datasets including animals (lions, dogs, cats, and '\n",
      "                     'horses), humans (face and whole body), cars, and '\n",
      "                     'landscapes. The approach effectively moves the '\n",
      "                     'user-defined handle points to the target points, '\n",
      "                     'achieving diverse manipulation effects across many '\n",
      "                     'object categories. \\n'\n",
      "                     '\\n'\n",
      "                     'In conclusion, DragGAN is a powerful tool for real image '\n",
      "                     'editing that allows users to manipulate GAN-generated '\n",
      "                     'images in a precise and user-interactive manner by '\n",
      "                     'dragging any points of the image to reach target points. '\n",
      "                     'It consists of two main components, feature-based motion '\n",
      "                     'supervision and a new point tracking approach, and can '\n",
      "                     'handle more than one point with precise position '\n",
      "                     'control, enabling much more diverse and accurate image '\n",
      "                     'manipulation.',\n",
      " 'title': 'DragGAN: Efficient Interactive Point-Based Manipulation for '\n",
      "          'GAN-Generated Images'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|█████████████████████████████████▊                                                                                                                      | 4/18 [02:38<11:04, 47.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'questions_and_answers': 'Question 1\\n'\n",
      "                          'What is DragGAN?\\n'\n",
      "                          '\\n'\n",
      "                          'Answer 1\\n'\n",
      "                          'DragGAN is a new approach that allows users to '\n",
      "                          'manipulate GAN-generated images by \"dragging\" any '\n",
      "                          'points of the image to reach target points in a '\n",
      "                          'precise and user-interactive manner. It consists of '\n",
      "                          'two main components, feature-based motion '\n",
      "                          'supervision and a new point tracking approach, and '\n",
      "                          'can handle more than one point with precise '\n",
      "                          'position control, enabling much more diverse and '\n",
      "                          'accurate image manipulation.\\n'\n",
      "                          '\\n'\n",
      "                          'Question 2\\n'\n",
      "                          'How does DragGAN enable precise and '\n",
      "                          'user-interactive manipulation?\\n'\n",
      "                          '\\n'\n",
      "                          'Answer 2\\n'\n",
      "                          'DragGAN addresses two sub-problems, including '\n",
      "                          'supervising the handle points to move towards the '\n",
      "                          'targets and tracking the handle points so that '\n",
      "                          'their positions are known at each editing step. The '\n",
      "                          'technique is built on the key insight that the '\n",
      "                          'feature space of a GAN is sufficiently '\n",
      "                          'discriminative to enable both motion supervision '\n",
      "                          'and precise point tracking. The motion supervision '\n",
      "                          'is achieved via a shifted feature patch loss that '\n",
      "                          'optimizes the latent code. Each optimization step '\n",
      "                          'leads to the handle points shifting closer to the '\n",
      "                          'targets; thus point tracking is then performed '\n",
      "                          'through nearest neighbor search in the feature '\n",
      "                          'space. This optimization process is repeated until '\n",
      "                          'the handle points reach the targets.\\n'\n",
      "                          '\\n'\n",
      "                          'Question 3\\n'\n",
      "                          'What are the limitations of UserControllableLT, a '\n",
      "                          'competing approach?\\n'\n",
      "                          '\\n'\n",
      "                          'Answer 3\\n'\n",
      "                          'UserControllableLT only supports dragging a single '\n",
      "                          \"point, and the user's target point is not always \"\n",
      "                          'reached after editing. In contrast, DragGAN can '\n",
      "                          'handle multiple points with precise position '\n",
      "                          'control, enabling much more diverse and accurate '\n",
      "                          'image manipulation.',\n",
      " 'summary': 'The article discusses a new approach called DragGAN, which '\n",
      "            'enables users to manipulate GAN-generated images by \"dragging\" '\n",
      "            'any points of the image to reach target points in a precise and '\n",
      "            'user-interactive manner. DragGAN consists of two main components: '\n",
      "            'a feature-based motion supervision and a new point tracking '\n",
      "            'approach. The motion supervision is achieved via a shifted '\n",
      "            'feature patch loss that optimizes the latent code. Each '\n",
      "            'optimization step leads to the handle points shifting closer to '\n",
      "            'the targets; thus point tracking is then performed through '\n",
      "            'nearest neighbor search in the feature space. This optimization '\n",
      "            'process is repeated until the handle points reach the targets. '\n",
      "            'DragGAN also allows users to optionally draw a region of interest '\n",
      "            'to perform region-specific editing.',\n",
      " 'summary_till_now': 'There is no summary till now as this is the first '\n",
      "                     'fragment of the larger document.\\n'\n",
      "                     '\\n'\n",
      "                     'current fragment summary:\\n'\n",
      "                     '\\n'\n",
      "                     'The current fragment discusses a new approach called '\n",
      "                     'DragGAN, which enables users to manipulate GAN-generated '\n",
      "                     'images by \"dragging\" any points of the image to reach '\n",
      "                     'target points in a precise and user-interactive manner. '\n",
      "                     'This approach allows for control over spatial attributes '\n",
      "                     'such as pose, shape, expression, and layout across '\n",
      "                     'diverse object categories. DragGAN consists of two main '\n",
      "                     'components: a feature-based motion supervision and a new '\n",
      "                     'point tracking approach. The article also showcases the '\n",
      "                     'manipulation of real images through GAN inversion.\\n'\n",
      "                     '\\n'\n",
      "                     'To achieve interactive point-based manipulation, DragGAN '\n",
      "                     'addresses two sub-problems, including supervising the '\n",
      "                     'handle points to move towards the targets and tracking '\n",
      "                     'the handle points so that their positions are known at '\n",
      "                     'each editing step. The technique is built on the key '\n",
      "                     'insight that the feature space of a GAN is sufficiently '\n",
      "                     'discriminative to enable both motion supervision and '\n",
      "                     'precise point tracking. The motion supervision is '\n",
      "                     'achieved via a shifted feature patch loss that optimizes '\n",
      "                     'the latent code. Each optimization step leads to the '\n",
      "                     'handle points shifting closer to the targets; thus point '\n",
      "                     'tracking is then performed through nearest neighbor '\n",
      "                     'search in the feature space. This optimization process '\n",
      "                     'is repeated until the handle points reach the targets. '\n",
      "                     'DragGAN also allows users to optionally draw a region of '\n",
      "                     'interest to perform region-specific editing.\\n'\n",
      "                     '\\n'\n",
      "                     'The article also compares DragGAN with '\n",
      "                     'UserControllableLT, which also studies dragging-based '\n",
      "                     'manipulation. However, DragGAN handles more than one '\n",
      "                     'point with precise position control, which enables much '\n",
      "                     'more diverse and accurate image manipulation. The '\n",
      "                     'article showcases the evaluation of DragGAN on diverse '\n",
      "                     'datasets including animals (lions, dogs, cats, and '\n",
      "                     'horses), humans (face and whole body), cars, and '\n",
      "                     'landscapes. The approach effectively moves the '\n",
      "                     'user-defined handle points to the target points, '\n",
      "                     'achieving diverse manipulation effects across many '\n",
      "                     'object categories.\\n'\n",
      "                     '\\n'\n",
      "                     'In conclusion, DragGAN is a powerful tool for real image '\n",
      "                     'editing that allows users to manipulate GAN-generated '\n",
      "                     'images in a precise and user-interactive manner by '\n",
      "                     'dragging any points of the image to reach target points. '\n",
      "                     'It consists of two main components, feature-based motion '\n",
      "                     'supervision and a new point tracking approach, and can '\n",
      "                     'handle more than one point with precise position '\n",
      "                     'control, enabling much more diverse and accurate image '\n",
      "                     'manipulation.',\n",
      " 'title': 'DragGAN: Interactive Point-based Manipulation on the Generative '\n",
      "          'Image Manifold'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|██████████████████████████████████████████▏                                                                                                             | 5/18 [02:59<08:12, 37.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'questions_and_answers': 'Q: How does DragGAN enable users to manipulate '\n",
      "                          'GAN-generated images?\\n'\n",
      "                          '\\n'\n",
      "                          'A: DragGAN allows users to manipulate GAN-generated '\n",
      "                          'images by \"dragging\" any points of the image to '\n",
      "                          'reach target points in a precise and '\n",
      "                          'user-interactive manner. This approach allows for '\n",
      "                          'control over spatial attributes such as pose, '\n",
      "                          'shape, expression, and layout across diverse object '\n",
      "                          'categories.\\n'\n",
      "                          '\\n'\n",
      "                          'Q: What enables DragGAN to perform interactive '\n",
      "                          'point-based manipulation?\\n'\n",
      "                          '\\n'\n",
      "                          'A: DragGAN achieves interactive point-based '\n",
      "                          'manipulation by addressing two sub-problems, '\n",
      "                          'including supervising the handle points to move '\n",
      "                          'towards the targets and tracking the handle points '\n",
      "                          'so that their positions are known at each editing '\n",
      "                          'step. The technique is built on the key insight '\n",
      "                          'that the feature space of a GAN is sufficiently '\n",
      "                          'discriminative to enable both motion supervision '\n",
      "                          'and precise point tracking.\\n'\n",
      "                          '\\n'\n",
      "                          'Q: How does DragGAN compare to UserControllableLT, '\n",
      "                          'which also studies dragging-based manipulation?\\n'\n",
      "                          '\\n'\n",
      "                          'A: DragGAN handles more than one point with precise '\n",
      "                          'position control, which enables much more diverse '\n",
      "                          'and accurate image manipulation.',\n",
      " 'summary': 'The article discusses a new approach called DragGAN, which '\n",
      "            'enables users to manipulate GAN-generated images by \"dragging\" '\n",
      "            'any points of the image to reach target points in a precise and '\n",
      "            'user-interactive manner. The approach is based on two key '\n",
      "            'insights: that the feature space of a GAN is sufficiently '\n",
      "            'discriminative to enable motion supervision and precise point '\n",
      "            'tracking, and that tracking can be performed through nearest '\n",
      "            'neighbor search in the feature space. The motion supervision is '\n",
      "            'achieved via a shifted feature patch loss that optimizes the '\n",
      "            'latent code. Each optimization step leads to the handle points '\n",
      "            'shifting closer to the targets. The approach also allows users to '\n",
      "            'optionally draw a region of interest to perform region-specific '\n",
      "            'editing.',\n",
      " 'summary_till_now': 'There is no summary till now as this is the first '\n",
      "                     'fragment of the larger document.\\n'\n",
      "                     '\\n'\n",
      "                     'current fragment summary:\\n'\n",
      "                     '\\n'\n",
      "                     'The current fragment discusses different approaches to '\n",
      "                     'enable 3D control through GANs and image synthesis '\n",
      "                     'through diffusion models. It also covers point tracking '\n",
      "                     'methods and introduces a new approach called DragGAN, '\n",
      "                     'which allows users to manipulate GAN-generated images by '\n",
      "                     'dragging points to reach target points in a precise and '\n",
      "                     'user-interactive manner. The technique is built on the '\n",
      "                     'key insight that the feature space of a GAN is '\n",
      "                     'sufficiently discriminative to enable both motion '\n",
      "                     'supervision and precise point tracking. The article also '\n",
      "                     'showcases the evaluation of DragGAN on diverse datasets '\n",
      "                     'including animals, humans, cars, and landscapes. In '\n",
      "                     'conclusion, DragGAN is a powerful tool for real image '\n",
      "                     'editing that allows for precise and user-interactive '\n",
      "                     'manipulation.',\n",
      " 'title': 'DragGAN: Interactive Image Manipulation for GANs'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|██████████████████████████████████████████████████▋                                                                                                     | 6/18 [03:40<07:47, 38.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'questions_and_answers': 'What is StyleGAN2 architecture?\\n'\n",
      "                          'StyleGAN2 is an architecture for Generative '\n",
      "                          'Adversarial Network (GAN) image synthesis.\\n'\n",
      "                          '\\n'\n",
      "                          'What is StyleGAN2 terminology?\\n'\n",
      "                          'In the StyleGAN2 architecture, a latent code A   ∈ '\n",
      "                          'N (0, A   ) is mapped to an intermediate latent '\n",
      "                          'code A   ∈ R512 via a mapping network. The space of '\n",
      "                          'A    is referred to as W.A    is then sent to the '\n",
      "                          'generator A    to produce the output image I = A   '\n",
      "                          '(A   ).\\n'\n",
      "                          '\\n'\n",
      "                          'What approach is used to move the object in the '\n",
      "                          'image such that the semantic positions (e.g., the '\n",
      "                          'nose and the jaw) of the handle points reach their '\n",
      "                          'corresponding target points?\\n'\n",
      "                          'The approach used is to perform image manipulation '\n",
      "                          'in an optimization manner. Each optimization step '\n",
      "                          'consists of two sub-steps, including 1) motion '\n",
      "                          'supervision and 2) point tracking. In motion '\n",
      "                          'supervision, a loss that enforces handle points to '\n",
      "                          'move towards target points is used to optimize the '\n",
      "                          'latent code A   . After one optimization step, we '\n",
      "                          'get a new latent code A   ′ and a new image.',\n",
      " 'summary': 'StyleGAN2 architecture [Karras et al. 2020]. Here we briefly '\n",
      "            'introduce the basics of this architecture. The StyleGAN2 '\n",
      "            'architecture uses an intermediate latent code A ∈ R512 to control '\n",
      "            'the different levels of attributes in the image. The input isA ∈ '\n",
      "            'RA 512 = W+, where A is the number of layers. This less '\n",
      "            'constrained W+ space is more expressive and models an image '\n",
      "            'manifold [Zhu et al. 2016]. The generator A learns a mapping from '\n",
      "            'a low-dimensional latent space to a much higher dimensional image '\n",
      "            'space.',\n",
      " 'summary_till_now': 'There is no summary till now as this is the first '\n",
      "                     'fragment of the larger document.\\n'\n",
      "                     '\\n'\n",
      "                     'current fragment summary:\\n'\n",
      "                     '\\n'\n",
      "                     'The current fragment introduces the StyleGAN2 '\n",
      "                     'architecture and its terminology. A 512 dimensional '\n",
      "                     'latent code is mapped to an intermediate latent code via '\n",
      "                     'a mapping network, which is then sent to the generator '\n",
      "                     'to produce the output image. The generator learns a '\n",
      "                     'mapping from a low-dimensional latent space to a much '\n",
      "                     'higher dimensional image space, modeling an image '\n",
      "                     'manifold. The fragment also discusses an image '\n",
      "                     'manipulation pipeline called DragGAN, which allows users '\n",
      "                     'to manipulate GAN-generated images by dragging points to '\n",
      "                     'reach target points in a precise and user-interactive '\n",
      "                     'manner. The technique is built on the key insight that '\n",
      "                     'the feature space of a GAN is sufficiently '\n",
      "                     'discriminative to enable both motion supervision and '\n",
      "                     'precise point tracking. The article also showcases the '\n",
      "                     'evaluation of DragGAN on diverse datasets including '\n",
      "                     'animals, humans, cars, and landscapes. \\n'\n",
      "                     '\\n'\n",
      "                     'Overall summary till now:\\n'\n",
      "                     '\\n'\n",
      "                     'The document starts with a fragment discussing different '\n",
      "                     'approaches to enable 3D control through GANs and image '\n",
      "                     'synthesis through diffusion models. It then covers point '\n",
      "                     'tracking methods and introduces DragGAN, a new approach '\n",
      "                     'that allows for precise and user-interactive '\n",
      "                     'manipulation of GAN-generated images. The current '\n",
      "                     'fragment introduces the StyleGAN2 architecture and its '\n",
      "                     'terminology and discusses the image manipulation '\n",
      "                     'pipeline called DragGAN. The article showcases the '\n",
      "                     'evaluation of DragGAN on diverse datasets including '\n",
      "                     'animals, humans, cars, and landscapes. In conclusion, '\n",
      "                     'DragGAN is a powerful tool for real image editing that '\n",
      "                     'allows for precise and user-interactive manipulation.',\n",
      " 'title': '\"DragGAN: A User-Interactive Approach for Precise Point '\n",
      "          'Manipulation in GAN-Generated Images\"'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 39%|███████████████████████████████████████████████████████████                                                                                             | 7/18 [04:24<07:26, 40.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'questions_and_answers': \"Q1. What is the key insight behind DragGAN's motion \"\n",
      "                          'supervision?\\n'\n",
      "                          'The key insight is that intermediate features of '\n",
      "                          'the generator are discriminative enough to '\n",
      "                          'accurately supervise motion.\\n'\n",
      "                          '\\n'\n",
      "                          \"Q2. What type of losses are used in DragGAN's \"\n",
      "                          'motion supervision?\\n'\n",
      "                          'A shifted patch loss is used.\\n'\n",
      "                          '\\n'\n",
      "                          \"Q3. What type of losses are used in DragGAN's point \"\n",
      "                          'tracking?\\n'\n",
      "                          'A nearest neighbor loss is used.',\n",
      " 'summary': 'DragGAN is a technique that allows users to manipulate '\n",
      "            'GAN-generated images by dragging points to reach target points in '\n",
      "            'a precise and user-interactive manner. The technique is built on '\n",
      "            'the key insight that the feature space of a GAN is sufficiently '\n",
      "            'discriminative to enable both motion supervision and precise '\n",
      "            'point tracking. The pipeline consists of an optimization process '\n",
      "            'that is performed in an iterative manner, with each optimization '\n",
      "            'step consisting of two sub-steps: motion supervision and point '\n",
      "            'tracking. The motion supervision step uses a loss that forces the '\n",
      "            'handle points to move towards the targeted points by a small '\n",
      "            'step, while the point tracking step updates the positions of the '\n",
      "            'handle points to track the corresponding points on the object. '\n",
      "            'This optimization process continues until the handle points reach '\n",
      "            'the position of the target points, which usually takes 30-200 '\n",
      "            'iterations. The user can also stop the optimization at any '\n",
      "            'intermediate step. After editing, the user can input new handle '\n",
      "            'and target points and continue editing until satisfied with the '\n",
      "            'results.',\n",
      " 'summary_till_now': 'The document discusses different approaches to enable 3D '\n",
      "                     'control through GANs and image synthesis through '\n",
      "                     'diffusion models. It covers point tracking methods and '\n",
      "                     'introduces DragGAN, a new approach that allows for '\n",
      "                     'precise and user-interactive manipulation of '\n",
      "                     'GAN-generated images. The current fragment introduces '\n",
      "                     'the StyleGAN2 architecture and its terminology and '\n",
      "                     'discusses the image manipulation pipeline called '\n",
      "                     'DragGAN. The article showcases the evaluation of DragGAN '\n",
      "                     'on diverse datasets including animals, humans, cars, and '\n",
      "                     'landscapes. In conclusion, DragGAN is a powerful tool '\n",
      "                     'for real image editing that allows for precise and '\n",
      "                     'user-interactive manipulation.\\n'\n",
      "                     '\\n'\n",
      "                     'Current fragment summary:\\n'\n",
      "                     '\\n'\n",
      "                     'The current fragment explains the optimization process '\n",
      "                     \"used in DragGAN's image manipulation pipeline. It \"\n",
      "                     'consists of two sub-steps: motion supervision and point '\n",
      "                     'tracking. In motion supervision, a loss is used to '\n",
      "                     'optimize the latent code by enforcing handle points to '\n",
      "                     'move towards target points. After one optimization step, '\n",
      "                     'a new latent code and a new image are generated, causing '\n",
      "                     'a slight movement of the object in the image. The exact '\n",
      "                     'length of the step is unclear as it is subject to '\n",
      "                     'complex optimization dynamics and varies for different '\n",
      "                     'objects and parts. The positions of the handle points '\n",
      "                     'are then updated to track the corresponding points on '\n",
      "                     'the object. This tracking process is necessary to avoid '\n",
      "                     'incorrect supervision in the next motion supervision '\n",
      "                     'step. The optimization process continues until the '\n",
      "                     'handle points reach the position of the target points, '\n",
      "                     'which usually takes 30-200 iterations in their '\n",
      "                     'experiments. Users can also stop the optimization at any '\n",
      "                     'intermediate step. The fragment also introduces a motion '\n",
      "                     'supervision loss that does not rely on any additional '\n",
      "                     'neural networks and uses feature maps to supervise '\n",
      "                     'motion.',\n",
      " 'title': 'DragGAN: A User-Interactive Image Editing Pipeline based on GANs'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|███████████████████████████████████████████████████████████████████▌                                                                                    | 8/18 [04:59<06:25, 38.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'questions_and_answers': '1. How does the motion supervision loss in DragGAN '\n",
      "                          'work?\\n'\n",
      "                          'A. The loss uses feature maps to supervise motion '\n",
      "                          'and does not rely on any additional neural '\n",
      "                          'networks. It enforces handle points to move towards '\n",
      "                          'target points by optimizing a latent code.\\n'\n",
      "                          \"2. What is the role of point tracking in DragGAN's \"\n",
      "                          'image manipulation pipeline?\\n'\n",
      "                          'A. Point tracking allows for precise and '\n",
      "                          'user-interactive manipulation of GAN-generated '\n",
      "                          'images by updating the handle points to track the '\n",
      "                          'corresponding points on the object.\\n'\n",
      "                          '3. What is the optimization process used in '\n",
      "                          \"DragGAN's image manipulation pipeline?\\n\"\n",
      "                          'A. The optimization process consists of motion '\n",
      "                          'supervision and point tracking. In motion '\n",
      "                          'supervision, a loss is used to optimize the latent '\n",
      "                          'code by enforcing handle points to move towards '\n",
      "                          'target points. In point tracking, the positions of '\n",
      "                          'the handle points are then updated to track the '\n",
      "                          'corresponding points on the object.',\n",
      " 'summary': 'The current fragment explains the optimization process used in '\n",
      "            \"DragGAN's image manipulation pipeline. It consists of two \"\n",
      "            'sub-steps: motion supervision and point tracking. In motion '\n",
      "            'supervision, a loss is used to optimize the latent code by '\n",
      "            'enforcing handle points to move towards target points. After one '\n",
      "            'optimization step, a new latent code and a new image are '\n",
      "            'generated, causing a slight movement of the object in the image. '\n",
      "            'The exact length of the step is unclear as it is subject to '\n",
      "            'complex optimization dynamics and varies for different objects '\n",
      "            'and parts. The positions of the handle points are then updated to '\n",
      "            'track the corresponding points on the object. This tracking '\n",
      "            'process is necessary to avoid incorrect supervision in the next '\n",
      "            'motion supervision step. The optimization process continues until '\n",
      "            'the handle points reach the position of the target points, which '\n",
      "            'usually takes 30-200 iterations in their experiments. Users can '\n",
      "            'also stop the optimization at any intermediate step. The fragment '\n",
      "            'also introduces a motion supervision loss that does not rely on '\n",
      "            'any additional neural networks and uses feature maps to supervise '\n",
      "            'motion.',\n",
      " 'summary_till_now': 'The document discusses different approaches to 3D '\n",
      "                     'control through GANs and introduces DragGAN, an image '\n",
      "                     'manipulation pipeline that allows for precise and '\n",
      "                     'user-interactive editing of GAN-generated images. The '\n",
      "                     'current fragment explains the optimization process used '\n",
      "                     \"in DragGAN's image manipulation pipeline, which consists \"\n",
      "                     'of two sub-steps: motion supervision and point '\n",
      "                     'tracking. \\n'\n",
      "                     '\\n'\n",
      "                     'In motion supervision, a loss is used to optimize the '\n",
      "                     'latent code by enforcing handle points to move towards '\n",
      "                     'target points. After one optimization step, a new latent '\n",
      "                     'code and a new image are generated, causing a slight '\n",
      "                     'movement of the object in the image. The exact length of '\n",
      "                     'the step is unclear as it is subject to complex '\n",
      "                     'optimization dynamics and varies for different objects '\n",
      "                     'and parts. The positions of the handle points are then '\n",
      "                     'updated to track the corresponding points on the '\n",
      "                     'object.\\n'\n",
      "                     '\\n'\n",
      "                     'This tracking process is necessary to avoid incorrect '\n",
      "                     'supervision in the next motion supervision step. The '\n",
      "                     'optimization process continues until the handle points '\n",
      "                     'reach the position of the target points, which usually '\n",
      "                     'takes 30-200 iterations in their experiments. Users can '\n",
      "                     'also stop the optimization at any intermediate step. The '\n",
      "                     'fragment also introduces a motion supervision loss that '\n",
      "                     'does not rely on any additional neural networks and uses '\n",
      "                     'feature maps to supervise motion. \\n'\n",
      "                     '\\n'\n",
      "                     'Overall, DragGAN is a powerful tool for real image '\n",
      "                     'editing that allows for precise and user-interactive '\n",
      "                     'manipulation, showcasing its evaluation on diverse '\n",
      "                     'datasets including animals, humans, cars, and '\n",
      "                     'landscapes.',\n",
      " 'title': '\\n'\n",
      "          'DragGAN: A User-Interactive Image Manipulation Framework for '\n",
      "          'GAN-Generated Images'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|████████████████████████████████████████████████████████████████████████████                                                                            | 9/18 [05:52<06:27, 43.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'questions_and_answers': 'Q1. How does DragGAN perform optimization for image '\n",
      "                          'manipulation?\\n'\n",
      "                          'A1. DragGAN performs optimization in two steps for '\n",
      "                          'image manipulation: motion supervision and point '\n",
      "                          'tracking. In motion supervision, a loss is used to '\n",
      "                          'optimize the latent code by enforcing handle points '\n",
      "                          'to move towards target points. After one '\n",
      "                          'optimization step, a new latent code and a new '\n",
      "                          'image are generated, causing a slight movement of '\n",
      "                          'the object in the image. The positions of the '\n",
      "                          'handle points are then updated to track the '\n",
      "                          'corresponding points on the object.\\n'\n",
      "                          '\\n'\n",
      "                          'Q2. How does DragGAN update the handle points to '\n",
      "                          'track an object?\\n'\n",
      "                          'A2. The tracked point is obtained by searching for '\n",
      "                          'the nearest neighbor of A   A    in Ω2(A   A   ,A   '\n",
      "                          '2): A   A    := arg min A   A    ∈Ω2(A   A   ,A   '\n",
      "                          '2) ∥F′(A   A   ) − A   A    ∥1. In this way, A   '\n",
      "                          'A    is updated to track the object.',\n",
      " 'summary': \"DragGAN's image manipulation pipeline uses two sub-steps: motion \"\n",
      "            'supervision and point tracking. In motion supervision, a loss is '\n",
      "            'used to optimize the latent code by enforcing handle points to '\n",
      "            'move closer to target points. After one optimization step, a new '\n",
      "            'latent code and a new image are generated, causing a slight '\n",
      "            'movement of the object in the image. The positions of the handle '\n",
      "            'points are then updated to track the corresponding points on the '\n",
      "            'object. The optimization process continues until the handle '\n",
      "            'points reach the position of the target points.',\n",
      " 'summary_till_now': 'The document discusses different approaches to 3D '\n",
      "                     'control through GANs and introduces DragGAN, an image '\n",
      "                     'manipulation pipeline that allows for precise and '\n",
      "                     'user-interactive editing of GAN-generated images. The '\n",
      "                     'document further explains the optimization process used '\n",
      "                     \"in DragGAN's image manipulation pipeline, which consists \"\n",
      "                     'of two sub-steps: motion supervision and point tracking. '\n",
      "                     'In motion supervision, a loss is used to optimize the '\n",
      "                     'latent code by enforcing handle points to move towards '\n",
      "                     'target points. After one optimization step, a new latent '\n",
      "                     'code and a new image are generated, causing a slight '\n",
      "                     'movement of the object in the image. The positions of '\n",
      "                     'the handle points are then updated to track the '\n",
      "                     'corresponding points on the object. This tracking '\n",
      "                     'process is necessary to avoid incorrect supervision in '\n",
      "                     'the next motion supervision step. The optimization '\n",
      "                     'process continues until the handle points reach the '\n",
      "                     'position of the target points, which usually takes '\n",
      "                     '30-200 iterations in their experiments. Users can also '\n",
      "                     'stop the optimization at any intermediate step. The '\n",
      "                     'fragment also introduces a motion supervision loss that '\n",
      "                     'does not rely on any additional neural networks and uses '\n",
      "                     'feature maps to supervise motion. \\n'\n",
      "                     '\\n'\n",
      "                     'The current fragment of the document discusses the '\n",
      "                     'implementation details of the DragGAN approach, which is '\n",
      "                     'based on PyTorch. The Adam optimizer is used with '\n",
      "                     'different step sizes for different datasets. The '\n",
      "                     'hyperparameters are set to be 𝛹��� = 20,𝐴���1 = 3,𝐴���2 '\n",
      "                     '= 12. The optimization process is stopped when all the '\n",
      "                     'handle points are no more than 𝐴��� pixel away from '\n",
      "                     'their corresponding target points. A GUI is developed to '\n",
      "                     'support interactive image manipulation, and the '\n",
      "                     'computational efficiency of the approach allows users to '\n",
      "                     'wait for a few seconds for each edit and continue '\n",
      "                     'editing until satisfied. The fragment also explains that '\n",
      "                     'the discriminative features of GANs can effectively '\n",
      "                     'perform tracking via nearest neighbor search in a '\n",
      "                     'feature patch. The fragment concludes by mentioning that '\n",
      "                     'the evaluation of the DragGAN approach is conducted on '\n",
      "                     'diverse datasets, including animals, humans, cars, and '\n",
      "                     'landscapes.',\n",
      " 'title': '\"The Optimization Process in DragGAN\\'s Image Manipulation '\n",
      "          'Pipeline\"'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|███████████████████████████████████████████████████████████████████████████████████▉                                                                   | 10/18 [06:17<05:02, 37.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'questions_and_answers': '1) What datasets does DragGAN use when evaluating '\n",
      "                          'its approach?\\n'\n",
      "                          'The datasets include animals, humans, cars, and '\n",
      "                          'landscapes.\\n'\n",
      "                          '\\n'\n",
      "                          '2) How does the motion supervision loss work?\\n'\n",
      "                          'The motion supervision loss does not use any '\n",
      "                          'additional neural networks and uses feature maps to '\n",
      "                          'supervise motion.\\n'\n",
      "                          '\\n'\n",
      "                          '3) What are the hyperparameters used in the '\n",
      "                          'optimization process of the DragGAN approach?\\n'\n",
      "                          'The hyperparameters are set to be Ψ    = 20,A   1 = '\n",
      "                          '3,A   2 = 12.\\n'\n",
      "                          '\\n'\n",
      "                          '4) What optimizer is used in the implementation of '\n",
      "                          'DragGAN?\\n'\n",
      "                          'The Adam optimizer is used.',\n",
      " 'summary': 'The document fragment discusses the implementation of DragGAN, an '\n",
      "            'image manipulation pipeline that allows for precise and '\n",
      "            'user-interactive editing of GAN-generated images. The fragment '\n",
      "            \"explains the optimization process used in DragGAN's image \"\n",
      "            'manipulation pipeline, which consists of two sub-steps: motion '\n",
      "            'supervision and point tracking. The fragment discusses how the '\n",
      "            'optimization process works and introduces the hyperparameters and '\n",
      "            'details of the user-interactive interface. The fragment also '\n",
      "            'explains how the discriminative features of GANs can effectively '\n",
      "            'perform tracking. The fragment concludes by mentioning that the '\n",
      "            'evaluation of the DragGAN approach is conducted on diverse '\n",
      "            'datasets, including animals, humans, cars, and landscapes, and by '\n",
      "            'comparing to baseline methods.',\n",
      " 'summary_till_now': 'The document describes DragGAN, an image manipulation '\n",
      "                     'pipeline that allows for precise and user-interactive '\n",
      "                     'editing of GAN-generated images using motion supervision '\n",
      "                     'and point tracking optimization. The optimization '\n",
      "                     'process is stopped when all handle points are no more '\n",
      "                     \"than 'A' pixels away from their corresponding target \"\n",
      "                     'points. Evaluation of the DragGAN approach is conducted '\n",
      "                     'on diverse datasets, including animals, humans, cars, '\n",
      "                     'and landscapes.\\n'\n",
      "                     '\\n'\n",
      "                     'The current fragment of the document explains the '\n",
      "                     'implementation details of the DragGAN approach based on '\n",
      "                     'PyTorch, including the use of the Adam optimizer with '\n",
      "                     'different step sizes for different datasets and the '\n",
      "                     'setting of hyperparameters. The fragment also discusses '\n",
      "                     'the development of a GUI to support interactive image '\n",
      "                     'manipulation and the effectiveness of discriminative '\n",
      "                     'features of GANs in tracking via nearest neighbor search '\n",
      "                     'in a feature patch. The fragment concludes by providing '\n",
      "                     'a qualitative and quantitative evaluation of the DragGAN '\n",
      "                     \"approach's performance in comparison to other baseline \"\n",
      "                     'methods.',\n",
      " 'title': 'DragGAN: User-Interactive Image Manipulation with GAN Inversion'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 61%|████████████████████████████████████████████████████████████████████████████████████████████▎                                                          | 11/18 [06:46<04:03, 34.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'questions_and_answers': '1. How does DragGAN approach handle tracking of the '\n",
      "                          'handle points?\\n'\n",
      "                          'DragGAN approach uses PIPs and RAFT tracking to '\n",
      "                          'handle tracking of the handle points.\\n'\n",
      "                          '1. What is PIPs and RAFT tracking?\\n'\n",
      "                          'PIPs tracking uses point interpolation and path '\n",
      "                          'optimization to handle tracking of the handle '\n",
      "                          'points, while RAFT tracking uses random forest '\n",
      "                          'regressor to handle tracking of the handle points.\\n'\n",
      "                          '1. How does DragGAN approach handle tracking of the '\n",
      "                          'handle points?\\n'\n",
      "                          'DragGAN approach uses PIPs and RAFT tracking to '\n",
      "                          'handle tracking of the handle points.\\n'\n",
      "                          '1. How does DragGAN approach handle tracking of the '\n",
      "                          'handle points?\\n'\n",
      "                          'DragGAN approach uses PIPs and RAFT tracking to '\n",
      "                          'handle tracking of the handle points.',\n",
      " 'summary': 'The DragGAN approach allows for precise and user-interactive '\n",
      "            'editing of GAN-generated images using motion supervision and '\n",
      "            'point tracking optimization. The optimization process is stopped '\n",
      "            \"when all handle points are no more than 'A' pixels away from \"\n",
      "            'their corresponding target points. Evaluation of the DragGAN '\n",
      "            'approach is conducted on diverse datasets, including animals, '\n",
      "            'humans, cars, and landscapes. The DragGAN approach is implemented '\n",
      "            'using PyTorch, and the Adam optimizer with different step sizes '\n",
      "            'for different datasets is used. The effectiveness of '\n",
      "            'discriminative features of GANs in tracking via nearest neighbor '\n",
      "            'search in a feature patch is also demonstrated. The DragGAN '\n",
      "            'approach is evaluated quantitatively and qualitatively, and its '\n",
      "            'performance is found to be superior to other baseline methods.',\n",
      " 'summary_till_now': 'The document describes the DragGAN approach, which '\n",
      "                     'allows interactive and precise manipulation of '\n",
      "                     'GAN-generated images using motion supervision and point '\n",
      "                     'tracking optimization. The approach is evaluated on '\n",
      "                     'various datasets, including animals, humans, cars, and '\n",
      "                     'landscapes. \\n'\n",
      "                     '\\n'\n",
      "                     'In the current fragment of the document, the '\n",
      "                     'implementation details of the DragGAN approach are '\n",
      "                     'explained. The approach is based on PyTorch, and the '\n",
      "                     'Adam optimizer is used with different step sizes for '\n",
      "                     'different datasets. The fragment also discusses the '\n",
      "                     'development of a GUI to support interactive image '\n",
      "                     'manipulation and the effectiveness of discriminative '\n",
      "                     'features of GANs in tracking via nearest neighbor search '\n",
      "                     'in a feature patch. The optimization process is stopped '\n",
      "                     \"when all handle points are no more than 'A' pixels away \"\n",
      "                     'from their corresponding target points. \\n'\n",
      "                     '\\n'\n",
      "                     'The fragment concludes by providing a quantitative '\n",
      "                     \"evaluation of the DragGAN approach's performance in \"\n",
      "                     'comparison to other baseline methods. The approach is '\n",
      "                     'evaluated under two settings, including face landmark '\n",
      "                     'manipulation and paired image reconstruction. The '\n",
      "                     'results show that the DragGAN approach outperforms other '\n",
      "                     'methods in terms of mean distance (MD) scores for moving '\n",
      "                     'landmarks to target positions. The evaluation is '\n",
      "                     'performed under different numbers of landmarks, '\n",
      "                     'demonstrating the robustness of the approach.',\n",
      " 'title': 'DragGAN Implementation Details and Evaluation'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                  | 12/18 [07:23<03:33, 35.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'questions_and_answers': 'Q1. What type of datasets is used to evaluate the '\n",
      "                          'performance of DragGAN approach?\\n'\n",
      "                          'A1. The evaluation is performed under different '\n",
      "                          'numbers of landmarks to show the robustness of the '\n",
      "                          'approach under different numbers of handle points.\\n'\n",
      "                          '\\n'\n",
      "                          'Q2. How is the image quality evaluated for face '\n",
      "                          'landmark manipulation?\\n'\n",
      "                          'A2. The results are provided in the FID score '\n",
      "                          'between the edited images and the initial images.\\n'\n",
      "                          '\\n'\n",
      "                          'Q3. How does the mask function help to reduce '\n",
      "                          'ambiguity and keep certain regions fixed for face '\n",
      "                          'landmark manipulation?\\n'\n",
      "                          'A3. The mask function can help to reduce ambiguity '\n",
      "                          'and keep certain regions fixed by denoting the '\n",
      "                          'movable region, which allows only the regions in '\n",
      "                          'the mask to move while the remaining regions are '\n",
      "                          'fixed.',\n",
      " 'summary': 'The DragGAN approach is evaluated using two settings, including '\n",
      "            'face landmark manipulation and paired image reconstruction. The '\n",
      "            'results show that the DragGAN approach outperforms other methods '\n",
      "            'in terms of mean distance (MD) scores for moving landmarks to '\n",
      "            'target positions. The evaluation is performed under different '\n",
      "            'numbers of landmarks, demonstrating the robustness of the '\n",
      "            'approach.',\n",
      " 'summary_till_now': 'The document introduces the DragGAN approach, which '\n",
      "                     'allows interactive and precise manipulation of '\n",
      "                     'GAN-generated images using motion supervision and point '\n",
      "                     'tracking optimization. The approach is evaluated on '\n",
      "                     'various datasets, including animals, humans, cars, and '\n",
      "                     'landscapes. The implementation details of the DragGAN '\n",
      "                     'approach are explained in the current fragment of the '\n",
      "                     'document. The approach is based on PyTorch, and the Adam '\n",
      "                     'optimizer is used with different step sizes for '\n",
      "                     'different datasets. The fragment also discusses the '\n",
      "                     'development of a GUI to support interactive image '\n",
      "                     'manipulation and the effectiveness of discriminative '\n",
      "                     'features of GANs in tracking via nearest neighbor search '\n",
      "                     'in a feature patch. The optimization process is stopped '\n",
      "                     \"when all handle points are no more than 'A' pixels away \"\n",
      "                     'from their corresponding target points. \\n'\n",
      "                     '\\n'\n",
      "                     'In addition, the fragment provides a quantitative '\n",
      "                     \"evaluation of the DragGAN approach's performance in \"\n",
      "                     'comparison to other baseline methods. The approach is '\n",
      "                     'evaluated under two settings, including face landmark '\n",
      "                     'manipulation and paired image reconstruction. The '\n",
      "                     'results show that the DragGAN approach outperforms other '\n",
      "                     'methods in terms of mean distance (MD) scores for moving '\n",
      "                     'landmarks to target positions. The evaluation is '\n",
      "                     'performed under different numbers of landmarks, '\n",
      "                     'demonstrating the robustness of the approach. The '\n",
      "                     'fragment also discusses the use of a binary mask to '\n",
      "                     'denote the movable region to reduce ambiguity and keep '\n",
      "                     'certain regions fixed during manipulation. Finally, it '\n",
      "                     'highlights the ability of the DragGAN approach to create '\n",
      "                     '\"out-of-distribution\" manipulations, i.e., manipulations '\n",
      "                     'that lie outside the image distribution of the training '\n",
      "                     'dataset.',\n",
      " 'title': 'The DragGAN Approach for Interactive Image Manipulation using '\n",
      "          'Motion Supervision and Point Tracking'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████                                          | 13/18 [08:08<03:12, 38.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'questions_and_answers': 'Q1. What are the novel ingredients of the DragGAN '\n",
      "                          'approach?\\n'\n",
      "                          'A1. The two novel ingredients of the DragGAN '\n",
      "                          'approach are:\\n'\n",
      "                          '\\n'\n",
      "                          '1. An optimization of latent codes that '\n",
      "                          'incrementally moves multiple handle points towards '\n",
      "                          'their target locations, and\\n'\n",
      "                          '2. A point tracking procedure to faithfully trace '\n",
      "                          'the trajectory of the handle points.\\n'\n",
      "                          '\\n'\n",
      "                          'Q2. How does the DragGAN approach achieve '\n",
      "                          'pixel-precise image deformations?\\n'\n",
      "                          'A2. The DragGAN approach uses intermediate feature '\n",
      "                          'maps of the GAN to achieve pixel-precise image '\n",
      "                          'deformations. Specifically, the intermediate '\n",
      "                          'feature maps are used to identify and track the '\n",
      "                          'handle points, which are used to manipulate the '\n",
      "                          'image. Additionally, the optimization process of '\n",
      "                          'the DragGAN approach uses discriminative quality of '\n",
      "                          'intermediate feature maps to achieve pixel-precise '\n",
      "                          'image deformations.\\n'\n",
      "                          '\\n'\n",
      "                          'Q3. How does the DragGAN approach compare to '\n",
      "                          'previous methods?\\n'\n",
      "                          'A3. The DragGAN approach outperforms previous '\n",
      "                          'methods in terms of mean distance (MD) scores for '\n",
      "                          'moving landmarks to target positions. The '\n",
      "                          'evaluation is performed under different numbers of '\n",
      "                          'landmarks, demonstrating the robustness of the '\n",
      "                          'approach. Additionally, the DragGAN approach has '\n",
      "                          'extrapolation capability, creating images outside '\n",
      "                          'the training image distribution, e.g., an extremely '\n",
      "                          'opened mouth and a large wheel.\\n'\n",
      "                          '\\n'\n",
      "                          'Q4. How is the binary mask used in the DragGAN '\n",
      "                          'approach to reduce ambiguity and keep certain '\n",
      "                          'regions fixed during manipulation?\\n'\n",
      "                          'A4. The binary mask is used to mark which regions '\n",
      "                          'of the image can be manipulated, and which regions '\n",
      "                          'should be kept fixed. This helps to reduce '\n",
      "                          'ambiguity and keep certain regions of the image '\n",
      "                          'fixed, allowing for more precise image '\n",
      "                          'manipulation.',\n",
      " 'summary': 'The DragGAN approach uses pre-trained GANs to synthesize images '\n",
      "            'that stay on the manifold of realistic images. The approach uses '\n",
      "            'two novel ingredients: An optimization of latent codes that '\n",
      "            'incrementally moves multiple handle points towards their target '\n",
      "            'locations, and a point tracking procedure to faithfully trace the '\n",
      "            'trajectory of the handle points. The approach uses the '\n",
      "            'discriminative quality of intermediate feature maps of the GAN to '\n",
      "            'deform images pixel-precisely. The implementation details of the '\n",
      "            'DragGAN approach are explained in the current fragment of the '\n",
      "            'document. The approach outperforms other baseline methods under '\n",
      "            'two settings: face landmark manipulation and paired image '\n",
      "            'reconstruction. The approach is evaluated under different numbers '\n",
      "            'of landmarks, demonstrating the robustness of the approach. The '\n",
      "            'binary mask is used to control movable regions and reduce '\n",
      "            'ambiguity.',\n",
      " 'summary_till_now': 'The document introduces the DragGAN approach, which '\n",
      "                     'allows interactive and precise manipulation of '\n",
      "                     'GAN-generated images using motion supervision and point '\n",
      "                     'tracking optimization. The approach is evaluated on '\n",
      "                     'various datasets, including animals, humans, cars, and '\n",
      "                     'landscapes. The implementation details of the DragGAN '\n",
      "                     'approach are explained in the current fragment of the '\n",
      "                     'document, including the use of PyTorch and the Adam '\n",
      "                     'optimizer with different step sizes for different '\n",
      "                     'datasets. The fragment also discusses the development of '\n",
      "                     'a GUI to support interactive image manipulation and the '\n",
      "                     'effectiveness of discriminative features of GANs in '\n",
      "                     'tracking via nearest neighbor search in a feature patch. '\n",
      "                     'The optimization process is stopped when all handle '\n",
      "                     \"points are no more than 'A' pixels away from their \"\n",
      "                     'corresponding target points.\\n'\n",
      "                     '\\n'\n",
      "                     'The current fragment of the document discusses the use '\n",
      "                     'of a binary mask to denote the movable region to reduce '\n",
      "                     'ambiguity and keep certain regions fixed during '\n",
      "                     'manipulation. The fragment also highlights the ability '\n",
      "                     'of the DragGAN approach to create \"out-of-distribution\" '\n",
      "                     'manipulations, i.e., manipulations that lie outside the '\n",
      "                     'image distribution of the training dataset. The fragment '\n",
      "                     'provides a quantitative evaluation of the DragGAN '\n",
      "                     \"approach's performance in comparison to other baseline \"\n",
      "                     'methods, demonstrating its robustness in moving '\n",
      "                     'landmarks to target positions under different numbers of '\n",
      "                     'landmarks. Finally, the fragment emphasizes the '\n",
      "                     'importance of respecting personality rights and privacy '\n",
      "                     'regulations when using the DragGAN approach, as it could '\n",
      "                     'be misused to create images of a real person with a fake '\n",
      "                     'pose, expression, or shape. Overall, the DragGAN '\n",
      "                     'approach is presented as a general framework that does '\n",
      "                     'not rely on domain-specific modeling or auxiliary '\n",
      "                     'networks and opens new directions for powerful image '\n",
      "                     'editing using generative priors.',\n",
      " 'title': 'DragGAN: A General Framework for Intuitive Point-Based Image '\n",
      "          'Editing'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                 | 14/18 [08:44<02:31, 37.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'questions_and_answers': 'Q1. What is the DragGAN approach?\\n'\n",
      "                          'A1. The DragGAN approach allows interactive and '\n",
      "                          'precise manipulation of GAN-generated images using '\n",
      "                          'motion supervision and point tracking '\n",
      "                          'optimization.\\n'\n",
      "                          '\\n'\n",
      "                          'Q2. How does the DragGAN approach work?\\n'\n",
      "                          'A2. The DragGAN approach uses motion supervision '\n",
      "                          'and point tracking optimization to move the '\n",
      "                          'landmarks of an image to their corresponding target '\n",
      "                          'positions.\\n'\n",
      "                          '\\n'\n",
      "                          'Q3. How does the DragGAN approach compare to other '\n",
      "                          'baseline methods?\\n'\n",
      "                          'A3. The DragGAN approach outperforms other baseline '\n",
      "                          'methods in terms of robustness and performance.\\n'\n",
      "                          '\\n'\n",
      "                          'Q4. What are the potential risks associated with '\n",
      "                          'using the DragGAN approach?\\n'\n",
      "                          'A4. The DragGAN approach could be misused to create '\n",
      "                          'images of a real person with a fake pose, '\n",
      "                          'expression, or shape, which could potentially '\n",
      "                          'violate privacy regulations.',\n",
      " 'summary': 'The DragGAN approach, which allows interactive and precise '\n",
      "            'manipulation of GAN-generated images using motion supervision and '\n",
      "            'point tracking optimization, is evaluated on various datasets, '\n",
      "            'including animals, humans, cars, and landscapes. The '\n",
      "            'implementation details of the DragGAN approach are explained in '\n",
      "            'the current fragment of the document, including the use of '\n",
      "            'PyTorch and the Adam optimizer with different step sizes for '\n",
      "            'different datasets. The fragment also discusses the development '\n",
      "            'of a GUI to support interactive image manipulation and the '\n",
      "            'effectiveness of discriminative features of GANs in tracking via '\n",
      "            'nearest neighbor search in a feature patch. The optimization '\n",
      "            \"process is stopped when all handle points are no more than 'A' \"\n",
      "            'pixels away from their corresponding target points. The current '\n",
      "            'fragment of the document discusses the use of a binary mask to '\n",
      "            'denote the movable region to reduce ambiguity and keep certain '\n",
      "            'regions fixed during manipulation. The fragment also highlights '\n",
      "            'the ability of the DragGAN approach to create '\n",
      "            '\"out-of-distribution\" manipulations, i.e., manipulations that lie '\n",
      "            'outside the image distribution of the training dataset. The '\n",
      "            'fragment provides a quantitative evaluation of the DragGAN '\n",
      "            \"approach's performance in comparison to other baseline methods, \"\n",
      "            'demonstrating its robustness in moving landmarks to target '\n",
      "            'positions under different numbers of landmarks. Finally, the '\n",
      "            'fragment emphasizes the importance of respecting personality '\n",
      "            'rights and privacy regulations when using the DragGAN approach, '\n",
      "            'as it could be misused to create images of a real person with a '\n",
      "            'fake pose, expression, or shape. Overall, the DragGAN approach is '\n",
      "            'presented as a general framework that does not rely on '\n",
      "            'domain-specific modeling or auxiliary networks and opens new '\n",
      "            'directions for powerful image editing using generative priors.',\n",
      " 'summary_till_now': 'The document introduces the DragGAN approach, a '\n",
      "                     'framework for interactive and precise manipulation of '\n",
      "                     'GAN-generated images using motion supervision and point '\n",
      "                     'tracking optimization. The approach is evaluated on '\n",
      "                     'various datasets and includes implementation details '\n",
      "                     'such as the use of PyTorch and the Adam optimizer with '\n",
      "                     'different step sizes for different datasets, as well as '\n",
      "                     'the development of a GUI to support interactive image '\n",
      "                     'manipulation. The effectiveness of discriminative '\n",
      "                     'features of GANs in tracking via nearest neighbor search '\n",
      "                     'in a feature patch is also discussed. The optimization '\n",
      "                     'process is stopped when all handle points are no more '\n",
      "                     \"than 'A' pixels away from their corresponding target \"\n",
      "                     'points. The importance of respecting personality rights '\n",
      "                     'and privacy regulations when using the DragGAN approach '\n",
      "                     'is also emphasized.\\n'\n",
      "                     '\\n'\n",
      "                     'Current fragment of the document:\\n'\n",
      "                     '\\n'\n",
      "                     'The current fragment of the document discusses the use '\n",
      "                     'of a binary mask to denote the movable region to reduce '\n",
      "                     'ambiguity and keep certain regions fixed during '\n",
      "                     'manipulation. The fragment also highlights the ability '\n",
      "                     'of the DragGAN approach to create \"out-of-distribution\" '\n",
      "                     'manipulations, i.e., manipulations that lie outside the '\n",
      "                     'image distribution of the training dataset. The fragment '\n",
      "                     'provides a quantitative evaluation of the DragGAN '\n",
      "                     \"approach's performance in comparison to other baseline \"\n",
      "                     'methods, demonstrating its robustness in moving '\n",
      "                     'landmarks to target positions under different numbers of '\n",
      "                     'landmarks. \\n'\n",
      "                     '\\n'\n",
      "                     'Overall, the DragGAN approach is presented as a general '\n",
      "                     'framework that does not rely on domain-specific modeling '\n",
      "                     'or auxiliary networks and opens new directions for '\n",
      "                     'powerful image editing using generative priors.',\n",
      " 'title': '\"DragGAN: Interactive Image Manipulation Using Generative Priors\"'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                         | 15/18 [09:47<02:15, 45.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'questions_and_answers': 'Q: What is DragGAN approach?\\n'\n",
      "                          'A: DragGAN approach is a GAN-based framework for '\n",
      "                          'interactive and precise manipulation of '\n",
      "                          'GAN-generated images using motion supervision and '\n",
      "                          'point tracking optimization.\\n'\n",
      "                          '\\n'\n",
      "                          'Q: How does the DragGAN approach work?\\n'\n",
      "                          'A: The DragGAN approach uses a binary mask to '\n",
      "                          'denote the movable region to reduce ambiguity and '\n",
      "                          'keep certain regions fixed during manipulation. The '\n",
      "                          'approach also utilizes a motion prior model, which '\n",
      "                          'encodes the motion patterns of the target region, '\n",
      "                          'and a point tracking optimization, which minimizes '\n",
      "                          'the pixel distance between the handle points and '\n",
      "                          'the target points.\\n'\n",
      "                          '\\n'\n",
      "                          'Q: How does the DragGAN approach compare to other '\n",
      "                          'baseline methods?\\n'\n",
      "                          'A: The DragGAN approach is more robust in moving '\n",
      "                          'landmarks to target positions under different '\n",
      "                          'numbers of landmarks compared to other baseline '\n",
      "                          'methods.\\n'\n",
      "                          '\\n'\n",
      "                          'Q: What are the potential applications of the '\n",
      "                          'DragGAN approach?\\n'\n",
      "                          'A: The DragGAN approach can be applied to various '\n",
      "                          'image editing tasks, such as removing unwanted '\n",
      "                          'objects, changing background, or performing other '\n",
      "                          'types of image manipulations.',\n",
      " 'summary': 'The current fragment of the document discusses the use of a '\n",
      "            'binary mask to denote the movable region to reduce ambiguity and '\n",
      "            'keep certain regions fixed during manipulation. The approach is '\n",
      "            'evaluated on various datasets and includes implementation details '\n",
      "            'such as the use of PyTorch and the Adam optimizer with different '\n",
      "            'step sizes for different datasets, as well as the development of '\n",
      "            'a GUI to support interactive image manipulation. The '\n",
      "            'effectiveness of discriminative features of GANs in tracking via '\n",
      "            'nearest neighbor search in a feature patch is also discussed. The '\n",
      "            'optimization process is stopped when all handle points are no '\n",
      "            \"more than 'A' pixels away from their corresponding target points. \"\n",
      "            'The importance of respecting personality rights and privacy '\n",
      "            'regulations when using the DragGAN approach is also emphasized.',\n",
      " 'summary_till_now': 'The document introduces the DragGAN approach, a '\n",
      "                     'framework for interactive and precise manipulation of '\n",
      "                     'GAN-generated images using motion supervision and point '\n",
      "                     'tracking optimization. The approach is evaluated on '\n",
      "                     'various datasets and includes implementation details '\n",
      "                     'such as the use of PyTorch and the Adam optimizer with '\n",
      "                     'different step sizes for different datasets, as well as '\n",
      "                     'the development of a GUI to support interactive image '\n",
      "                     'manipulation. The effectiveness of discriminative '\n",
      "                     'features of GANs in tracking via nearest neighbor search '\n",
      "                     'in a feature patch is also discussed. The optimization '\n",
      "                     'process is stopped when all handle points are no more '\n",
      "                     \"than 'A' pixels away from their corresponding target \"\n",
      "                     'points. The importance of respecting personality rights '\n",
      "                     'and privacy regulations when using the DragGAN approach '\n",
      "                     'is also emphasized.\\n'\n",
      "                     '\\n'\n",
      "                     'current fragment:\\n'\n",
      "                     '\\n'\n",
      "                     'The current fragment of the document discusses the use '\n",
      "                     'of a binary mask to denote the movable region to reduce '\n",
      "                     'ambiguity and keep certain regions fixed during '\n",
      "                     'manipulation. The fragment also highlights the ability '\n",
      "                     'of the DragGAN approach to create \"out-of-distribution\" '\n",
      "                     'manipulations, i.e., manipulations that lie outside the '\n",
      "                     'image distribution of the training dataset. The fragment '\n",
      "                     'provides a quantitative evaluation of the DragGAN '\n",
      "                     \"approach's performance in comparison to other baseline \"\n",
      "                     'methods, demonstrating its robustness in moving '\n",
      "                     'landmarks to target positions under different numbers of '\n",
      "                     'landmarks. \\n'\n",
      "                     '\\n'\n",
      "                     'running summary:\\n'\n",
      "                     '\\n'\n",
      "                     'The DragGAN approach, a framework for interactive and '\n",
      "                     'precise manipulation of GAN-generated images using '\n",
      "                     'motion supervision and point tracking optimization, is '\n",
      "                     'the focus of this document. The approach is evaluated on '\n",
      "                     'various datasets, and implementation details such as the '\n",
      "                     'use of PyTorch and the Adam optimizer with different '\n",
      "                     'step sizes for different datasets are discussed. A GUI '\n",
      "                     'is developed to support interactive image manipulation, '\n",
      "                     'and the effectiveness of discriminative features of GANs '\n",
      "                     'in tracking via nearest neighbor search in a feature '\n",
      "                     'patch is highlighted. The optimization process is '\n",
      "                     \"stopped when all handle points are no more than 'A' \"\n",
      "                     'pixels away from their corresponding target points. The '\n",
      "                     'importance of respecting personality rights and privacy '\n",
      "                     'regulations when using the DragGAN approach is '\n",
      "                     'emphasized. The current fragment of the document '\n",
      "                     'discusses the use of a binary mask to denote the movable '\n",
      "                     'region to reduce ambiguity and keep certain regions '\n",
      "                     'fixed during manipulation. The fragment also highlights '\n",
      "                     'the ability of the DragGAN approach to create '\n",
      "                     '\"out-of-distribution\" manipulations, and a quantitative '\n",
      "                     \"evaluation of the DragGAN approach's performance in \"\n",
      "                     'comparison to other baseline methods is provided, '\n",
      "                     'demonstrating its robustness in moving landmarks to '\n",
      "                     'target positions under different numbers of landmarks. '\n",
      "                     'The DragGAN approach is presented as a general framework '\n",
      "                     'that does not rely on domain-specific modeling or '\n",
      "                     'auxiliary networks and opens new directions for powerful '\n",
      "                     'image editing using generative priors.',\n",
      " 'title': 'DragGAN: A General Framework for Interactive Image Manipulation '\n",
      "          'Using Motion Supervision and Point Tracking Optimization'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 89%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                | 16/18 [10:50<01:41, 50.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'questions_and_answers': '1. What is DragGAN approach?\\n'\n",
      "                          'DragGAN approach is a technique for interactive and '\n",
      "                          'precise manipulation of GAN-generated images using '\n",
      "                          'motion supervision and point tracking '\n",
      "                          'optimization.\\n'\n",
      "                          '2. How does DragGAN approach work?\\n'\n",
      "                          'The approach uses motion supervision and point '\n",
      "                          'tracking optimization to move image landmarks to '\n",
      "                          'target positions. The optimization process is '\n",
      "                          \"stopped when all handle points are no more than 'A' \"\n",
      "                          'pixels away from their corresponding target '\n",
      "                          'points.\\n'\n",
      "                          '3. What is a binary mask?\\n'\n",
      "                          'A binary mask is a binary image that denotes the '\n",
      "                          'movable region, that is, the region that can be '\n",
      "                          'manipulated during image manipulation. This helps '\n",
      "                          'in reducing ambiguity and keeping certain regions '\n",
      "                          'fixed during manipulation.\\n'\n",
      "                          '4. What is an out-of-distribution manipulation?\\n'\n",
      "                          'An out-of-distribution manipulation is a '\n",
      "                          'manipulation that lies outside the image '\n",
      "                          'distribution of the training dataset. The DragGAN '\n",
      "                          'approach can create such manipulations.\\n'\n",
      "                          '5. What is the quantitative evaluation of the '\n",
      "                          \"DragGAN approach's performance in comparison to \"\n",
      "                          'other baseline methods?\\n'\n",
      "                          'The evaluation compares the performance of the '\n",
      "                          'DragGAN approach with other baseline methods in '\n",
      "                          'terms of its ability to move landmarks to target '\n",
      "                          'positions under different numbers of landmarks. The '\n",
      "                          'DragGAN approach is found to be more robust than '\n",
      "                          'the other methods.',\n",
      " 'summary': 'The DragGAN approach, a framework for interactive and precise '\n",
      "            'manipulation of GAN-generated images using motion supervision and '\n",
      "            'point tracking optimization, is the focus of this document. The '\n",
      "            'approach is evaluated on various datasets, and implementation '\n",
      "            'details such as the use of PyTorch and the Adam optimizer with '\n",
      "            'different step sizes for different datasets are discussed. A GUI '\n",
      "            'is developed to support interactive image manipulation, and the '\n",
      "            'effectiveness of discriminative features of GANs in tracking via '\n",
      "            'nearest neighbor search in a feature patch is highlighted. The '\n",
      "            'optimization process is stopped when all handle points are no '\n",
      "            \"more than 'A' pixels away from their corresponding target points. \"\n",
      "            'The importance of respecting personality rights and privacy '\n",
      "            'regulations when using the DragGAN approach is emphasized. The '\n",
      "            'current fragment of the document discusses the use of a binary '\n",
      "            'mask to denote the movable region to reduce ambiguity and keep '\n",
      "            'certain regions fixed during manipulation. The fragment also '\n",
      "            'highlights the ability of the DragGAN approach to create '\n",
      "            '\"out-of-distribution\" manipulations, and a quantitative '\n",
      "            \"evaluation of the DragGAN approach's performance in comparison to \"\n",
      "            'other baseline methods is provided, demonstrating its robustness '\n",
      "            'in moving landmarks to target positions under different numbers '\n",
      "            'of landmarks. The DragGAN approach is presented as a general '\n",
      "            'framework that does not rely on domain-specific modeling or '\n",
      "            'auxiliary networks and opens new directions for powerful image '\n",
      "            'editing using generative priors.',\n",
      " 'summary_till_now': 'The document introduces the DragGAN approach, a '\n",
      "                     'framework for interactive and precise manipulation of '\n",
      "                     'GAN-generated images using motion supervision and point '\n",
      "                     'tracking optimization. The approach is evaluated on '\n",
      "                     'various datasets and includes implementation details '\n",
      "                     'such as the use of PyTorch and the Adam optimizer with '\n",
      "                     'different step sizes for different datasets, as well as '\n",
      "                     'the development of a GUI to support interactive image '\n",
      "                     'manipulation. The effectiveness of discriminative '\n",
      "                     'features of GANs in tracking via nearest neighbor search '\n",
      "                     'in a feature patch is also discussed. The optimization '\n",
      "                     'process is stopped when all handle points are no more '\n",
      "                     \"than 'A' pixels away from their corresponding target \"\n",
      "                     'points. The importance of respecting personality rights '\n",
      "                     'and privacy regulations when using the DragGAN approach '\n",
      "                     'is also emphasized.\\n'\n",
      "                     '\\n'\n",
      "                     'current fragment:\\n'\n",
      "                     '\\n'\n",
      "                     'The current fragment of the document discusses the use '\n",
      "                     'of a binary mask to denote the movable region to reduce '\n",
      "                     'ambiguity and keep certain regions fixed during '\n",
      "                     'manipulation. The fragment also highlights the ability '\n",
      "                     'of the DragGAN approach to create \"out-of-distribution\" '\n",
      "                     'manipulations, i.e., manipulations that lie outside the '\n",
      "                     'image distribution of the training dataset. The fragment '\n",
      "                     'provides a quantitative evaluation of the DragGAN '\n",
      "                     \"approach's performance in comparison to other baseline \"\n",
      "                     'methods, demonstrating its robustness in moving '\n",
      "                     'landmarks to target positions under different numbers of '\n",
      "                     'landmarks. \\n'\n",
      "                     '\\n'\n",
      "                     'revised summary:\\n'\n",
      "                     '\\n'\n",
      "                     'The DragGAN approach, a framework for interactive and '\n",
      "                     'precise manipulation of GAN-generated images using '\n",
      "                     'motion supervision and point tracking optimization, is '\n",
      "                     'the focus of this document. The approach is evaluated on '\n",
      "                     'various datasets, and implementation details such as the '\n",
      "                     'use of PyTorch and the Adam optimizer with different '\n",
      "                     'step sizes for different datasets are discussed. A GUI '\n",
      "                     'is developed to support interactive image manipulation, '\n",
      "                     'and the effectiveness of discriminative features of GANs '\n",
      "                     'in tracking via nearest neighbor search in a feature '\n",
      "                     'patch is highlighted. The optimization process is '\n",
      "                     \"stopped when all handle points are no more than 'A' \"\n",
      "                     'pixels away from their corresponding target points. The '\n",
      "                     'importance of respecting personality rights and privacy '\n",
      "                     'regulations when using the DragGAN approach is '\n",
      "                     'emphasized. \\n'\n",
      "                     '\\n'\n",
      "                     'The current fragment of the document discusses the use '\n",
      "                     'of a binary mask to denote the movable region to reduce '\n",
      "                     'ambiguity and keep certain regions fixed during '\n",
      "                     'manipulation. It also highlights the ability of the '\n",
      "                     'DragGAN approach to create \"out-of-distribution\" '\n",
      "                     'manipulations, which lie outside the image distribution '\n",
      "                     'of the training dataset. A quantitative evaluation of '\n",
      "                     \"the DragGAN approach's performance is provided, \"\n",
      "                     'comparing it to other baseline methods, and '\n",
      "                     'demonstrating its robustness in moving landmarks to '\n",
      "                     'target positions under different numbers of landmarks. '\n",
      "                     'The DragGAN approach is presented as a general framework '\n",
      "                     'that does not rely on domain-specific modeling or '\n",
      "                     'auxiliary networks and opens new directions for powerful '\n",
      "                     'image editing using generative priors.',\n",
      " 'title': '\"The DragGAN Approach for Interactive and Precise Image '\n",
      "          'Manipulation\"'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 94%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌        | 17/18 [12:04<00:57, 57.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'questions_and_answers': '1. What is the DragGAN approach?\\n'\n",
      "                          'The DragGAN approach is a GAN-based framework for '\n",
      "                          'interactive and precise manipulation of '\n",
      "                          'GAN-generated images using motion supervision and '\n",
      "                          'point tracking optimization.\\n'\n",
      "                          '2. What datasets has DragGAN been tested on?\\n'\n",
      "                          'The DragGAN approach has been tested on various '\n",
      "                          'datasets, including CelebA, LSUN, and RGB-D.\\n'\n",
      "                          '3. How does DragGAN perform in comparison to other '\n",
      "                          'baseline methods?\\n'\n",
      "                          'The DragGAN approach outperforms other baseline '\n",
      "                          'methods in terms of its ability to move landmarks '\n",
      "                          'to target positions under different numbers of '\n",
      "                          'landmarks.\\n'\n",
      "                          '4. How does DragGAN use motion supervision and '\n",
      "                          'point tracking optimization to perform image '\n",
      "                          'manipulation?\\n'\n",
      "                          'DragGAN uses motion supervision and point tracking '\n",
      "                          'optimization to precisely control the movement of '\n",
      "                          'landmarks in the GAN-generated images, allowing for '\n",
      "                          'interactive and precise image manipulation.',\n",
      " 'summary': 'The DragGAN approach, a framework for interactive and precise '\n",
      "            'manipulation of GAN-generated images using motion supervision and '\n",
      "            'point tracking optimization, is the focus of this document. The '\n",
      "            'approach is evaluated on various datasets, and implementation '\n",
      "            'details such as the use of PyTorch and the Adam optimizer with '\n",
      "            'different step sizes for different datasets are discussed. A GUI '\n",
      "            'is developed to support interactive image manipulation, and the '\n",
      "            'effectiveness of discriminative features of GANs in tracking via '\n",
      "            'nearest neighbor search in a feature patch is highlighted. The '\n",
      "            'optimization process is stopped when all handle points are no '\n",
      "            \"more than 'A' pixels away from their corresponding target points. \"\n",
      "            'The importance of respecting personality rights and privacy '\n",
      "            'regulations when using the DragGAN approach is emphasized. The '\n",
      "            'current fragment of the document discusses the use of a binary '\n",
      "            'mask to denote the movable region to reduce ambiguity and keep '\n",
      "            'certain regions fixed during manipulation. It also highlights the '\n",
      "            'ability of the DragGAN approach to create \"out-of-distribution\" '\n",
      "            'manipulations, which lie outside the image distribution of the '\n",
      "            'training dataset. A quantitative evaluation of the DragGAN '\n",
      "            \"approach's performance is provided, comparing it to other \"\n",
      "            'baseline methods, and demonstrating its robustness in moving '\n",
      "            'landmarks to target positions under different numbers of '\n",
      "            'landmarks. The DragGAN approach is presented as a general '\n",
      "            'framework that does not rely on domain-specific modeling or '\n",
      "            'auxiliary networks and opens new directions for powerful image '\n",
      "            'editing using generative priors.',\n",
      " 'summary_till_now': 'The document introduces the DragGAN approach, a '\n",
      "                     'framework for interactive and precise manipulation of '\n",
      "                     'GAN-generated images using motion supervision and point '\n",
      "                     'tracking optimization. The approach is evaluated on '\n",
      "                     'various datasets and includes implementation details '\n",
      "                     'such as the use of PyTorch and the Adam optimizer with '\n",
      "                     'different step sizes for different datasets, as well as '\n",
      "                     'the development of a GUI to support interactive image '\n",
      "                     'manipulation. The effectiveness of discriminative '\n",
      "                     'features of GANs in tracking via nearest neighbor search '\n",
      "                     'in a feature patch is also discussed. The optimization '\n",
      "                     'process is stopped when all handle points are no more '\n",
      "                     \"than 'A' pixels away from their corresponding target \"\n",
      "                     'points. The importance of respecting personality rights '\n",
      "                     'and privacy regulations when using the DragGAN approach '\n",
      "                     'is also emphasized.\\n'\n",
      "                     '\\n'\n",
      "                     'current fragment:\\n'\n",
      "                     '\\n'\n",
      "                     'The current fragment of the document discusses the use '\n",
      "                     'of a binary mask to denote the movable region to reduce '\n",
      "                     'ambiguity and keep certain regions fixed during '\n",
      "                     'manipulation. The fragment also highlights the ability '\n",
      "                     'of the DragGAN approach to create \"out-of-distribution\" '\n",
      "                     'manipulations, i.e., manipulations that lie outside the '\n",
      "                     'image distribution of the training dataset. The fragment '\n",
      "                     'provides a quantitative evaluation of the DragGAN '\n",
      "                     \"approach's performance in comparison to other baseline \"\n",
      "                     'methods, demonstrating its robustness in moving '\n",
      "                     'landmarks to target positions under different numbers of '\n",
      "                     'landmarks. \\n'\n",
      "                     '\\n'\n",
      "                     'revised summary:\\n'\n",
      "                     '\\n'\n",
      "                     'The DragGAN approach is a framework for interactive and '\n",
      "                     'precise manipulation of GAN-generated images using '\n",
      "                     'motion supervision and point tracking optimization. The '\n",
      "                     'approach has been evaluated on various datasets and '\n",
      "                     'includes implementation details such as the use of '\n",
      "                     'PyTorch and the Adam optimizer with different step sizes '\n",
      "                     'for different datasets. A GUI has been developed to '\n",
      "                     'support interactive image manipulation, and the '\n",
      "                     'effectiveness of discriminative features of GANs in '\n",
      "                     'tracking via nearest neighbor search in a feature patch '\n",
      "                     'has been highlighted. The optimization process is '\n",
      "                     \"stopped when all handle points are no more than 'A' \"\n",
      "                     'pixels away from their corresponding target points. The '\n",
      "                     'importance of respecting personality rights and privacy '\n",
      "                     'regulations when using the DragGAN approach has been '\n",
      "                     'emphasized.\\n'\n",
      "                     '\\n'\n",
      "                     'The current fragment of the document discusses the use '\n",
      "                     'of a binary mask to denote the movable region to reduce '\n",
      "                     'ambiguity and keep certain regions fixed during '\n",
      "                     'manipulation. The ability of the DragGAN approach to '\n",
      "                     'create \"out-of-distribution\" manipulations, i.e., '\n",
      "                     'manipulations that lie outside the image distribution of '\n",
      "                     'the training dataset, is also highlighted. The fragment '\n",
      "                     'provides a quantitative evaluation of the DragGAN '\n",
      "                     \"approach's performance in comparison to other baseline \"\n",
      "                     'methods, demonstrating its robustness in moving '\n",
      "                     'landmarks to target positions under different numbers of '\n",
      "                     'landmarks. The DragGAN approach is presented as a '\n",
      "                     'general framework that does not rely on domain-specific '\n",
      "                     'modeling or auxiliary networks and opens new directions '\n",
      "                     'for powerful image editing using generative priors.',\n",
      " 'title': '\"The DragGAN Approach for Interactive and Precise Manipulation of '\n",
      "          'GAN-Generated Images\"'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 18/18 [13:05<00:00, 43.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'questions_and_answers': '\\n'\n",
      "                          'Question 1: What is the DragGAN approach?\\n'\n",
      "                          'Answer 1: The DragGAN approach is a generative '\n",
      "                          'prior-based framework for interactive and precise '\n",
      "                          'manipulation of GAN-generated images using motion '\n",
      "                          'supervision and point tracking optimization.\\n'\n",
      "                          '\\n'\n",
      "                          'Question 2: How does DragGAN work?\\n'\n",
      "                          'Answer 2: The DragGAN approach uses motion '\n",
      "                          'supervision and point tracking optimization to '\n",
      "                          'achieve precise manipulation of GAN-generated '\n",
      "                          'images. The approach has been implemented using '\n",
      "                          'PyTorch and the Adam optimizer with different step '\n",
      "                          'sizes for different datasets.\\n'\n",
      "                          '\\n'\n",
      "                          'Question 3: What is the importance of DragGAN?\\n'\n",
      "                          'Answer 3: The DragGAN approach opens new directions '\n",
      "                          'for powerful image editing using generative priors, '\n",
      "                          'and it does not rely on domain-specific modeling or '\n",
      "                          'auxiliary networks.\\n'\n",
      "                          '\\n'\n",
      "                          'Question 4: What are some limitations of DragGAN?\\n'\n",
      "                          'Answer 4: The DragGAN approach is limited in the '\n",
      "                          'domain of images it has been trained on, and it '\n",
      "                          'struggles to achieve out-of-distribution '\n",
      "                          'manipulations.',\n",
      " 'summary': 'The DragGAN approach is a general framework that allows for '\n",
      "            'interactive and precise manipulation of GAN-generated images '\n",
      "            'using motion supervision and point tracking optimization. A '\n",
      "            'binary mask is used to denote movable regions and keep certain '\n",
      "            'regions fixed during manipulation, allowing for '\n",
      "            '\"out-of-distribution\" manipulations that lie outside the image '\n",
      "            'distribution of the training dataset. The approach has been '\n",
      "            'evaluated on various datasets and includes implementation details '\n",
      "            'such as the use of PyTorch and the Adam optimizer with different '\n",
      "            'step sizes for different datasets. A GUI has been developed to '\n",
      "            'support interactive image manipulation, and the effectiveness of '\n",
      "            'discriminative features of GANs in tracking via nearest neighbor '\n",
      "            'search in a feature patch has been highlighted. The optimization '\n",
      "            \"process is stopped when all handle points are no more than 'A' \"\n",
      "            'pixels away from their corresponding target points. The '\n",
      "            'importance of respecting personality rights and privacy '\n",
      "            'regulations when using the DragGAN approach has been emphasized.',\n",
      " 'summary_till_now': 'The document introduces the DragGAN approach as a '\n",
      "                     'framework for interactive and precise manipulation of '\n",
      "                     'GAN-generated images using motion supervision and point '\n",
      "                     'tracking optimization. The approach has been evaluated '\n",
      "                     'on various datasets and includes implementation details '\n",
      "                     'such as the use of PyTorch and the Adam optimizer with '\n",
      "                     'different step sizes for different datasets. A GUI has '\n",
      "                     'been developed to support interactive image '\n",
      "                     'manipulation, and the effectiveness of discriminative '\n",
      "                     'features of GANs in tracking via nearest neighbor search '\n",
      "                     'in a feature patch has been highlighted. The '\n",
      "                     'optimization process is stopped when all handle points '\n",
      "                     \"are no more than 'A' pixels away from their \"\n",
      "                     'corresponding target points. The importance of '\n",
      "                     'respecting personality rights and privacy regulations '\n",
      "                     'when using the DragGAN approach has been emphasized.\\n'\n",
      "                     '\\n'\n",
      "                     'current fragment:\\n'\n",
      "                     '\\n'\n",
      "                     'The current fragment of the document discusses the use '\n",
      "                     'of a binary mask to denote the movable region to reduce '\n",
      "                     'ambiguity and keep certain regions fixed during '\n",
      "                     'manipulation. The ability of the DragGAN approach to '\n",
      "                     'create \"out-of-distribution\" manipulations, i.e., '\n",
      "                     'manipulations that lie outside the image distribution of '\n",
      "                     'the training dataset, is also highlighted. The fragment '\n",
      "                     'provides a quantitative evaluation of the DragGAN '\n",
      "                     \"approach's performance in comparison to other baseline \"\n",
      "                     'methods, demonstrating its robustness in moving '\n",
      "                     'landmarks to target positions under different numbers of '\n",
      "                     'landmarks.\\n'\n",
      "                     '\\n'\n",
      "                     'updated summary:\\n'\n",
      "                     '\\n'\n",
      "                     'The document introduces the DragGAN approach, a general '\n",
      "                     'framework for interactive and precise manipulation of '\n",
      "                     'GAN-generated images using motion supervision and point '\n",
      "                     'tracking optimization. The approach has been evaluated '\n",
      "                     'on various datasets with implementation details such as '\n",
      "                     'the use of PyTorch, the Adam optimizer with different '\n",
      "                     'step sizes for different datasets, and a GUI to support '\n",
      "                     'interactive image manipulation. The effectiveness of '\n",
      "                     'discriminative features of GANs in tracking via nearest '\n",
      "                     'neighbor search in a feature patch has been highlighted. '\n",
      "                     'The optimization process is stopped when all handle '\n",
      "                     \"points are no more than 'A' pixels away from their \"\n",
      "                     'corresponding target points. The importance of '\n",
      "                     'respecting personality rights and privacy regulations '\n",
      "                     'when using the DragGAN approach has been emphasized.\\n'\n",
      "                     '\\n'\n",
      "                     'The current fragment of the document discusses the use '\n",
      "                     'of a binary mask to denote the movable region to reduce '\n",
      "                     'ambiguity and keep certain regions fixed during '\n",
      "                     'manipulation. The ability of the DragGAN approach to '\n",
      "                     'create \"out-of-distribution\" manipulations, i.e., '\n",
      "                     'manipulations that lie outside the image distribution of '\n",
      "                     'the training dataset, is also highlighted. The fragment '\n",
      "                     'provides a quantitative evaluation of the DragGAN '\n",
      "                     \"approach's performance in comparison to other baseline \"\n",
      "                     'methods, demonstrating its robustness in moving '\n",
      "                     'landmarks to target positions under different numbers of '\n",
      "                     'landmarks. This approach opens up new directions for '\n",
      "                     'powerful image editing using generative priors without '\n",
      "                     'relying on domain-specific modeling or auxiliary '\n",
      "                     'networks.',\n",
      " 'title': \"Quantitative Evaluation of DragGAN's Performance in Moving \"\n",
      "          'Landmarks to Target Positions'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 17%|█████████████████████████▌                                                                                                                               | 1/6 [00:40<03:24, 40.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"methodology\": \"In this fragment, the authors discuss using a binary mask to denote the movable region in DragGAN, which reduces ambiguity and keeps certain regions fixed during image manipulation. The approach is capable of generating 'out-of-distribution' manipulations, which are manipulations that lie outside the image distribution of the training dataset.\",\n",
      "    \"previous_literature_and_differentiation\": \"\",\n",
      "    \"experiments_and_evaluation\": \"A quantitative evaluation of the DragGAN approach is provided in this fragment, comparing its performance to baseline methods in moving landmarks to target positions under different numbers of landmarks.\",\n",
      "    \"results_and_comparison\": \"The quantitative evaluation shows the robustness of DragGAN in moving landmarks to their target positions compared to other methods.\",\n",
      "    \"limitations_and_future_work\": \"\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███████████████████████████████████████████████████                                                                                                      | 2/6 [01:22<02:45, 41.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"methodology\": \"The methodology described in this fragment involves an optimization process consisting of motion supervision and point tracking for moving handle points towards corresponding target points in GAN-generated images. This process iteratively performs these two steps until the handle points reach their target points.\",\n",
      "    \"previous_literature_and_differentiation\": \"The authors mention various prior works focusing on editing unconditional GANs, EditGAN, GANWarping, UserControllableLT, 3D-aware GANs, and diffusion models. DragGAN provides a unique solution for editing GAN-generated images using motion supervision and point tracking optimization.\",\n",
      "    \"experiments_and_evaluation\": \"\",\n",
      "    \"results_and_comparison\": \"\",\n",
      "    \"limitations_and_future_work\": \"\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|████████████████████████████████████████████████████████████████████████████▌                                                                            | 3/6 [02:18<02:23, 47.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"methodology\": \"The authors use a motion supervision loss that does not rely on any additional neural networks. They consider feature maps after the 6th block of StyleGAN2 and perform point tracking on the same feature space via the nearest neighbor search. The proposed DragGAN approach uses a binary mask to denote movable regions, allowing for more constrained image manipulation and the creation of out-of-distribution manipulations.\",\n",
      "    \"previous_literature_and_differentiation\": \"\",\n",
      "    \"experiments_and_evaluation\": \"The experiments involve evaluating the DragGAN approach based on StyleGAN2 pretrained models on several datasets such as FFHQ, AFHQCat, SHHQ, and LSUN Car, among others. The authors compare their approach with UserControllableLT, RAFT, and PIPs for point tracking.\",\n",
      "    \"results_and_comparison\": \"The DragGAN approach outperforms UserControllableLT, RAFT, and PIPs in providing accurate tracking and producing more natural and superior results on various datasets.\",\n",
      "    \"limitations_and_future_work\": \"\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████████████████████████████████████████████████████████████████████████████████████████████████████                                                   | 4/6 [03:53<02:13, 66.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"methodology\": \"The approach uses a mask function to help reduce ambiguity and keep certain regions fixed during image manipulation. The optimization process is stopped when all handle points are no more than 'A' pixels away from their corresponding target points.\",\n",
      "    \"previous_literature_and_differentiation\": \"\",\n",
      "    \"experiments_and_evaluation\": \"The authors perform quantitative evaluation under two settings, including face landmark manipulation and paired image reconstruction. They evaluate their method using 1, 5, and 68 landmarks and report the FID score between the edited images and initial images as an indication of image quality.\",\n",
      "    \"results_and_comparison\": \"The results show that DragGAN significantly outperforms UserControllableLT under different numbers of points and achieves more accurate manipulation than RAFT and PIPs, with better image quality as indicated by the FID scores.\",\n",
      "    \"limitations_and_future_work\": \"The DragGAN approach has limitations when creating human poses that deviate from the training distribution, which can lead to artifacts. Handle points in texture-less regions sometimes suffer from more drift in tracking, so it is suggested to pick texture-rich handle points if possible.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                         | 5/6 [04:19<00:52, 52.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"methodology\": \"\",\n",
      "    \"previous_literature_and_differentiation\": \"\",\n",
      "    \"experiments_and_evaluation\": \"\",\n",
      "    \"results_and_comparison\": \"\",\n",
      "    \"limitations_and_future_work\": \"\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [05:18<00:00, 53.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"methodology\": \"\",\n",
      "    \"previous_literature_and_differentiation\": \"\",\n",
      "    \"experiments_and_evaluation\": \"In this fragment, the authors present qualitative comparisons, showcasing their method's effectiveness in face landmark manipulation, even in dense keypoint cases. They demonstrate continuous image manipulation, where users can successively manipulate images based on previous results, as well as real image manipulation. They highlight the limitations of their approach in terms of out-of-distribution poses, texture-less handle points, and texture-rich handle points. In addition, the authors discuss the effects of the mask, showing how by masking the foreground object, they can fix the background, preserving details and potentially improving background preservation through feature blending.\",\n",
      "    \"results_and_comparison\": \"\",\n",
      "    \"limitations_and_future_work\": \"The limitations discussed in this fragment include the method's struggle with out-of-distribution poses, such as editing arms and legs to be in upward positions when the StyleGAN-human was trained on a fashion dataset with downward positions, causing distortion artifacts. Additionally, handle points in texture-less regions may suffer from more drift during tracking, and it is suggested that texture-rich handle points should be chosen if possible.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Can you explain the feature-based motion supervision used in DragGAN in more detail? \n",
      "The feature-based motion supervision in DragGAN involves a feature patch centered at each handle point. The feature patch is used to compute a dense displacement field between the handle point and its target point. This dense displacement field is then used to update the position of the handle point iteratively until it reaches the target point. The displacement field is computed using the nearest neighbor search in the feature space, which is based on the discriminative features of GANs. \n",
      "\n",
      "2. How does DragGAN address the limitations of prior approaches in controlling GANs? \n",
      "DragGAN addresses the limitations of prior approaches in controlling GANs by providing flexible, precise, and generic controllability. Unlike prior approaches that rely on prior 3D models or manually annotated data, DragGAN allows users to interactively manipulate images by clicking handle points and target points. This point-based manipulation is agnostic to object categories and allows users to control diverse spatial attributes with high precision. \n",
      "\n",
      "3. Can you explain the binary mask used in DragGAN to denote the movable region? \n",
      "The binary mask used in DragGAN is used to denote the movable region and reduce ambiguity during manipulation. Users can optionally draw a mask of the flexible region, keeping the rest of the image fixed. This allows users to control only specific parts of the image while keeping other regions fixed.\n",
      "\n",
      "4. How does DragGAN generate \"out-of-distribution\" manipulations? \n",
      "DragGAN is able to generate \"out-of-distribution\" manipulations, i.e., manipulations that lie outside the image distribution of the training dataset, by leveraging the learned generative image manifold of the GAN. The point-based manipulation is performed on this manifold, which tends to produce realistic outputs even for challenging scenarios such as hallucinating occluded content and deforming shapes that consistently follow the object's rigidity. \n",
      "\n",
      "5. What is the importance of respecting personality rights and privacy regulations when using the DragGAN approach? \n",
      "The importance of respecting personality rights and privacy regulations when using the DragGAN approach is emphasized to ensure ethical use of the technology. DragGAN allows users to manipulate images in a precise and flexible manner, which can raise concerns about the potential misuse of the technology for malicious purposes. Therefore, it is important to ensure that the technology is used in a responsible and ethical manner. \n",
      "\n",
      "6. How does DragGAN compare to other baseline methods in terms of performance? \n",
      "DragGAN has been evaluated on various datasets and has demonstrated superior performance compared to other baseline methods in the tasks of image manipulation and point tracking. The quantitative evaluation in the current fragment shows that DragGAN is robust in moving landmarks to target positions under different numbers of landmarks. \n",
      "\n",
      "7. What are the potential applications of DragGAN in real-world scenarios? \n",
      "DragGAN has potential applications in a variety of real-world scenarios, such as social media image editing, professional movie pre-visualization and media editing, and car design. The flexible and precise controllability of the pose, shape, expression, and layout of the generated objects can meet diverse user requirements. The technology can also be used for interactive modification of real images through GAN inversion. \n",
      "\n",
      "8. What are the limitations of using point-based manipulation to control GANs? \n",
      "One limitation of using point-based manipulation to control GANs is that it may not be suitable for all types of images or object categories. The effectiveness of the approach can also depend on the quality and diversity of the training dataset. Additionally, point-based manipulation may not provide as much control over certain spatial attributes as other approaches that rely on prior 3D models or manually annotated data.\n",
      "1. Can you explain the feature-based motion supervision in DragGAN and how it helps drive handle points towards their corresponding target points?\n",
      "\n",
      "A: The feature-based motion supervision in DragGAN optimizes the latent code using a shifted feature patch loss. This optimization process leads to the handle points shifting closer to their corresponding target points. The feature space of a GAN is sufficiently discriminative to enable this motion supervision, which helps to achieve precise point-based manipulation.\n",
      "\n",
      "2. How does DragGAN allow for the manipulation of real images through GAN inversion?\n",
      "\n",
      "A: DragGAN's GAN-based point tracking algorithm can be combined with GAN inversion techniques to enable the manipulation of real images. This allows users to perform powerful image editing using generative priors without relying on domain-specific modeling or auxiliary networks.\n",
      "\n",
      "3. Can you explain how DragGAN achieves efficient manipulation and allows for live, interactive editing sessions?\n",
      "\n",
      "A: DragGAN does not rely on any additional networks like RAFT, which makes it an efficient manipulation tool. It only takes a few seconds on a single RTX 3090 GPU in most cases, which allows for live, interactive editing sessions. Users can quickly iterate on different layouts until the desired output is achieved.\n",
      "\n",
      "4. What are some challenges faced in interactive point-based manipulation, and how does DragGAN address these challenges?\n",
      "\n",
      "A: One challenge in interactive point-based manipulation is handling more than one point with precise position control. DragGAN addresses this challenge by allowing users to click any number of handle points and target points on the image and driving the handle points to reach their corresponding target points. Another challenge is tracking the handle points so that their positions are known at each editing step. DragGAN addresses this challenge by performing nearest neighbor search in the feature space and repeating the optimization process until the handle points reach the targets.\n",
      "\n",
      "5. In what ways does DragGAN differ from UserControllableLT, and how does it outperform it?\n",
      "\n",
      "A: DragGAN differs from UserControllableLT in two ways. Firstly, DragGAN allows for the control of more than one point, while UserControllableLT does not handle this well. Secondly, DragGAN requires the handle points to precisely reach the target points, while UserControllableLT does not. DragGAN outperforms UserControllableLT in both qualitative and quantitative comparisons, as it achieves diverse manipulation effects across many object categories and enables much more accurate image manipulation.\n",
      "\n",
      "6. How does DragGAN respect personality rights and privacy regulations when using the approach?\n",
      "\n",
      "A: The importance of respecting personality rights and privacy regulations when using the DragGAN approach has been emphasized in the document. DragGAN provides users with the ability to draw a binary mask to denote the movable region to reduce ambiguity and keep certain regions fixed during manipulation. This helps to ensure that the approach is used in an ethical and responsible manner.\n",
      "1. Can you explain how the DragGAN approach ensures that the generated images stay on the manifold of realistic images?\n",
      "\n",
      "A1. The DragGAN approach utilizes two novel ingredients to ensure that the generated images stay on the manifold of realistic images. First, it uses an optimization of latent codes that incrementally moves multiple handle points towards their target locations. Second, it uses a point tracking procedure to faithfully trace the trajectory of the handle points. Both components utilize the discriminative quality of intermediate feature maps of the GAN to yield pixel-precise image deformations and interactive performance.\n",
      "\n",
      "2. How does the DragGAN approach compare to other baseline methods in terms of accuracy in moving landmarks to target positions under different numbers of landmarks?\n",
      "\n",
      "A2. The DragGAN approach significantly outperforms other baseline methods, including UserControllableLT, RAFT tracking, and PIPs tracking, in terms of accuracy in moving landmarks to target positions under different numbers of landmarks. The approach is able to achieve much more faithful manipulation while maintaining a comfortable running time for users.\n",
      "\n",
      "3. What are some limitations of the DragGAN approach?\n",
      "\n",
      "A3. Despite some extrapolation capability, the editing quality of the DragGAN approach is still affected by the diversity of training data. In addition, handle points in texture-less regions sometimes suffer from more drift in tracking. The approach also requires texture-rich handle points if possible. \n",
      "\n",
      "4. How does the DragGAN approach ensure that personality rights and privacy regulations are respected when using the approach?\n",
      "\n",
      "A4. The DragGAN approach emphasizes the importance of respecting personality rights and privacy regulations when using the approach. Any application or research that uses the approach has to strictly respect personality rights and privacy regulations.\n",
      "\n",
      "5. Can you explain how the DragGAN approach creates \"out-of-distribution\" manipulations?\n",
      "\n",
      "A5. The DragGAN approach has some extrapolation capability, creating images outside the training image distribution, for example, an extremely opened mouth and a large wheel. This is achieved by leveraging the discriminative quality of intermediate feature maps of the GAN to yield pixel-precise image deformations and interactive performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. What is the significance of using a binary mask in the DragGAN approach for image manipulation? \n",
      "\n",
      "Answer: The use of a binary mask in DragGAN approach helps to reduce ambiguity and keep certain regions fixed during manipulation. It allows the approach to achieve more precise and controlled image manipulation by defining a movable region and keeping other regions fixed. This is especially important when dealing with complex images with multiple objects or regions.\n",
      "\n",
      "2. How does the DragGAN approach compare to other baseline methods in terms of performance?\n",
      "\n",
      "Answer: The DragGAN approach has been evaluated against other baseline methods and has demonstrated robustness in moving landmarks to target positions under different numbers of landmarks. The quantitative evaluation of the DragGAN approach's performance shows that it outperforms other baseline methods in terms of accuracy and efficiency. This highlights the effectiveness of the approach in achieving precise and controlled image manipulation.\n",
      "\n",
      "3. What are some ethical considerations that need to be taken into account when using the DragGAN approach?\n",
      "\n",
      "Answer: The DragGAN approach can be a powerful tool for image manipulation, but it also raises important ethical considerations related to privacy and personality rights. It is important to respect these rights when using the approach and to be aware of the potential consequences of image manipulation. The DragGAN approach should be used responsibly and with consideration for the potential impact on individuals and society as a whole.\n",
      "\n",
      "4. How does the DragGAN approach deal with the limitations of training data in achieving out-of-distribution manipulations?\n",
      "\n",
      "Answer: The DragGAN approach uses a W+ space optimization to achieve out-of-distribution manipulations. This allows the approach to optimize the latent code in W+ space, which is easier to achieve out-of-distribution manipulations such as closing only one eye of the cat. In contrast, W space struggles to achieve this as it tends to keep the image within the distribution of training data. This highlights the effectiveness of the approach in achieving out-of-distribution manipulations.\n",
      "\n",
      "5. What are some potential future directions for research based on the DragGAN approach?\n",
      "\n",
      "Answer: The DragGAN approach opens up new directions for research in powerful image editing using generative priors without relying on domain-specific modeling or auxiliary networks. One potential direction for future research could be to explore the use of DragGAN on real-world images and videos, which could have important implications for applications such as video editing and special effects. Another potential direction for research could be to explore the use of DragGAN in other domains, such as natural language processing or audio processing.\n"
     ]
    },
    {
     "ename": "RetryError",
     "evalue": "RetryError[<Future at 0x7fe0a2e31be0 state=finished raised AssertionError>]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tenacity/__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 382\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n",
      "Input \u001b[0;32mIn [266]\u001b[0m, in \u001b[0;36mCallGpt.get_turbo_call.<locals>.call\u001b[0;34m(text, temperature, num_tokens)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m#             print(easy_model + \" \" + str(len(self.easy_enc.encode(self.system +\" \\n \" + text))) + \" \" + str(len(self.easy_enc.encode(message))) + \" \" + \" \"+ finish_reason + \" \" + message + \" \\n\")\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m             \u001b[38;5;28;01massert\u001b[39;00m finish_reason \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     76\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m message\n",
      "\u001b[0;31mAssertionError\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRetryError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [480]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# doremi = create_document_index(\"https://arxiv.org/pdf/2305.10429.pdf\")\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m dgn \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_document_index\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://vcai.mpi-inf.mpg.de/projects/DragGAN/data/paper.pdf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [477]\u001b[0m, in \u001b[0;36mcreate_document_index\u001b[0;34m(pdf_url)\u001b[0m\n\u001b[1;32m      7\u001b[0m dpr \u001b[38;5;241m=\u001b[39m DeepReader()(doc_text, full_summary)\n\u001b[1;32m      8\u001b[0m dpr \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([d[k] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m dpr])\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(dpr[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys())}\n\u001b[0;32m---> 11\u001b[0m qa_generations \u001b[38;5;241m=\u001b[39m \u001b[43mQuestionAnswerGenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_summary\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m full_summary[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetailed_qna\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m qa_generations\n\u001b[1;32m     15\u001b[0m full_summary[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeep_reader_details\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m dpr\n",
      "Input \u001b[0;32mIn [297]\u001b[0m, in \u001b[0;36mQuestionAnswerGenerator.__call__\u001b[0;34m(self, long_document, full_summary)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rsp\n\u001b[1;32m     85\u001b[0m calls \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: p, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.7\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m4000\u001b[39m} \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m---> 86\u001b[0m responses \u001b[38;5;241m=\u001b[39m \u001b[43mcall_api_parallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcalls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprinted_gpt_call\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m chunk_questions \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ix, (qna_resp, call) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(responses, calls)):\n",
      "Input \u001b[0;32mIn [287]\u001b[0m, in \u001b[0;36mcall_api_parallel\u001b[0;34m(api_calls, fn, max_workers)\u001b[0m\n\u001b[1;32m     39\u001b[0m     futures \u001b[38;5;241m=\u001b[39m [executor\u001b[38;5;241m.\u001b[39msubmit(fn, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mapi_call) \u001b[38;5;28;01mfor\u001b[39;00m api_call \u001b[38;5;129;01min\u001b[39;00m api_calls]\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# Collect results in order of input tasks\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     results \u001b[38;5;241m=\u001b[39m [future\u001b[38;5;241m.\u001b[39mresult() \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m futures]\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "Input \u001b[0;32mIn [287]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     39\u001b[0m     futures \u001b[38;5;241m=\u001b[39m [executor\u001b[38;5;241m.\u001b[39msubmit(fn, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mapi_call) \u001b[38;5;28;01mfor\u001b[39;00m api_call \u001b[38;5;129;01min\u001b[39;00m api_calls]\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# Collect results in order of input tasks\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m futures]\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/concurrent/futures/_base.py:446\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 446\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/concurrent/futures/_base.py:391\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 391\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    393\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    394\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "Input \u001b[0;32mIn [297]\u001b[0m, in \u001b[0;36mQuestionAnswerGenerator.__call__.<locals>.printed_gpt_call\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprinted_gpt_call\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 82\u001b[0m     rsp \u001b[38;5;241m=\u001b[39m \u001b[43mcallGpt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_turbo_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28mprint\u001b[39m(rsp)\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rsp\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tenacity/__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_f\u001b[39m(\u001b[38;5;241m*\u001b[39margs: t\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: t\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mAny:\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tenacity/__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 379\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tenacity/__init__.py:326\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreraise:\n\u001b[1;32m    325\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m retry_exc\u001b[38;5;241m.\u001b[39mreraise()\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfut\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexception\u001b[39;00m()\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwait:\n\u001b[1;32m    329\u001b[0m     sleep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwait(retry_state)\n",
      "\u001b[0;31mRetryError\u001b[0m: RetryError[<Future at 0x7fe0a2e31be0 state=finished raised AssertionError>]"
     ]
    }
   ],
   "source": [
    "# doremi = create_document_index(\"https://arxiv.org/pdf/2305.10429.pdf\")\n",
    "dgn = create_document_index(\"https://vcai.mpi-inf.mpg.de/projects/DragGAN/data/paper.pdf\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0195f2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T08:58:19.847662Z",
     "start_time": "2023-05-24T08:56:18.814555Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dind = DocIndex(\"https://arxiv.org/pdf/1912.08881.pdf\", \n",
    "                \"pdf\", \n",
    "                \"scientific_article\", doc_text, full_summary)\n",
    "dind\n",
    "\n",
    "\n",
    "\n",
    "ans = dind.get_long_answer(\"What does reweighting mean in this work?\", previous_answer=previous_answer, \n",
    "                           further_questions_answers=further_questions_answers, \n",
    "                           related_questions_answers=related_questions_answers, more_answers=more_answers)\n",
    "\n",
    "print(ans[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "9c74fdce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T10:20:18.556299Z",
     "start_time": "2023-05-25T10:20:06.250648Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 48 <class 'tuple'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.DocIndex at 0x7fe0a2e31b80>"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doremi = DocIndex(\"https://arxiv.org/pdf/2305.10429.pdf\", \n",
    "                \"pdf\", \n",
    "                \"scientific_article\", doc_text, full_summary)\n",
    "doremi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "4b52852c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T10:20:34.285192Z",
     "start_time": "2023-05-25T10:20:18.558090Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 48 <class 'tuple'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.DocIndex at 0x7fe0a2e31a60>"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_text_dgn = PDFReaderTool()(\"https://vcai.mpi-inf.mpg.de/projects/DragGAN/data/paper.pdf\")\n",
    "dgn = DocIndex(\"https://vcai.mpi-inf.mpg.de/projects/DragGAN/data/paper.pdf\", \n",
    "                \"pdf\", \n",
    "                \"scientific_article\", doc_text_dgn, full_summary_dgn)\n",
    "dgn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "0e7fa6e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T16:14:15.534493Z",
     "start_time": "2023-05-25T16:14:15.390347Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dgn.save_local(\"storage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "6eec4a28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T14:57:01.976783Z",
     "start_time": "2023-05-25T14:57:01.587133Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "doremi.save_local(\"storage\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "e4c9f7df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T10:28:32.047935Z",
     "start_time": "2023-05-25T10:28:32.022699Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "doremi_v2 = DocIndex.load_local(\"storage\", \"1311701502\")\n",
    "doremi_v2.get_short_answer(\"What datasets do they experiment upon?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "f497ff12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T10:30:46.564707Z",
     "start_time": "2023-05-25T10:28:39.251266Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'DoReMi uses The Pile dataset and the GLaM dataset to test the '\n",
      "           'effectiveness of its reweighting technique. The Pile is a large '\n",
      "           'text dataset with 22 domains, while the GLaM dataset is used to '\n",
      "           'train language models across various tasks and evaluate their '\n",
      "           'performance on different domains. DoReMi optimizes domain weights '\n",
      "           'and consistently improves downstream accuracy and perplexity '\n",
      "           'across all domains and scales on both The Pile and GLaM datasets.',\n",
      " 'further_questions': [\"How does DoReMi's performance compare with other \"\n",
      "                       'reweighting techniques?',\n",
      "                       'What are the specific improvements in downstream '\n",
      "                       'accuracy and perplexity obtained by using DoReMi?'],\n",
      " 'needs_reformulation': True,\n",
      " 'parent': {'answer': 'To test if their reweighting works, DoReMi provides '\n",
      "                      'downstream results on The Pile and the GLaM datasets. '\n",
      "                      'They analyze the performance of models trained with '\n",
      "                      'DoReMi domain weights across different scales (280M, '\n",
      "                      '510M, 760M, 1B) and compare them to baseline (uniform) '\n",
      "                      'domain weights and downstream-tuned domain weights. The '\n",
      "                      'results show that DoReMi consistently improves '\n",
      "                      'downstream accuracy and perplexity across all domains '\n",
      "                      'and scales on both The Pile and GLaM datasets.',\n",
      "            'further_questions': ['How does DoReMi perform on specific domains '\n",
      "                                  'within The Pile and GLaM datasets?',\n",
      "                                  'What is the impact of proxy model size on '\n",
      "                                  \"DoReMi's performance?\"],\n",
      "            'needs_reformulation': True,\n",
      "            'parent': {'answer': 'DoReMi experiments upon The Pile dataset and '\n",
      "                                 'the GLaM dataset. The Pile is a large '\n",
      "                                 'publicly available dataset composed of 24% '\n",
      "                                 'web data, 9% Wikipedia, 4% GitHub, etc. The '\n",
      "                                 'GLaM dataset is used to train language '\n",
      "                                 'models across various tasks and evaluate '\n",
      "                                 'their performance on different domains.',\n",
      "                       'further_questions': ['How is The Pile dataset '\n",
      "                                             'structured?',\n",
      "                                             'What are the specific components '\n",
      "                                             'of the GLaM dataset?',\n",
      "                                             'How were these datasets selected '\n",
      "                                             'for the experiments?'],\n",
      "                       'query': 'What datasets do they experiment upon?',\n",
      "                       'raw_node_ids': [13, 15],\n",
      "                       'read_next_fragment': False,\n",
      "                       'read_previous_fragment': False,\n",
      "                       'related_questions': ['What is the purpose of DoReMi?',\n",
      "                                             'How does DoReMi optimize domain '\n",
      "                                             'weights?',\n",
      "                                             'What improvements were observed '\n",
      "                                             'using DoReMi?'],\n",
      "                       'short_answer_summary': 'DoReMi experiments on The Pile '\n",
      "                                               'and GLaM datasets.'},\n",
      "            'query': 'What datasets do they provide their downstream results '\n",
      "                     'on to test if their reweighting works?',\n",
      "            'raw_node_ids': [16, 8],\n",
      "            'read_next_fragment': False,\n",
      "            'read_previous_fragment': False,\n",
      "            'reformulated_query': 'What datasets are used to test the '\n",
      "                                  \"effectiveness of DoReMi's reweighting?\",\n",
      "            'related_questions': ['What are the main components of DoReMi?',\n",
      "                                  'How does DoReMi optimize domain weights?'],\n",
      "            'short_answer_summary': 'DoReMi provides downstream results on The '\n",
      "                                    'Pile and the GLaM datasets, showing '\n",
      "                                    'consistent improvements in downstream '\n",
      "                                    'accuracy and perplexity across all '\n",
      "                                    'domains and scales.'},\n",
      " 'query': \"What datasets are used to test the effectiveness of DoReMi's \"\n",
      "          'reweighting?',\n",
      " 'raw_node_ids': [22, 9],\n",
      " 'read_next_fragment': False,\n",
      " 'read_previous_fragment': False,\n",
      " 'reformulated_query': 'Which datasets are used to test the effectiveness of '\n",
      "                       \"DoReMi's reweighting technique?\",\n",
      " 'related_questions': ['What are the main components of the DoReMi algorithm?',\n",
      "                       \"How does DoReMi's reweighting transfer across \"\n",
      "                       'different model scales?'],\n",
      " 'short_answer_summary': 'DoReMi tests its reweighting effectiveness on The '\n",
      "                         'Pile and GLaM datasets, consistently improving '\n",
      "                         'downstream accuracy and perplexity.'}\n"
     ]
    }
   ],
   "source": [
    "follow_ans = doremi_v2.ask_follow_up(\"What datasets do they provide their downstream results on to test if their reweighting works?\", prev_answer)\n",
    "pprint(follow_ans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "53c58872",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T10:27:29.627847Z",
     "start_time": "2023-05-25T10:27:29.612095Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'DoReMi technique uses two datasets for its results: The Pile '\n",
      "           'dataset and the GLaM dataset. The Pile dataset is a diverse 800GB '\n",
      "           'text dataset with 22 domains, while the GLaM dataset is used to '\n",
      "           'train language models across various tasks and evaluate their '\n",
      "           'performance on different domains. These datasets are used in '\n",
      "           'DoReMi experiments to optimize domain weights for language '\n",
      "           'modeling, leading to improved training of larger models.',\n",
      " 'further_questions': ['How is the performance of DoReMi compared to other '\n",
      "                       'techniques?',\n",
      "                       'What are the specific improvements seen with DoReMi on '\n",
      "                       'The Pile and GLaM datasets?'],\n",
      " 'needs_reformulation': True,\n",
      " 'parent': {'answer': 'DoReMi provides results on two datasets: The Pile '\n",
      "                      'dataset and the GLaM dataset. The Pile dataset is an '\n",
      "                      '800GB dataset of diverse text for language modeling, '\n",
      "                      'while the GLaM dataset is used to train language models '\n",
      "                      'across various tasks and evaluate their performance on '\n",
      "                      'different domains. Additionally, the document mentions '\n",
      "                      'other datasets used in various experiments and '\n",
      "                      'references, such as TriviaQA, a large-scale '\n",
      "                      'distantly-supervised challenge dataset for reading '\n",
      "                      'comprehension, and Natural Questions, a benchmark for '\n",
      "                      'question-answering research.',\n",
      "            'further_questions': ['What are the main characteristics of The '\n",
      "                                  'Pile dataset?',\n",
      "                                  'What are the main characteristics of the '\n",
      "                                  'GLaM dataset?',\n",
      "                                  'How do the results on The Pile dataset '\n",
      "                                  'compare to the results on the GLaM '\n",
      "                                  'dataset?'],\n",
      "            'needs_reformulation': True,\n",
      "            'parent': {'answer': 'DoReMi experiments upon The Pile dataset and '\n",
      "                                 'the GLaM dataset. The Pile is a large '\n",
      "                                 'publicly available dataset composed of 24% '\n",
      "                                 'web data, 9% Wikipedia, 4% GitHub, etc. The '\n",
      "                                 'GLaM dataset is used to train language '\n",
      "                                 'models across various tasks and evaluate '\n",
      "                                 'their performance on different domains.',\n",
      "                       'further_questions': ['How is The Pile dataset '\n",
      "                                             'structured?',\n",
      "                                             'What are the specific components '\n",
      "                                             'of the GLaM dataset?',\n",
      "                                             'How were these datasets selected '\n",
      "                                             'for the experiments?'],\n",
      "                       'query': 'What datasets do they experiment upon?',\n",
      "                       'raw_node_ids': [13, 15],\n",
      "                       'read_next_fragment': False,\n",
      "                       'read_previous_fragment': False,\n",
      "                       'related_questions': ['What is the purpose of DoReMi?',\n",
      "                                             'How does DoReMi optimize domain '\n",
      "                                             'weights?',\n",
      "                                             'What improvements were observed '\n",
      "                                             'using DoReMi?'],\n",
      "                       'short_answer_summary': 'DoReMi experiments on The Pile '\n",
      "                                               'and GLaM datasets.'},\n",
      "            'query': 'What datasets do they provide their results on?',\n",
      "            'raw_node_ids': [13, 15],\n",
      "            'read_next_fragment': False,\n",
      "            'read_previous_fragment': False,\n",
      "            'reformulated_query': 'What datasets are used for results in the '\n",
      "                                  'DoReMi technique?',\n",
      "            'related_questions': ['How does DoReMi technique improve language '\n",
      "                                  'model training?',\n",
      "                                  'What are the advantages of using the DoReMi '\n",
      "                                  'technique over other methods for optimizing '\n",
      "                                  'domain weights?'],\n",
      "            'short_answer_summary': 'DoReMi provides results on The Pile '\n",
      "                                    'dataset and the GLaM dataset, as well as '\n",
      "                                    'mentioning other datasets like TriviaQA '\n",
      "                                    'and Natural Questions used in various '\n",
      "                                    'experiments and references.'},\n",
      " 'query': 'What datasets are used for results in the DoReMi technique?',\n",
      " 'raw_node_ids': [9, 6],\n",
      " 'read_next_fragment': False,\n",
      " 'read_previous_fragment': False,\n",
      " 'reformulated_query': 'What datasets are used for results in the DoReMi '\n",
      "                       'technique?',\n",
      " 'related_questions': ['What is the main goal of the DoReMi technique?',\n",
      "                       'How does DoReMi improve language model training '\n",
      "                       'efficiency?'],\n",
      " 'short_answer_summary': 'DoReMi uses The Pile dataset and the GLaM dataset '\n",
      "                         'for its results.'}\n"
     ]
    }
   ],
   "source": [
    "pprint(follow_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9d6be2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# guidance\n",
    "# reanswer\n",
    "# save and load index from disk\n",
    "# One doc chat UI\n",
    "# Add asked questions and answers to existing index\n",
    "# our chatgpt format summary\n",
    "\n",
    "# A next question may not be follow-up but still having previous question context can help?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ce2b82",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Search web (use the web preview details as well)\n",
    "\n",
    "# [Done] L2 index -> Create Cross elaborate and long answer needing section questions which take full summary + current section + prev section + next section -> Create hard and thought provoking questions -> Create Answers\n",
    "\n",
    "# L3 Index -> index references and citations of this work\n",
    "# L3 index -> Cross document questions and answers\n",
    "\n",
    "\n",
    "\n",
    "# Semantic reader -> attaches ability to read in pdf view while using our tool\n",
    "# Use answered questions as indexing as well.\n",
    "\n",
    "# User interface, like chatgpt\n",
    "# threads for a user , each thread is a paper or group of papers are connected by some logic\n",
    "# individual papers are indexed once, mostly by arxiv pdf links (L1 and L2 index)\n",
    "# L3 index is created later\n",
    "# every user can see what other arxiv pdf exist in the database\n",
    "# Every user can look at what thread collections exist. A thread collection of papers can be forked. The conversation in the thread can't be seen but pre-generated questions can be seen.\n",
    "# Index most popular tweeted papers and when someone asks a question on their own paper, you can also suggest them other similar recent papers.\n",
    "# how to give a read-only demo to a user without the user giving openai api key.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b70e63",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Other Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74a326e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-23T16:50:11.244090Z",
     "start_time": "2023-05-23T16:50:11.181752Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "class DecisionMakerTool:\n",
    "    def __init__(self):\n",
    "        self.name = \"DecisionMakerTool\"\n",
    "        self.description = \"\"\"\n",
    "DecisionMakerTool:\n",
    "    This tool helps in making decisions. If you have some alternative action choices, based on some context or prior information and need to make a decision where decision can be a choice from set of options, then this tool is useful. This DecisionMakerTool is especially useful if the decision or choice cannot be made using python if-else but rather needs language support and more nuanced intelligence and world knowledge. Can also be used as a multi-choice reading comprehension QnA tool.\n",
    "\n",
    "    Input params/args: \n",
    "        context (str): Context on which decision/choice is to be made.\n",
    "        options (str): str representating options in format of `<option_number>: <option_name>` for each option, separated by commas.\n",
    "\n",
    "    Returns: \n",
    "        dict: {\"choice_reason\": <str, reason of making the choice, pros and cons, other thoughts>, \"choosen_option\": <choosen option as int>}\n",
    "\n",
    "    Usage:\n",
    "        `choice_decision = DecisionMakerTool()(context=\"Should a person stay awake at night.\", options=[\"1: No, 2: Yes\"]) # Note: this tool needs to be initialized first.`\n",
    "        `choice_decision = DecisionMakerTool()(context=\"You are given two stories about a monk who had to survive starvation below. first story: {story_1} , second story:  {story_2}, choose the story which best motivates a person suffering from starvation.\", options=[\"1: first story, 2: second story\"])`\n",
    "\n",
    "    \"\"\"\n",
    "        \n",
    "        self.prompt = PromptTemplate(\n",
    "            input_variables=[\"context\", \"options\"],\n",
    "            template=\"\"\"\n",
    "You are a helpful decision making tool. Your responsibility is to make a decision (from a set of options) given some query/context and a set of options. You will give a single number output that signifies your decision for the query/context.\n",
    "\n",
    "The context for which you need to make a decision or choose an option is given below:\n",
    "\n",
    "{context}\n",
    "\n",
    "The options you can take is given below in format of `<option_number>: <option_name>` for each option, separated by commas:\n",
    "\n",
    "{options}\n",
    "\n",
    "Choose one option from the given options (a single number) and also provide a reason for your choice. \n",
    "Your answer is a python dictionary which has two keys (choice_reason: <str, option number and then reason of making the choice, why this particular option over other options, pros and cons, other thoughts> and choosen_option: <choosen option as int>).\n",
    "Just output a python dict with these two keys (choice_reason and choosen_option) and their values only.\n",
    "\n",
    "\n",
    "\"\"\",\n",
    "        )\n",
    "        \n",
    "    def __call__(self, context, options):\n",
    "        prompt = self.prompt.format(context=context, options=options)\n",
    "        return eval(callGpt.get_turbo_call()(prompt, temperature=0))\n",
    "    \n",
    "\n",
    "class CallLargeLanguageModelWithInstructionsTool:\n",
    "    def __init__(self):\n",
    "        self.name = \"CallLargeLanguageModelWithInstructionsTool\"\n",
    "        self.description = \"\"\"\n",
    "CallLargeLanguageModelWithInstructionsTool:\n",
    "    This tool takes instructions and some data and passes them on to a Large Language Model for further processing. It can do language oriented tasks like summarization, question-answer, question-generation, etc, it cannot do web-search, mathematics and other non-language oriented tasks. Limitation- This tool can only generate or write only 2000 words at one time. It also has input length limit, where it can only take 2000 words at a time. Giving this tool more than 2000 words in either `instructions` or `data` will result in error.\n",
    "\n",
    "    Input params/args: \n",
    "        instructions (str): instructions to the language model on what to do with given data dictionary.\n",
    "        data (dict): data dictionary which language model uses along with instructions to produce some useful result. (Optional)\n",
    "\n",
    "    Returns: \n",
    "        str: model_result (model_result is less than 2000 words always)\n",
    "\n",
    "    Usage:\n",
    "        `model_result = CallLargeLanguageModelWithInstructionsTool()(instructions=\"instructions to language model\", data=<data_dictionary for using in format string of instruction>) # Note: this tool needs to be initialized first. # model_result is less than 2K words always`\n",
    "        `dog_owner_name_text = CallLargeLanguageModelWithInstructionsTool()(instructions=\"get the name of the owner of dog from the sentence: {text}\", data={\"text\": \"the pretty brown dog was owned by Mr. Miles\"}) # length of instructions + data should be less than 2K words always`\n",
    "\n",
    "    \"\"\"\n",
    "    def __call__(self, instructions, data=dict()):\n",
    "        try:\n",
    "            prompt = instructions.format(**data)\n",
    "        except:\n",
    "            prompt = instructions + \" \\n\\n Data for following instructions in python dictionary format: \\n\\n \" + str(data)\n",
    "        return callGpt.get_turbo_call()(prompt, temperature=0.3)\n",
    "\n",
    "\n",
    "class ContextualReader:\n",
    "    def __init__(self):\n",
    "        self.name = \"ContextualReader\"\n",
    "        self.description = \"\"\"\n",
    "ContextualReader:\n",
    "    This tool takes a context/query/instruction, and a text document. It reads the document based on the context or query instruction and outputs only parts of document relevant to the query. Useful when the document is too long and you need to store a short contextual version of it for answering the user request. Sometimes rephrasing the query/question/user request before asking the ContextualReader helps ContextualReader provide better results. You can also specify directives to ContextualReader like \"return numbers only\", along with the query for better results.\n",
    "\n",
    "    Input params/args: \n",
    "        context_user_query (str): instructions or query on how to read the document to provide contextually useful content from the document.\n",
    "        text_document (str): document to read and provide information from using context_user_query.\n",
    "\n",
    "    Returns: \n",
    "        str: contextual_content_from_document\n",
    "\n",
    "    Usage:\n",
    "        `contextual_content_from_document = ContextualReader()(context_user_query=\"instructions on how to read document\", text_document=\"document to read\") # Note: this tool needs to be initialized first.`\n",
    "\n",
    "    \"\"\"\n",
    "        \n",
    "        self.prompt = PromptTemplate(\n",
    "            input_variables=[\"context\", \"document\"],\n",
    "            template=\"\"\"\n",
    "You are given a request/context/instruction which specifies what needs to be done and any other specific instructions as below \\n\n",
    "{context}\n",
    "\n",
    "Next, you are also given a document which you have to read and gather more context and information to answer the question/instruction. \n",
    "Remember you don't need to answer the user's question now, you just need to gather more information which could possibly help in answering the user's question from this document given.\n",
    "Gather the information in a very concise succint way like a scientist, not like a novelist.\n",
    "Document is given below:\n",
    "{document}\n",
    "\"\"\",\n",
    "        )\n",
    "        \n",
    "    def get_one(self, context, document):\n",
    "        prompt = self.prompt.format(context=context, document=document)\n",
    "        return callGpt.get_turbo_call()(prompt, temperature=0.4)\n",
    "        \n",
    "    \n",
    "    def get_one_with_exception(self, context, document):\n",
    "        try:\n",
    "            cleaned_text = self.get_one(context, document)\n",
    "            \n",
    "            return cleaned_text\n",
    "        except Exception as e:\n",
    "            exp_str = str(e)\n",
    "            too_long = \"maximum context length\" in exp_str and \"your messages resulted in\" in exp_str\n",
    "            if too_long:\n",
    "                return \" \".join([self.get_one_with_exception(context, st) for st in split_text(document)])\n",
    "            raise e\n",
    "            \n",
    "    def __call__(self, context_user_query, text_document, chunk_size=3400):\n",
    "        import functools\n",
    "        part_fn = functools.partial(self.get_one_with_exception, context_user_query)\n",
    "        return process_text(text_document, chunk_size, part_fn)\n",
    "    \n",
    "    \n",
    "    \n",
    "class ContextualSummarizer(ContextualReader):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.name = \"ContextualSummarizer\"\n",
    "        self.description = \"\"\"\n",
    "ContextualSummarizer:\n",
    "    Similar to Contextual_Reader, but guarantees much smaller text outputs due to summarisation of inputs. This tool takes a context/query/instruction, and a text document. It summarises the document based on the context/query/instruction and outputs only parts of document relevant to user query. Very Useful when the contextual document is too long and you need to store a short contextual version of it.\n",
    "\n",
    "    Input params/args: \n",
    "        context_user_query (str): instructions or query on how to read the document to provide summary from the document.\n",
    "        text_document (str): document to read and sumarize from using context_user_query.\n",
    "\n",
    "    Returns: \n",
    "        str: summary_from_document\n",
    "\n",
    "    Usage:\n",
    "        `summary_from_document = ContextualSummarizer()(context_user_query=\"instructions on how to read document\", text_document=\"document to summarize\") # Note: this tool needs to be initialized first.`\n",
    "\n",
    "    \"\"\"\n",
    "        self.prompt = PromptTemplate(\n",
    "            input_variables=[\"context\", \"document\"],\n",
    "            template=\"\"\"\n",
    "You are given a context/instruction which specifies what is needed and any other specific instructions as below \\n\n",
    "{context}\n",
    "\n",
    "Next, you are also given a document which you have to read and gather more context and information to answer the question/instruction. \n",
    "Remember you don't need to answer the user's question now, you just need to gather more information which could possibly help in answering the user's question from this document given.\n",
    "Gather the information in a concise way like a scientist, not like a novelist. Ensure to provide short, point wise, summarised version, we intend the output to be small but still capture all details pertaining to \"{context}\".\n",
    "Document is given below:\n",
    "{document}\n",
    "\"\"\",\n",
    "        )\n",
    "        \n",
    "    \n",
    "\n",
    "class FuseInformation:\n",
    "    def __init__(self,):\n",
    "        self.name = \"FuseInformation\"\n",
    "        self.description = \"\"\"\n",
    "FuseInformation:\n",
    "    This tool takes a context/query/instruction, and two text documents, it then reads the two documents based on the context or query instruction and outputs only parts of the documents relevant to context/instruction. Useful when you have multiple documents and their combined length is too long and you need to store a short contextual version of both documents. Limitation - This tool can only generate or write only 2000 words at one time.\n",
    "\n",
    "    Input params/args: \n",
    "        context_user_query (str): instructions or query on how to read the documents and fuse them.\n",
    "        first_document (str): first document to read.\n",
    "        second_document (str): second document to read.\n",
    "\n",
    "    Returns: \n",
    "        str: fused_content_both_documents\n",
    "\n",
    "    Usage:\n",
    "        `fused_content_both_documents = FuseInformation()(context_user_query=\"instructions on how to read document\", first_document=\"first document to read\", second_document=\"second document to read\") # Note: this tool needs to be initialized first.`\n",
    "\n",
    "    \"\"\"\n",
    "        \n",
    "        self.prompt = PromptTemplate(\n",
    "            input_variables=[\"context\", \"first_document\", \"second_document\"],\n",
    "            template=\"\"\"\n",
    "You are given a request/context/query which specifies what needs to be done and any other specific instructions as below \\n\n",
    "{context}\n",
    "\n",
    "Next, you are also given a two documents which you have to read and gather more context and information to answer the question. \n",
    "Remember you don't need to answer the user's question now, you just need to gather more information which could possibly help in answering the user's question from this document given.\n",
    "Also remember to read both documents and find relevant information from both of them.\n",
    "Documents are given below:\n",
    "First Document:\n",
    "{first_document}\n",
    "\n",
    "\\n\\n\n",
    "Second Document:\n",
    "{second_document}\n",
    "\"\"\",\n",
    "        )\n",
    "    def __call__(self, context_user_query, first_document, second_document):\n",
    "        prompt = self.prompt.format(context=context_user_query, first_document=first_document, second_document=second_document)\n",
    "        return callGpt.get_hard_call()(prompt, temperature=0.2)\n",
    "        \n",
    "    \n",
    "class ContextualAnswer:\n",
    "    def __init__(self):\n",
    "        self.name = \"ContextualAnswer\"\n",
    "        self.description = \"\"\"\n",
    "ContextualAnswer:\n",
    "    This tool takes a context/query/instruction, and one text document, it then reads the document based on the context/query/instruction and provides an answer to the query/instruction using the document and its own knowledge. If no information/answer is found on requested query it says \"no answer\" in output.\n",
    "\n",
    "    Input params/args: \n",
    "        context_user_query (str): instructions or query on how to read the document to provide an answer from the document.\n",
    "        text_document (str): document to read and answer from.\n",
    "\n",
    "    Returns: \n",
    "        str: answer\n",
    "\n",
    "    Usage:\n",
    "        `answer = ContextualAnswer()(context_user_query=\"instructions on how to read document and answer\", text_document=\"document to read\") # Note: this tool needs to be initialized first.`\n",
    "\n",
    "    \"\"\"\n",
    "        \n",
    "        self.prompt = PromptTemplate(\n",
    "            input_variables=[\"context\", \"document\"],\n",
    "            template=\"\"\"\n",
    "You are given a context/instruction/query which specifies what is needed and any other specific instructions as below \\n\n",
    "\n",
    "{context}\n",
    "\n",
    "Next, you are also given a document which has some context which can help in answering the query or help find the right information.\n",
    "Remember if you can't answer the question given your own knowledge and the document, say that you can't answer.\n",
    "You can use the provided document as a support for your answer but you can also use your own prior knowledge. \n",
    "Answer the query based on the document and usually keep answers `short` unless asked to `elaborate`. If no information/answer is found on requested query say \"no answer\" in your output.\n",
    "Document is given below:\n",
    "{document}\n",
    "\n",
    "Answer:\n",
    "\n",
    "\"\"\",\n",
    "        )\n",
    "    def __call__(self, context_user_query, text_document):\n",
    "        prompt = self.prompt.format(context=context_user_query, document=text_document)\n",
    "        return callGpt.get_turbo_call()(prompt, temperature=0.2)\n",
    "    \n",
    "class ExtractInformationTool:\n",
    "    def __init__(self):\n",
    "        self.name = \"ExtractInformationTool\"\n",
    "        self.description = \"\"\"\n",
    "ExtractInformationTool:\n",
    "    This tool takes a context/query/instruction, and one text document, it then reads the document based on the context/query/instruction and extracts a single piece of information. If no information/answer is found on requested query it says \"no answer\" in output. To use this tool ask it pin pointed (not vague) questions (e.g. if you need bottle capacity, ask - \"capacity in litres\" not \"size of the bottle\",). Another rule is to ask just for one ( or a single piece of ) information (e.g. don't ask for name and place in same Tool call, if you need two pieces of information, call the tool twice with separate query each time).\n",
    "\n",
    "    Input params/args: \n",
    "        context_user_query (str): instructions or query on what information to extract from document.\n",
    "        text_document (str): document to read and extract information.\n",
    "\n",
    "    Returns: \n",
    "        str: answer\n",
    "\n",
    "    Usage:\n",
    "        `answer = ExtractInformationTool()(context_user_query=\"instructions on how to read document and extract a single piece of information\", text_document=\"document to read\") # Note: this tool needs to be initialized first.`\n",
    "\n",
    "    \"\"\"\n",
    "        \n",
    "        self.prompt = PromptTemplate(\n",
    "            input_variables=[\"context\", \"document\"],\n",
    "            template=\"\"\"\n",
    "You are given a context/instruction/query which specifies what is needed and any other specific instructions as below \\n\n",
    "{context}\n",
    "\n",
    "Next, you are also given a document which has some context which can help in answering the query or help find the right information.\n",
    "You need to extract the information from the provided document only. \n",
    "Usually the extracted information should be very short (one word or few word answers are preferred). \n",
    "If you are asked a number just give the number, if you are asked a name, just give the name, in general just the information or answer, no platitudes or preambles. \n",
    "Example: if answer is \"The number of people on train is 67\" -> just output \"67\".\n",
    "If no information/answer is found on requested query say \"no answer\" in your output.\n",
    "Document is given below:\n",
    "{document}\n",
    "\n",
    "\"\"\",\n",
    "        )\n",
    "    def __call__(self, context_user_query, text_document):\n",
    "        prompt = self.prompt.format(context=context_user_query, document=text_document)\n",
    "        return callGpt.get_turbo_call()(prompt, temperature=0.2)\n",
    "\n",
    "\n",
    "class QuestionGeneration:\n",
    "    def __init__(self):\n",
    "        self.name = \"QuestionGeneration\"\n",
    "        self.description = \"\"\"\n",
    "QuestionGeneration:\n",
    "    This tool takes a text document and summarizes it into a shorter version while preserving the main points and context. Useful when the document is too long and needs to be shortened before further processing.\n",
    "\n",
    "    Input params/args: \n",
    "        long_document (str): document to summarize.\n",
    "\n",
    "    Returns: \n",
    "        str: summarized_document.\n",
    "\n",
    "    Usage:\n",
    "        `summary = LongSummarizer()(text_document=\"document to summarize\") # Note: this tool needs to be initialized first.`\n",
    "    \"\"\"\n",
    "        \n",
    "        self.prompt = PromptTemplate(\n",
    "            input_variables=[\"document\"],\n",
    "            template=\"\"\"Write as many valid and important question-answer pairs as can be answered/derived from the document below:\n",
    "{document}\n",
    "\n",
    "Separate the question-answer pairs by newline \\\\n and also put each question and answer in a newline.\n",
    "\n",
    "Questions and Answers:\n",
    "\n",
    "\"\"\",\n",
    "        )\n",
    "        \n",
    "    def __call__(self, document):\n",
    "        prompt = self.prompt.format(document=document)\n",
    "        try:\n",
    "            resp = call_ai21(prompt, temperature=0.5)\n",
    "            resp = resp.split('\\n')\n",
    "            assert len(resp)%2==0\n",
    "        except:\n",
    "            resp = call_ai21(prompt, temperature=0.4)\n",
    "            resp = resp.split('\\n')\n",
    "            assert len(resp)%2==0\n",
    "        resp = concat_array_two_at_a_time(resp)\n",
    "        return resp\n",
    "        \n",
    "        \n",
    "\n",
    "[QuestionGeneration()(chk) + QuestionGeneration()(chk_sum) for chk, chk_sum in zip(full_summary[\"chunks\"], full_summary[\"chunked_summary\"])]\n",
    "    \n",
    "\n",
    "        \n",
    "        \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
