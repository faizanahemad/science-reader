{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28db1c39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T10:04:51.305779Z",
     "start_time": "2023-06-03T10:04:46.145789Z"
    }
   },
   "outputs": [],
   "source": [
    "from DocIndex import DocIndex, create_document_index, CallGpt\n",
    "from typing import List, Dict\n",
    "import glob\n",
    "from langchain.prompts import PromptTemplate\n",
    "import os\n",
    "import tiktoken\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bd89cfd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T17:00:14.872826Z",
     "start_time": "2023-05-26T17:00:09.853561Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "indexed_docs: Dict[str, DocIndex] = {}\n",
    "bm25_corpus: List[List[str]] = []\n",
    "doc_id_to_bm25_index: Dict[str, int] = {}\n",
    "def load_documents(folder):\n",
    "    global indexed_docs, bm25_corpus, doc_id_to_bm25_index\n",
    "    for filepath in glob.glob(os.path.join(folder, '*.index')):\n",
    "        filename = os.path.basename(filepath)\n",
    "        doc_index = DocIndex.load_local(folder, filename)\n",
    "        indexed_docs[doc_index.doc_id] = doc_index\n",
    "\n",
    "streaming_more_details = PromptTemplate(\n",
    "            input_variables=[\"query\", \"answer\", \"fragment\", \"summary\", \"full_summary\", \"questions_answers\"],\n",
    "            template=\"\"\"\n",
    "Our user had previously asked us a query to answer from a large document. \n",
    "We had answered the question and now the user has asked a for more details on the same query and answer.\n",
    "You will help further details from context (text chunks of larger document) you are provided. \n",
    "The provided context is part/fragment of a larger main document.\n",
    "\n",
    "The original question or request is given below:\n",
    "\n",
    "{query}\n",
    "\n",
    "\n",
    "You are also given the summary of the larger main document below:\n",
    "\n",
    "{full_summary}\n",
    "\n",
    "You are given few text chunks from within the document to answer this question as below:\n",
    "\n",
    "{fragment}\n",
    "\n",
    "Next, You are given few question and answer pairs from the document below:\n",
    "\n",
    "{questions_answers}\n",
    "\n",
    "You are also given summarized text chunks of certain parts of document below:\n",
    "\n",
    "{summary}\n",
    "\n",
    "\n",
    "Answer elaborately providing as much detail as possible. Remember the user is seeking further information over what is already provided in your previous answer. Continue expanding on your previous answer with more details and elaboration. Be detail oriented.\n",
    "\n",
    "Question: {query}\n",
    "Answer till now: {answer}\n",
    "\n",
    "\"\"\",\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bc1323",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb8ca809",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T17:00:30.600602Z",
     "start_time": "2023-05-26T17:00:30.586354Z"
    }
   },
   "outputs": [],
   "source": [
    "load_documents(\"storage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4188e5aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T17:00:31.219234Z",
     "start_time": "2023-05-26T17:00:31.210041Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['query', 'answer', 'fragment', 'summary', 'full_summary', 'questions_answers'], output_parser=None, partial_variables={}, template='\\nOur user had previously asked us a query to answer from a large document. \\nWe had answered the question and now the user has asked a for more details on the same query and answer.\\nYou will help further details from context (text chunks of larger document) you are provided. \\nThe provided context is part/fragment of a larger main document.\\n\\nThe original question or request is given below:\\n\\n{query}\\n\\n\\nYou are also given the summary of the larger main document below:\\n\\n{full_summary}\\n\\nYou are given few text chunks from within the document to answer this question as below:\\n\\n{fragment}\\n\\nNext, You are given few question and answer pairs from the document below:\\n\\n{questions_answers}\\n\\nYou are also given summarized text chunks of certain parts of document below:\\n\\n{summary}\\n\\n\\nAnswer elaborately providing as much detail as possible. Remember the user is seeking further information over what is already provided in your previous answer. Continue expanding on your previous answer with more details and elaboration. Be detail oriented.\\n\\nQuestion: {query}\\nAnswer till now: {answer}\\n\\n', template_format='f-string', validate_template=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexed_docs\n",
    "\n",
    "indexed_docs['1311701502'].streaming_more_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c83f6ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T17:00:27.361198Z",
     "start_time": "2023-05-26T17:00:27.357215Z"
    }
   },
   "outputs": [],
   "source": [
    "setattr(indexed_docs['1311701502'], \"streaming_more_details\", streaming_more_details)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "241fb281",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T17:00:28.113608Z",
     "start_time": "2023-05-26T17:00:28.030331Z"
    }
   },
   "outputs": [],
   "source": [
    "indexed_docs['1311701502'].save_local(\"storage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb495fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick Index -> Show a running summary being written for whole paper (use the streaming_more_details style)\n",
    "    # Make it in same view but keep QnA button disabled first.\n",
    "# Similar Papers and web search\n",
    "\n",
    "# Streaming -> methodology and other stuff as well. Just like summary\n",
    "\n",
    "# Use the new title and summary in your queries\n",
    "\n",
    "# Check duplicates for new doc addition\n",
    "\n",
    "# Enable search\n",
    "\n",
    "# Web search , references and citations\n",
    "\n",
    "# Collections\n",
    "\n",
    "# Long Aspect read\n",
    "\n",
    "\n",
    "\n",
    "# OpenAI api fault tolerance\n",
    "\n",
    "# Clean-up\n",
    "\n",
    "# Resilient Later indexing\n",
    "\n",
    "# Multi-account and Locks on API for concurrency\n",
    "\n",
    "# Crowd sourced content and qna is important, People should be able to answer as well.\n",
    "\n",
    "# Upvote downvote for an answer, Comments for an answer.\n",
    "\n",
    "# Search, Reader and Cites + Refs Ability.\n",
    "\n",
    "# User id and user level conversations or forums for reading -> Peer reading\n",
    "\n",
    "# Use twitter to get relevant conversations for a paper/work to seed it.\n",
    "\n",
    "# Voice Inputs\n",
    "\n",
    "# Second reading (First Reading anyway has to be full?)\n",
    "\n",
    "# what does the first part or first chunk or \"x\" part of document say\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa0c3ed",
   "metadata": {},
   "source": [
    "https://replit.com/\n",
    "https://github.com/mckaywrigley/chatbot-ui\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6286f37f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-12T15:47:15.697492Z",
     "start_time": "2023-06-12T15:47:02.661702Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "pdf_url = \"https://arxiv.org/pdf/2304.05187.pdf\"\n",
    "loader = PyPDFLoader(pdf_url)\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4d152868",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-12T15:53:39.261512Z",
     "start_time": "2023-06-12T15:53:28.053821Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20931"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import PDFMinerLoader\n",
    "loader = PDFMinerLoader(pdf_url)\n",
    "data = loader.load()\n",
    "len(enc.encode(\" \".join([p.page_content for p in data])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "076a6c0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-12T15:48:27.620041Z",
     "start_time": "2023-06-12T15:48:09.948319Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='3 2 0 2\\n\\nr p A 1 1\\n\\n]\\n\\nG L . s c [\\n\\n1 v 7 8 1 5 0 . 4 0 3 2 : v i X r a\\n\\nAutomatic Gradient Descent: Deep Learning without Hyperparameters\\n\\nJeremy Bernstein‹ MIT\\n\\nChris Mingard‹ U. Oxford\\n\\nKevin Huang U. Washington\\n\\nNavid Azizan MIT\\n\\nYisong Yue Caltech\\n\\n‹ denotes equal contribution.\\n\\nAbstract\\n\\nThe architecture of a deep neural network is deﬁned explicitly in terms of the number of layers, the width of each layer and the general network topology. Existing optimisation frameworks neglect this information in favour of implicit architectural information (e.g. second-order methods) or architecture-agnostic distance functions (e.g. mirror descent). Meanwhile, the most popular optimiser in practice—Adam—is based on heuristics. This paper builds a new framework for deriving optimisation algorithms that explicitly leverage neural architecture. The theory extends mirror descent to non-convex composite objective functions: the idea is to transform a Bregman divergence to account for the non-linear structure of neural architecture. Working through the details for deep fully-connected networks yields automatic gradient descent: a ﬁrst-order optimiser without any hyperparameters. Automatic gradient descent trains both fully-connected and convolutional networks out-of-the-box and at ImageNet scale. A PyTorch implementation is available at https://github.com/jxbz/agd and also in Appendix B. Overall, the paper supplies a rigorous theoretical foundation for a next-generation of architecture-dependent optimisers that work automatically and without hyperparameters.\\n\\nKeywords: majorise-minimise meta-algorithm, operator perturbation theory, architecture-aware optimisation\\n\\nContents\\n\\n1 Introduction\\n\\n1.1 Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.2 Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\\n2 Majorise-Minimise for Generic Learning Problems\\n\\n2.1 Decomposition of linearisation error\\n\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\\n2.2 Functional expansion and functional majorisation . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\\n2.3 Recovering existing frameworks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\\n3 Majorise-Minimise for Deep Learning Problems\\n\\n3.1 Deriving automatic gradient descent\\n\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\\n3.2 Convergence analysis\\n\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\\n3.3 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\\n4 Discussion\\n\\nA Proofs\\n\\nB PyTorch Implementation\\n\\n2 3 4\\n\\n5\\n\\n5\\n\\n7\\n\\n7\\n\\n8\\n\\n8\\n\\n10\\n\\n11\\n\\n12\\n\\n18\\n\\n23\\n\\nFigure 1: Automatic gradient descent trains neural networks reliably without hyperparameters. Solid lines show train accuracy and dotted lines show test accuracy. The networks are unregularised with biases and aﬃne parameters disabled, as these features are not yet supported by AGD. In the left panel—unlike AGD—Adam and SGD failed to train a 32-layer fully-connected network on CIFAR-10 with their default learning rates of 0.001 for Adam and 0.1 for SGD. The middle panel displays a learning rate grid search for ResNet-18 trained on CIFAR-10. AGD attained performance comparable to the best tuned performance of Adam and SGD. In the right panel, AGD trained ResNet-50 on ImageNet to a top-1 test accuracy of 65.5%. The ImageNet baseline is SGD with a learning rate of 0.1 and no learning rate decay schedule.\\n\\n1 Introduction\\n\\nAutomatic diﬀerentiation has contributed to the rapid pace of innovation in the ﬁeld of deep learning. Software packages such as PyTorch (Paszke et al., 2019) and Theano (Al-Rfou et al., 2016) have advanced a programming paradigm where the user (1) deﬁnes a neural network architecture by composing diﬀerentiable operators and (2) supplies training data. The package then automatically computes the gradient of the error on the training data via recursive application of the chain rule. At this point, the user must become involved again by (3) selecting one of numerous optimisation algorithms and (4) manually tuning its hyperparameters: in particular, the initial learning rate and the learning rate decay schedule (Goodfellow et al., 2016).\\n\\nBut manually tuning hyperparameters is irksome. An abundance of hyperparameters makes it diﬃcult to rank the performance of diﬀerent deep learning algorithms (Lucic et al., 2017; Schmidt et al., 2021) and diﬃcult to reproduce results in the literature (Henderson et al., 2018). Hyperparameters confound our eﬀorts to build a scientiﬁc understanding of generalisation in deep learning (Jiang et al., 2020; Farhang et al., 2022). And, when training neural networks at the largest scale, in pursuit of stronger forms of artiﬁcial intelligence, hyperparameter grid search can rack up millions of dollars in compute costs (Sharir et al., 2020).\\n\\nAre hyperparameters just a fact of life? The thesis of this paper is that no: they are not. Deep learning involves ﬁtting a known function to known data via minimising a known objective. If we could characterise these components both individually and in how they interact, then—in principle—there should be no leftover degrees of freedom to be tuned (Orabona & Cutkosky, 2020). Taking this idea and running with it leads to automatic gradient descent (AGD): a neural network optimiser without any hyperparameters. AGD is complementary to automatic diﬀerentiation and could help to automate general machine learning workﬂows.\\n\\nTwo existing tools are central to our derivation, and it is their novel combination that presents the main theoretical contribution of this paper. First, a classic tool from convex analysis known as the Bregman divergence (Bregman, 1967; Dhillon & Tropp, 2008) is used to characterise how the neural network interacts with the loss function. And second, a tool called deep relative trust (Bernstein et al., 2020) is used to characterise the highly non-linear interaction between the weights and the network output. With these tools\\n\\n2\\n\\nOptimiser\\n\\nReference\\n\\nHyperparameter Free\\n\\nWidth Scaling\\n\\nDepth Scaling\\n\\nAutomatic Schedule\\n\\nMemory Cost\\n\\nKingma & Ba (2015)\\n\\nAdam SGD + mom. Bottou et al. (2018) SGD + muP AGD\\n\\nYang & Hu (2021) this paper\\n\\n(cid:55) (cid:55) (cid:55) (cid:51)\\n\\n(cid:55) (cid:55) (cid:51) (cid:51)\\n\\n(cid:55) (cid:55) (cid:55) (cid:51)\\n\\n(cid:55) (cid:55) (cid:55) (cid:51)\\n\\n3 ˆ #weights 2 ˆ #weights 1 ˆ #weights 1 ˆ #weights\\n\\nTable 1: Comparing practical optimisers. Adam and momentum-SGD employ running estimates of gradient statistics and thereby use more memory than AGD. In addition, Adam and SGD do not provide guidance on scaling hyperparameters with network architecture, although muP ﬁxes this for the case of width scaling.\\n\\nin hand, we can apply the majorise-minimise meta-algorithm (Lange, 2016) to derive an optimiser explicitly tailored to deep network objective functions. To summarise, the derivation of AGD follows three main steps:\\n\\nStep 1: Functional expansion. We use a Bregman divergence to express the linearisation error of the objective\\n\\nfunction\\n\\npwq in terms of the functional perturbation ∆f to the network f .\\n\\nL\\n\\nStep 2: Architectural perturbation bounds. We use deep relative trust to relate the size and structure of the\\n\\nweight perturbation ∆w to the size of the induced functional perturbation ∆f .\\n\\nStep 3: Majorise-minimise. We substitute deep relative trust into the Bregman divergence to obtain an explicitly architecture-dependent majorisation. Minimising with respect to ∆w yields an optimiser.\\n\\nSummary of contributions This paper derives automatic gradient descent (AGD) by applying the majorise- minimise meta-algorithm to deep network objective functions. AGD trains all tested network architectures without hyperparameters, and scales to deep networks such as ResNet-50 and large datasets such as ImageNet. AGD trains out-of-the-box even when Adam and SGD fail to train with their default hyperparameters.\\n\\n1.1 Related work\\n\\npwq—in particular, the gradient\\n\\nOptimisation theory First-order optimisers leverage the ﬁrst-order Taylor expansion of the objective function pwq. Theoretical treatments include mirror descent (Nemirovsky & L Yudin, 1983), natural gradient descent (Amari, 1998) and the Gauss-Newton method (Björck, 1996). These methods have been explored in the context of deep learning (Pascanu & Bengio, 2014; Azizan & Hassibi, 2019; Sun et al., 2022). First-order methods are amenable to deep learning since the gradient of the objective is available via recursive application of the chain rule—a.k.a. error back-propagation (Rumelhart et al., 1986).\\n\\n∇\\n\\nL\\n\\nw\\n\\npwq—in Second-order optimisers leverage the second-order Taylor expansion of the objective function pwq. Examples include Newton’s method (Nocedal & particular, the gradient Wright, 1999) and cubic-regularised Newton’s method (Nesterov & Polyak, 2006). Naïvely, second-order methods are less amenable to deep learning since the cost of the relevant Hessian computations is prohibitive at high dimension. That being said, eﬀorts have been made to circumvent this issue (Agarwal et al., 2017).\\n\\npwq and Hessian\\n\\n2 wL\\n\\n∇\\n\\n∇\\n\\nL\\n\\nL\\n\\nw\\n\\nThe majorise-minimise meta-algorithm (Lange, 2016) is an algorithmic pattern that can be used to derive optimisers. To apply the meta-algorithm, one must ﬁrst derive an upper bound on the objective which matches the objective up to kth-order in its Taylor series for some integer k. This majorisation can then be minimised as a proxy for reducing the original objective. Figure 2 illustrates the meta-algorithm for k “ 1.\\n\\nDeep learning theory The Lipschitz smoothness assumption—a global constraint on the eigenvalues of the Hessian—is often used to derive and analyse neural network optimisers (Agarwal et al., 2016). But this assumption has been questioned (Zhang et al., 2020) and evidence has even been found for the reverse relationship, where the Hessian spectrum is highly sensitive to the choice of optimiser (Cohen et al., 2021).\\n\\nThese considerations motivate the development of theory that is more explicitly tailored to neural architecture. For instance, Bernstein et al. (2020) used an architectural perturbation bound termed deep relative trust to\\n\\n3\\n\\nperturbation applied by optimiser\\n\\n∆w\\n\\nobjective\\n\\nloss\\n\\nmodel\\n\\nL\\n\\n∆\\n\\nL\\n\\n‘\\n\\nf\\n\\nw\\n\\nweights\\n\\n∆‘\\n\\n∆f\\n\\nlooooooooooooooooooooomooooooooooooooooooooon\\n\\nperturbations induced by optimiser\\n\\nFigure 2: Majorise-minimise and the perturbation hierarchy. The left panel depicts the majorise-minimise meta-algorithm (Lange, 2016), which is an algorithmic pattern for reducing an objective (blue) by minimising a sequence of upper bounds (one shown in red). The upper bounds, known as a majorisation, must lie tangent to the objective to guarantee an improvement in one step of the meta-algorithm. The right panel depicts the perturbation hierarchy of a generic machine learning model: the optimiser perturbs the weights and this induces perturbations to the model output, the loss on individual training examples and ultimately the overall objective. Majorising machine learning objective functions requires addressing the full perturbation hierarchy.\\n\\ncharacterise the neural network optimisation landscape as a function of network depth. Similarly, Yang & Hu (2021) sought to understand the role of width, leading to their maximal update parameterisation. Tables 1 and 2 provide some points of comparison between automatic gradient descent and these and other frameworks.\\n\\n1.2 Preliminaries\\n\\nGiven a vector v in Rn, we will need to measure its size in three diﬀerent ways: Deﬁnition 1 (Manhattan norm) The Manhattan norm } ¨ }1 of a vector v is deﬁned by }v}1 :“ Deﬁnition 2 (Euclidean norm) The Euclidean norm } ¨ }2 of a vector v is deﬁned by }v}2 :“ Deﬁnition 3 (Inﬁnity norm) The inﬁnity norm } ¨ }8 of a vector v is deﬁned by }v}8 :“ maxi |vi|.\\n\\nř ař\\n\\ni |vi|. i v2 i .\\n\\nFor a matrix M in Rmˆn, the reader should be aware that it has a singular value decomposition: Fact 1 (SVD) Every matrix M in Rmˆn admits a singular value decomposition (SVD) of the form M “ ř i where the left singular vectors tuiu are orthonormal vectors in Rm, the right singular\\n\\nσipM q ¨ uivJ\\n\\nvectors tviu are orthonormal vectors in Rm and the singular values tσipM qu are non-negative scalars.\\n\\nminpm,nq i“1\\n\\nThe singular value decomposition allows us to measure the size of a matrix in two diﬀerent ways: ař Deﬁnition 4 (Frobenius norm) The Frobenius norm } ¨ }F of a matrix M is given by }M }F :“ Deﬁnition 5 (Operator norm) The operator norm } ¨ }˚ of a matrix M is given by }M }˚ :“ maxi σipM q.\\n\\ni σipM q2.\\n\\na\\n\\nWhile the operator norm }M }˚ reports the largest singular value, the quantity }M }F { minpm, nq reports the root mean square singular value. Finally, we will need to understand two aspects of matrix conditioning: Deﬁnition 6 (Rank) The rank of a matrix counts the number of non-zero singular values. Deﬁnition 7 (Stable rank) The stable rank of a matrix M is deﬁned by rankstable M :“ }M }2\\n\\nF {}M }2 ˚.\\n\\nThe stable rank provides an approximation to the rank that ignores the presence of very small singular values. Let us consider the extremes. An orthogonal matrix O P Rmˆn has both full rank and full stable rank: rank O “ rankstable O “ minpm, nq. A rank-one matrix P has unit stable rank and satisﬁes }P }˚ “ }P }F .\\n\\n4\\n\\nHandles the Loss\\n\\nNon-Linear Network\\n\\nTheory\\n\\nReference\\n\\nL\\n\\nmirror descent Gauss-Newton method natural gradient descent neural tangent kernel deep relative trust tensor programs automatic gradient descent\\n\\nNemirovsky & Yudin (1983) Björck (1996) Amari (1998) Jacot et al. (2018) Bernstein et al. (2020) Yang & Hu (2021) this paper\\n\\n‘\\n\\n(cid:51) (cid:51) (cid:51) (cid:51) (cid:55) (cid:55) (cid:51)\\n\\nf\\n\\nf\\n\\nw\\n\\n(cid:55) (cid:55) (cid:55) (cid:55) (cid:51) (cid:51) (cid:51)\\n\\nTable 2: Comparing popular frameworks for ﬁrst-order optimisation theory. Frameworks diﬀer in whether they can handle the interaction between the model output f and the objective , and the complex non-linear interaction between the weights w and the model output f . Our framework handles both aspects.\\n\\nL\\n\\n2 Majorise-Minimise for Generic Learning Problems\\n\\nThis section develops a framework for applying the majorise-minimise meta-algorithm to generic optimisation problems in machine learning. In particular, the novel technique of functional expansion is introduced. Section 3 will apply this technique to deep neural networks. All proofs are supplied in Appendix A.\\n\\nGiven a machine learning model and a set of training data, our objective is to minimise the error of the model, averaged over the training data. Formally, we would like to minimise the following function: Deﬁnition 8 (Composite objective) Consider a machine learning model f that maps an input x and a weight vector w to output f px; wq. Given data S and a convex loss function ‘, the objective\\n\\npwq is deﬁned by:\\n\\nL\\n\\npwq :“\\n\\nL\\n\\n1 |S|\\n\\nÿ\\n\\npx,yqPS\\n\\n‘pf px; wq, yq.\\n\\nWe refer to this objective as composite since the loss function ‘ is composed with a machine learning model f . While the loss function itself is convex, the overall composite is often non-convex due to the non-linear machine learning model. Common convex loss functions include the square loss and the cross-entropy loss: Example 1 (Square loss) The square loss is deﬁned by: ‘pf px; wq, yq :“ 1 2dL Example 2 (Xent loss) The cross-entropy (xent) loss is deﬁned by: ‘pf pxq, yq :“ ´ logrsoftmaxpf pxqqsJy, where the softmax function is deﬁned by softmaxpf pxqq :“ exp f pxq{}exp f pxq}1.\\n\\n}f px; wq ´ y}2 2.\\n\\n2.1 Decomposition of linearisation error\\n\\nFirst-order optimisers leverage the linearisation of the objective at the current iterate. To design such methods, we must understand the realm of validity of this linearisation. To that end, we derive a very general decomposition of the linearisation error of a machine learning system. The result is stated in terms of a perturbation hierarchy. In particular, perturbing the weight vector of a machine learning model w Ñ w ` ∆w induces perturbations to the model output f Ñ f ` ∆f , to the loss on individual data samples ‘ Ñ ‘ ` ∆‘ ` ∆ and, at last, to the overall objective function\\n\\n. Formally, a weight perturbation ∆w induces:\\n\\nÑ\\n\\nL\\n\\nL\\n\\nL\\n\\n∆f pxq ∆‘pf pxq, yq pwq\\n\\n∆\\n\\nL\\n\\n:“ f px; w ` ∆wq ´ f px; wq; :“ ‘pf pxq ` ∆f pxq, yq ´ ‘pf pxq, yq; :“ 1 px,yqPS ∆‘pf pxq, yq. |S|\\n\\nř\\n\\n(functional perturbation)\\n\\n(loss perturbation)\\n\\n(objective perturbation)\\n\\nWe have adopted a compact notation where the dependence of f px; wq on w is at times suppressed. The perturbation hierarchies of a generic machine learning model and a deep neural network are visualised in Figures 2 and 3, respectively. The linearisation error of the objective perturbation ∆\\n\\ndecomposes as:\\n\\nL\\n\\n5\\n\\nperturbations applied by optimiser hkkkkkkkkkkkkkkkkkkkkkkkikkkkkkkkkkkkkkkkkkkkkkkj\\n\\n∆WL\\n\\n∆W2\\n\\n∆W1\\n\\nobjective\\n\\nloss\\n\\noutput\\n\\nL\\n\\n∆\\n\\nL\\n\\n‘\\n\\nf\\n\\nWL\\n\\nW2\\n\\nW1\\n\\nlayer L\\n\\nlayer 2\\n\\nlayer 1\\n\\n∆‘\\n\\n∆f\\n\\nlooooooooooooooooooooooomooooooooooooooooooooooon\\n\\nperturbations induced by optimiser\\n\\nFigure 3: Perturbation hierarchy of a deep neural network. When training a neural network, the optimiser applies structured perturbations to the weights, in the form of one perturbation matrix ∆Wk per weight matrix Wk. Deep relative trust (Bernstein et al., 2020) provides a tool to understand how structured weight perturbations of this form aﬀect the network output f . Combining deep relative trust with a Bregman divergence (Bregman, 1967) allows us to analyse the full perturbation hierarchy.\\n\\nProposition 1 (Decomposition of linearisation error) For any diﬀerentiable loss ‘ and any diﬀerentiable ma- chine learning model f the linearisation error of the objective function admits the following decomposition:\\n\\npwq ´\\n\\npwqJ∆w ∆ looooooooooooooomooooooooooooooon w ∇ linearisation error of objective\\n\\nL\\n\\nL\\n\\n“\\n\\n1 |S|\\n\\n1 |S|\\n\\n`\\n\\nÿ\\n\\npx,yqPS ÿ\\n\\npx,yqPS\\n\\nL ∇f pxq‘pf pxq, yqJ r∆f pxq ´\\n\\nwf pxq∆ws loooooooooooooomoooooooooooooon ∇ linearisation error of model\\n\\n∇f pxq‘pf pxq, yqJ∆f pxq ∆‘pf pxq, yq ´ looooooooooooooooooooooooomooooooooooooooooooooooooon\\n\\n.\\n\\nlinearisation error of loss\\n\\nIn words: the linearisation error of the objective decomposes into two terms. The ﬁrst depends on the linearisation error of the machine learning model and the second the loss. This decomposition relies on nothing but diﬀerentiability. For a convex loss, the second term may be interpreted as a Bregman divergence: Deﬁnition 9 (Bregman divergence of loss) For any convex loss ‘:\\n\\nbregman‘p¨,yqpf pxq, ∆f pxqq :“ ∆‘pf pxq, yq ´\\n\\n∇f pxq‘pf pxq, yqJ∆f pxq.\\n\\nA Bregman divergence is just the linearisation error of a convex function. Two important examples are: Lemma 1 (Bregman divergence of square loss) When ‘ is set to square loss, then:\\n\\nbregman‘p¨,yqpf pxq, ∆f pxqq “ 1 2dL\\n\\n}∆f pxq}2 2.\\n\\nLemma 2 (Bregman divergence of xent loss) When ‘ is set to cross-entropy loss, and if yJ1 “ 1, then:\\n\\nbregman‘p¨,yqpf pxq, ∆f pxqq “ DKL\\n\\nsoftmaxpf pxqq\\n\\n´\\n\\nˇ ˇ ˇ\\n\\nˇ ˇ ˇ softmaxpf pxq ` ∆f pxqq\\n\\n¯\\n\\nď 1\\n\\n2 }∆f pxq}2\\n\\n8 `\\n\\np∆f 3q.\\n\\nO\\n\\nOur methods may be applied to other convex losses by calculating or bounding their Bregman divergence.\\n\\n6\\n\\n2.2 Functional expansion and functional majorisation\\n\\nBefore continuing, we make one simplifying assumption. Observe that the ﬁrst term on the right-hand side of Proposition 1 is a high-dimensional inner product between two vectors. Since there is no clear reason why these two vectors should be aligned, let us assume that their inner product is zero: Assumption 1 (Orthogonality of model linearisation error) In the same setting as Proposition 1:\\n\\n1 |S|\\n\\nÿ\\n\\n∇f pxq‘pf pxq, yqJ r∆f pxq ´\\n\\nwf pxq∆ws loooooooooooooomoooooooooooooon ∇ linearisation error of model\\n\\n“ 0.\\n\\npx,yqPS\\n\\nWhile it is possible to work without this assumption (Bernstein, 2022), we found that its inclusion simpliﬁes the analysis and in practice did not lead to a discernible weakening of the resulting algorithm. In any case, this assumption is considerably milder than the common assumption in the literature (Pascanu & Bengio, 2014; Lee et al., 2019) that the model linearisation error is itself zero: r∆f pxq ´\\n\\nwf pxq∆ws “ 0.\\n\\n∇\\n\\nArmed with Proposition 1 and Assumption 1, we are ready to introduce functional expansion and majorisation: Theorem 1 (Functional expansion) Consider a convex diﬀerentiable loss ‘ and a diﬀerentiable machine learning model f . Under Assumption 1, the corresponding composite objective\\n\\nadmits the expansion:\\n\\npw ` ∆wq “\\n\\nL\\n\\npwq `\\n\\npwqJ∆w looooooooooooomooooooooooooon w L\\n\\n∇\\n\\nL\\n\\nﬁrst-order Taylor series\\n\\n`\\n\\n1 |S|\\n\\nÿ\\n\\npx,yqPS\\n\\nL\\n\\nbregman‘p¨,yqpf pxq, ∆f pxqq.\\n\\npw ` ∆wq may be written as the sum of its ﬁrst-order Taylor expansion with a So the perturbed objective Bregman divergence in the model outputs averaged over the training set. It is straightforward to specialise this result to diﬀerent losses by substituting in their Bregman divergence: Corollary 1 (Functional expansion of mean squared error) Under Assumption 1, for square loss:\\n\\nL\\n\\npw ` ∆wq “\\n\\nL\\n\\npwq `\\n\\nL\\n\\nw\\n\\n∇\\n\\npwqJ∆w `\\n\\nL\\n\\n1 |S|\\n\\nÿ\\n\\npx,yqPS\\n\\n1 2dL\\n\\n}∆f pxq}2 2.\\n\\nCorollary 2 (Functional majorisation for xent loss) Under Assumption 1, for cross-entropy loss, if yJ1 “ 1:\\n\\npw ` ∆wq ď\\n\\nL\\n\\npwq `\\n\\nL\\n\\nw\\n\\n∇\\n\\npwqJ∆w `\\n\\nL\\n\\n1 |S|\\n\\nÿ\\n\\npx,yqPS\\n\\n1\\n\\n2 }∆f pxq}2\\n\\n8 `\\n\\np∆f 3q.\\n\\nO\\n\\nWhen the functional perturbation is reasonably “spread out”, we would expect }∆f pxq}2 2{dL. In this setting, the functional majorisation of cross-entropy loss agrees with the functional expansion of mean squared error to second order. While the paper derives automatic gradient descent for the square loss, this observation justiﬁes its application to cross-entropy loss, as in the case of the ImageNet experiments.\\n\\n8 « }∆f pxq}2\\n\\n2.3 Recovering existing frameworks\\n\\nWe brieﬂy observe that three existing optimisation frameworks may be recovered eﬃciently from Theorem 1:\\n\\nMirror descent For linear models f px; W q :“ W x, the Bregman divergence bregman‘p¨,yqpf pxq, ∆f pxqq may be written bregman‘p¨,yqpW x, ∆W xq. This is a convex function of the weight perturbation ∆W . Substituting into Theorem 1 and minimising with respect to ∆W is the starting point for mirror descent.\\n\\nGauss-Newton method Substituting the linearised functional perturbation ∆f pxq « Corollary 1 and minimising with respect to ∆w is the starting point for the Gauss-Newton method.\\n\\n∇\\n\\nwf pxq∆w into\\n\\nNatural gradient descent Substituting the linearised functional perturbation ∆f pxq « Corollary 2 and minimising with respect to ∆w is the starting point for natural gradient descent.\\n\\n∇\\n\\nwf pxq∆w into\\n\\n7\\n\\n3 Majorise-Minimise for Deep Learning Problems\\n\\nIn this section, we will focus our eﬀorts on deriving an optimiser for deep fully-connected networks trained with square loss. The derivation for cross-entropy loss is analogous. Proofs are relegated to Appendix A. Deﬁnition 10 (Fully-connected network) A fully-connected network (FCN) f of depth L maps an input x P Rd0 to an output f px; wq P RdL via L matrix multiplications interspersed by non-linearity relupzq :“ maxp0, zq:\\n\\nf px; wq :“ WL ˝ prelu ˝ WL´1q ˝ prelu ˝ WL´2q ˝ ¨ ¨ ¨ ˝ prelu ˝ W1xq.\\n\\nIn this expression, w denotes the tuple of matrices w “ pW1, ..., WLq with kth matrix Wk in Rdkˆdk´1. In what follows, we will ﬁnd the following dimensional scaling to be particularly convenient: Prescription 1 (Dimensional scaling) For η ą 0, the data px, yq, weights Wk and updates ∆Wk should obey:\\n\\n}x}2 “ }Wk}˚ “\\n\\n}∆Wk}˚ “\\n\\n}y}2 “\\n\\na\\n\\nd0;\\n\\na\\n\\na\\n\\na\\n\\ndk{dk´1 dk{dk´1 ¨ η L dL.\\n\\nfor all k “ 1, ..., L;\\n\\nfor all k “ 1, ..., L;\\n\\n(input scaling)\\n\\n(weight scaling)\\n\\n(update scaling)\\n\\n(target scaling)\\n\\nWhile results can be derived without adopting Prescription 1, the scalings substantially simplify our formulae. dL{d0. One reason for this is that, under Prescription 1, we have the telescoping property that For a concrete example of how this helps, consider the following bound on the norm of the network outputs: Lemma 3 (Output bound) The output norm of a fully-connected network f obeys the following bound:\\n\\nL k“1 }Wk}˚ “\\n\\nś\\n\\na\\n\\n«\\n\\nLź\\n\\nﬀ\\n\\na\\n\\n}f px; wq}2 ď\\n\\n}Wk}˚\\n\\nˆ }x}2 “\\n\\ndL under Prescription 1.\\n\\nk“1\\n\\nSo, under Prescription 1, the bound is simple. Furthermore, the scaling of the update with a single parameter η reduces the problem of solving for an optimiser to a single parameter problem. To see how this might make life easier, consider the following lemma that relates weight perturbations to functional perturbations: Lemma 4 (Deep relative trust) When adjusting the weights w “ pW1, ..., WLq of a fully-connected network f by ∆w “ p∆W1, ..., ∆WLq, the induced functional perturbation ∆f pxq :“ f px; w ` ∆wq ´ f px; wq obeys:\\n\\n«\\n\\nLź\\n\\nﬀ\\n\\n}∆f pxq}2 ď\\n\\n}Wk}˚\\n\\nˆ }x}2 ˆ\\n\\nk“1\\n\\n«\\n\\nˆ\\n\\nLź\\n\\nk“1\\n\\n1 `\\n\\n}∆Wk}˚ }Wk}˚\\n\\n˙\\n\\nﬀ\\n\\na\\n\\n´ 1\\n\\nď\\n\\ndL ˆ pexp η ´ 1q under Prescription 1.\\n\\nSo, under Prescription 1, the single parameter η directly controls the size of functional perturbations.\\n\\nIn terms of enforcing Prescription 1 in practice, the norms of the data px, yq may be set via pre-processing, the norm of the update ∆Wk may be set via the optimisation algorithm and the norm of the weight matrix Wk may be set by the choice of initialisation. While, yes, }Wk}˚ may drift during training, the amount that this can happen is limited by Weyl (1912)’s inequality for singular values. In particular, after one step the perturbed operator norm }Wk ` ∆WK}˚ is sandwiched like p1 ´ η{Lq ¨ }Wk}˚ ď }Wk ` ∆WK}˚ ď p1 ` η{Lq ¨ }Wk}˚.\\n\\n3.1 Deriving automatic gradient descent\\n\\nWith both functional majorisation and deep relative trust in hand, we can majorise the deep network objective: Lemma 5 (Exponential majorisation) For an FCN with square loss, under Assumption 1 and Prescription 1:\\n\\npw ` ∆wq ď\\n\\nL\\n\\npwq `\\n\\nL\\n\\nη L\\n\\nLÿ\\n\\n„\\n\\na\\n\\ndk{dk´1 ˆ tr\\n\\nk“1\\n\\n8\\n\\n∆W J\\n\\nk ∇ }∆Wk}˚\\n\\nWk L\\n\\n\\uf6be\\n\\n` 1\\n\\n2 pexp η ´ 1q2.\\n\\ndef initialise_weights():\\n\\nfor layer k in t1, ..., Lu:\\n\\nWk „ uniformporthogonalpdk, dk´1qq\\n\\nb\\n\\nWk Ð Wk ¨\\n\\ndk dk´1\\n\\n# sample a semi-orthogonal matrix\\n\\n# rescale its singular values\\n\\ndef update_weights(): ř\\n\\nG Ð 1 L\\n\\nη Ð log 1`\\n\\nL l“1 } ?\\n\\n∇ 1`4G 2\\n\\n}F ¨\\n\\nWk L\\n\\nb\\n\\ndk dk´1\\n\\n# get gradient summary\\n\\n# set automatic learning rate\\n\\nfor layer k in t1, ..., Lu:\\n\\nWk Ð Wk ´ η\\n\\nL ¨ ∇ } ∇\\n\\nWk L }F Wk L\\n\\nb\\n\\n¨\\n\\ndk dk´1\\n\\n# update weights\\n\\nAlgorithm 1: Automatic gradient descent. The matrix Wk in Rdkˆdk´1 is the weight matrix at layer k. evaluated on a mini-batch B of training samples. The gradient\\n\\nis with respect to the objective\\n\\nWk L\\n\\n∇\\n\\nL\\n\\nk ∇\\n\\nWk L\\n\\n{}∆Wk}˚ between the perturbation matrix ∆Wk and the gradient matrix\\n\\nObserve that the majorisation only depends on the magnitude of the scalar η and on some notion of angle tr ∆W J . To derive an optimiser, we would now like to minimise this majorisation with respect to η and this angle. First, let us introduce one additional assumption and one additional deﬁnition: Assumption 2 (Gradient conditioning) The gradient satisﬁes rankstable ∇ }˚ of the gradient This assumption implies that the Frobenius norm } Wk L ∇ at layer k are equal. It is not immediately obvious why this should be a good assumption. After all, the “ 1 hk ‘pf pxq, yq b hk´1, where hk´1pxq and gradient is a sum of |S| rank-one matrices: |S| hkpxq denote the inputs and outputs of the weight matrix Wk at layer k, and b denotes the outer product. to have a stable rank of minpdk, dk´1, |S|q. But it turns out So, naïvely, one might expect the gradient to be a good assumption in practice (Yang & Hu, 2021; Yang et al., 2021). And for the deﬁnition: Deﬁnition 11 (Gradient summary) At a weight setting w, the gradient summary G is given by:\\n\\n“ 1 at all layers k “ 1, ..., L.\\n\\n}F and operator norm }\\n\\npx,yqPS ∇\\n\\nWk L ř\\n\\nWk L\\n\\nWk L\\n\\nWk L\\n\\nWk L\\n\\n∇\\n\\n∇\\n\\n∇\\n\\n∇\\n\\nG :“\\n\\n1 L\\n\\nLÿ\\n\\na\\n\\ndk{dk´1 ¨ }\\n\\npwq}F .\\n\\nWk L\\n\\n∇\\n\\nk“1\\n\\nThe gradient summary is a weighted average of gradient norms over layers. It can be thought of as a way to measure the size of the gradient while accounting for the fact that the weight matrices at diﬀerent layers may be on diﬀerent scales. This is related to the concept of the gradient scale coeﬃcient of Philipp et al. (2017).\\n\\nWe now have everything we need to derive automatic gradient descent via the majorise-minimise principle: Theorem 2 (Automatic gradient descent) For a deep fully-connected network, under Assumptions 1 and 2 and Prescription 1, the majorisation of square loss given in Lemma 5 is minimised by setting:\\n\\n1 `\\n\\nη “ log\\n\\n?\\n\\n1 ` 4G 2\\n\\n,\\n\\n∆Wk “ ´\\n\\na\\n\\nη L\\n\\n¨\\n\\ndk{dk´1 ¨ ∇ } ∇\\n\\nWk L }F Wk L\\n\\n,\\n\\nfor all layers k “ 1, ..., L.\\n\\nWe present pseudocode for this theorem in Algorithm 1, and a PyTorch implementation in Appendix B. Via a simple derivation based on clear algorithmic principles, automatic gradient descent uniﬁes various heuristic and theoretical ideas that have appeared in the literature:\\n\\n9\\n\\nRelative updates. The update is scaled relative to the norm of the weight matrix to which it is applied— assuming the weight matrices are scaled according to Prescription 1. Such a scaling was proposed by You et al. (2017) and further explored by Carbonnelle & Vleeschouwer (2019) and Bernstein et al. (2020). There is evidence that such relative synaptic updates may occur in neuroscience (Loewenstein et al., 2011).\\n\\nDepth scaling. Scaling the perturbation strength like 1{L for networks of depth L was proposed on\\n\\ntheoretical grounds by Bernstein et al. (2020) based on analysis via deep relative trust.\\n\\nWidth scaling. The dimensional factors of dk and dk´1 that appear closely relate to the maximal update parameterisation of Yang & Hu (2021) designed to ensure hyperparameter transfer across network width.\\n\\nGradient clipping. The logarithmic dependence of the update on the gradient summary may be seen as an automatic form of adaptive gradient clipping (Brock et al., 2021)—a technique which clips the gradient once its magnitude surpasses a certain threshold set by a hyperparameter.\\n\\n3.2 Convergence analysis\\n\\nThis section presents theoretical convergence rates for automatic gradient descent. While the spirit of the analysis is standard in optimisation theory, the details may still prove interesting for their detailed characterisation of the optimisation properties of deep networks. For instance, we propose a novel Polyak- Łojasiewicz inequality tailored to the operator structure of deep networks. We begin with two observations: Lemma 6 (Bounded objective) For square loss, the objective is bounded as follows:\\n\\npwq ď\\n\\nL\\n\\n1 |S|\\n\\nÿ\\n\\npx,yqPS\\n\\n}f px; wq}2 2dL\\n\\n2 ` }y}2 2\\n\\nď 1 under Prescription 1.\\n\\nLemma 7 (Bounded gradient) For square loss, the norm of the gradient at layer k is bounded as follows:\\n\\n} ∇\\n\\nWk L\\n\\n}F ď\\n\\nś\\n\\nL l“1 }Wl}˚ }Wk}˚\\n\\nd\\n\\n¨\\n\\npwq 2 L dL\\n\\n¨\\n\\nd\\n\\n1 |S|\\n\\nc\\n\\n}x}2\\n\\n2 ď\\n\\n2 ¨\\n\\ndk´1 dk\\n\\nÿ\\n\\npx,yqPS\\n\\nunder Prescription 1.\\n\\nThese results help us prove that automatic gradient descent converges to a point where the gradient vanishes: Lemma 8 (Convergence rate to critical point) Consider a fully-connected network trained by automatic gradi- ent descent (Theorem 2) and square loss for T iterations. Let Gt denote the gradient summary (Deﬁnition 11) at step t ď T . Under Assumptions 1 and 2 and Prescription 1, AGD converges at the following rate:\\n\\nmin tPt1,...,T u\\n\\nG2\\n\\nt ď\\n\\n11 T\\n\\n.\\n\\nThis lemma can be converted into a convergence rate to a global minimum with one additional assumption: Assumption 3 (Deep Polyak-Łojasiewicz inequality) For some α ą 0, the gradient norm is lower bounded by:\\n\\n} ∇\\n\\nWk L\\n\\n}F ě α ˆ\\n\\nś\\n\\nL l“1 }Wl}˚ }Wk}˚\\n\\nd\\n\\n¨\\n\\npwq 2 L dL\\n\\n¨\\n\\nd\\n\\n1 |S|\\n\\nÿ\\n\\npx,yqPS\\n\\nc\\n\\n}x}2\\n\\n2 “ α ˆ\\n\\n2 ¨\\n\\npwq ¨\\n\\nL\\n\\ndk´1 dk\\n\\nunder Prescription 1.\\n\\nThis lower bound mirrors the structure of the upper bound in Lemma 7. The parameter α captures how much of the gradient is attenuated by small singular values in the weights and by deactivated relu units. While Polyak-Łojasiewicz inequalities are common in the literature (Liu et al., 2022), our assumption is novel in that it pays attention to the operator structure of the network. Assumption 3 leads to the following theorem: Theorem 3 (Convergence rate to global minima) For automatic gradient descent (Theorem 2) in the same setting as Lemma 8 but with the addition of Assumption 3, the mean squared error objective at step T obeys:\\n\\npwT q ď\\n\\nL\\n\\n1 α2\\n\\nˆ\\n\\n6 T\\n\\n.\\n\\n10\\n\\nFigure 4: Benchmarking automatic gradient descent on a range of architectures and datasets. Solid lines are AGD and faint dashed lines are tuned Adam except for ImageNet where the dashed line is SGD with a ﬁxed learning rate of 0.1. ImageNet used cross-entropy loss with a mini-batch size of 1024. The other experiments used square loss with a mini-batch size of 128. The top row plots the automatic learning rate (η in the main text) and objective value. The maximum and minimum learning rate for each epoch is included in addition to the mean for the ﬁrst three plots. The bottom row shows the train and test accuracy.\\n\\n3.3 Experiments\\n\\nThe goal of our experiments was twofold. First, we wanted to test automatic gradient descent (AGD, Algorithm 1) on a broad variety of networks architectures and datasets to check that it actually works. In particular, we tested AGD on fully-connected networks (FCNs, Deﬁnition 10), and both VGG-style (Simonyan & Zisserman, 2015) and ResNet-style (He et al., 2015) convolutional neural networks on the CIFAR-10, CIFAR-100 (Krizhevsky, 2009) and ImageNet (Deng et al., 2009, ILSVRC2012) datasets with standard data augmentation. And second, to see what AGD may have to oﬀer beyond the status quo, we wanted to compare AGD to tuned Adam and SGD baselines, as well as Adam and SGD run with their default hyperparameters.\\n\\nTo get AGD working with convolutional layers, we adopted a per-submatrix normalisation scheme. Speciﬁcally, for a convolutional tensor with ﬁlters of size kx ˆ ky, we implemented the normalisation separately for each of the kx ˆ ky submatrices of dimension channelsin ˆ channelsout. Since AGD does not yet support biases or aﬃne parameters in batchnorm, we disabled these parameters in all architectures. To at least adhere to Prescription 1 at initialisation, AGD draws initial weight matrices uniform semi-orthogonal and re-scaled fan_in{fan_out. Adam and SGD baselines used the PyTorch default initialisation. A by a factor of PyTorch implementation of AGD reﬂecting these details is given in Appendix B. All experiments use square loss except ImageNet which used cross-entropy loss. Cross-entropy loss has been found to be superior to square loss for datasets with a large number of classes (Demirkaya et al., 2020; Hui & Belkin, 2021).\\n\\na\\n\\nOur experimental results are spread across ﬁve ﬁgures:\\n\\nFigure 1 presents some highlights of our results: First, AGD can train networks that Adam and SGD with default hyperparameters cannot. Second, for ResNet-18 on CIFAR-10, AGD attained performance comparable to the best-tuned performance of Adam and SGD. And third, AGD scales up to ImageNet.\\n\\n11\\n\\nFigure 5: Comparing automatic gradient descent to tuned Adam and SGD. An eight-layer fully-connected network was trained on CIFAR-10 with square loss. Dotted lines show test and solid lines show train performance. The left panel shows the objective value: AGD and Adam attained a smaller training objective than SGD. The middle panel shows train and test accuracies. The right panel shows the relative update size L averaged over layers: 1 k“1 }∆Wk}F {}Wk}F . We plot the maximum, minimum and mean over an epoch. L\\n\\nř\\n\\nFigure 4 displays the breadth of our experiments: from training a 16-layer fully-connected network on CIFAR-10 to training ResNet-50 on ImageNet. Adam’s learning rate was tuned over the logarithmic grid t10´5, 10´4, ..., 10´1u while for ImageNet we used a default learning rate of 0.1 for SGD without any manual decay. AGD and Adam performed almost equally well on the depth-16 width-512 fully-connected network: 52.7% test accuracy for AGD compared to 53.5% for Adam. For ResNet-18 on CIFAR-10, Adam attained 92.9% test accuracy compared to AGD’s 91.2%. On this benchmark, a fully-tuned SGD with learning rate schedule, weight decay, cross-entropy loss and bias and aﬃne parameters can attain 93.0% test accuracy (Liu, 2017). For VGG-16 on CIFAR-100, AGD achieved 67.4% test accuracy compared to Adam’s 69.7%. Finally, on ImageNet AGD achieved a top-1 test accuracy of 65.5% after 350 epochs.\\n\\nFigure 5 compares AGD to Adam and SGD for training an eight-layer fully-connected network of width 256. Adam and SGD’s learning rates were tuned over the logarithmic grid t10´5, 10´4, ..., 10´1u. Adam’s optimal learning rate of 10´4 was three orders of magnitude smaller than SGD’s optimal learning rate of 10´1. SGD did not attain as low of an objective value as Adam or AGD.\\n\\nFigure 6 shows that AGD can train FCNs with width ranging from 64 to 2048 and depth from 2 to 32 and Figure 7 shows that AGD successfully trains a four-layer FCN at varying mini-batch size: from 32 to 4096.\\n\\n4 Discussion\\n\\nThis paper has proposed a new framework for deriving optimisation algorithms for non-convex composite objective functions, which are particularly prevalent in the ﬁeld of machine learning and the subﬁeld of deep learning. What we have proposed is truly a framework: it can be applied to a new loss function by writing down its Bregman divergence, or a new machine learning model by writing down its architectural perturbation bound. The framework is properly placed in the context of existing frameworks such as the majorise-minimise meta-algorithm, mirror descent and natural gradient descent.\\n\\nRecent papers have proposed a paradigm of hyperparameter transfer where a small network is tuned and the resulting hyperparameters are transferred to a larger network (Yang et al., 2021; Bernstein, 2022). The methods and results in this paper suggest a stronger paradigm of hyperparameter elimination: by detailed analysis of the structure and interactions between diﬀerent components of a machine learning system, we may hope—if not to outright outlaw hyperparameters—at least to reduce their abundance and opacity.\\n\\n12\\n\\nFigure 6: Benchmarking automatic gradient descent on networks of varying width and depth. We trained fully-connected networks on CIFAR-10 with square loss and a mini-batch size of 128. The depth ranged from 2 to 32, and the width from 64 to 2048, in powers of two. In terms of training performance, wider was always better, while depth 8 and depth 16 were superior to depth 32. In terms of test accuracy, the best performance was achieved at depth 4 and width 2048: 63.7%. The worst test performance was achieved by the smallest network of depth 2 and width 64: 42.55%. Larger networks display two broadly distinct phases of training: the automatic learning rate increases slowly while the objective decreases slowly, followed by a rapid decrease in the automatic learning rate and objective. This second phase typically coincides with reaching 100% train accuracy. See Figure 5 for a comparison between Adam, SGD and AGD for the 256-width 8-layer FCN.\\n\\n13\\n\\nFigure 7: Benchmarking automatic gradient descent at varying mini-batch size. We trained four-layer fully-connected networks on CIFAR-10. The mini-batch size ranged from 32 to 4096. Test accuracy generally improved with increasing mini-batch size: the ﬁnal test accuracies, in order of increasing mini-batch size, were 55.0%, 58.0%, 60.0% and 59.8%. The automatic learning rate seemed to initially dip, and this eﬀect was more pronounced for larger mini-batch sizes. Metrics were computed every iteration during the ﬁrst epoch and once per epoch from thereon—this explains the kinks visible in the plots.\\n\\nThe main product of this research is automatic gradient descent (AGD), with pseudocode given in Algorithm 1 and PyTorch code given in Appendix B. We have found AGD to be genuinely useful, and believe that it may complement automatic diﬀerentiation in helping to automate general machine learning workﬂows.\\n\\nThe analysis leading to automatic gradient descent is elementary: we leverage basic concepts in linear algebra such as matrix and vector norms, and use simple bounds such as the triangle inequality for vector–vector sums, and the operator norm bound for matrix–vector products. The analysis is non-asymptotic: it does not rely on taking dimensions to inﬁnity, and deterministic: it does not involve random matrix theory. We believe that the accessibility of the analysis could make this paper a good starting point for future developments.\\n\\nDirections for future work Here we list some promising avenues for theoretical and practical research. We are exploring some of these ideas in our development codebase: https://github.com/C1510/agd_exp.\\n\\nStochastic optimisation. Automatic gradient descent is derived in the full-batch optimisation setting, but the algorithm is evaluated experimentally in the mini-batch setting. It would be interesting to try to extend our theoretical and practical methods to more faithfully address stochastic optimisation.\\n\\nMore architectures. Automatic gradient descent is derived for fully-connected networks and extended heuristically to convolutional networks. We are curious to extend the methods to more varied architectures such as transformers (Vaswani et al., 2017) and architectural components such as biases. Since most neural networks resemble fully-connected networks in the sense that they are all just deep compound operators, we expect much of the structure of automatic gradient descent as presented to carry through.\\n\\nRegularisation. The present paper deals purely with the optimisation structure of deep neural networks, and little thought is given to either generalisation or regularisation. Future work could look at both theoretical and practical regularisation schemes for automatic gradient descent. It would be interesting to try to do this without introducing hyperparameters, although we suspect that when it comes to regularisation at least one hyperparameter may become necessary.\\n\\nAcceleration. We have found in some preliminary experiments that slightly increasing the update size of automatic gradient descent with a gain hyperparameter, or introducing a momentum hyperparameter, can lead to faster convergence. We emphasise that no experiment in this paper used such hyperparameters. Still, these observations may provide a valuable starting point for improving AGD in future work.\\n\\n14\\n\\nOperator perturbation theory. Part of the inspiration for this paper was the idea of applying operator perturbation theory to deep learning. While perturbation theory is well-studied in the context of linear operators (Weyl, 1912; Kato, 1966; Stewart, 2006), in deep learning we are concerned with non-linear compound operators. It may be interesting to try to further extend results in perturbation theory to deep neural networks. One could imagine cataloging the perturbation structure of diﬀerent neural network building blocks, and using a result similar to deep relative trust (Lemma 4) to describe how they compound.\\n\\nAcknowledgments\\n\\nThe authors are grateful to MIT SuperCloud, Oxford Hydra, NVIDIA and Virgile Richard for providing GPUs. Thanks are due to Greg Yang and Jamie Simon for helpful discussions. A paper with Greg and Jamie is in preparation to explain the relationship between muP (Yang & Hu, 2021) and the operator norm.\\n\\nReferences\\n\\nNaman Agarwal, Zeyuan Allen Zhu, Brian Bullins, Elad Hazan and Tengyu Ma. Finding approximate local\\n\\nminima faster than gradient descent. Symposium on Theory of Computing, 2016. Cited on page 3.\\n\\nNaman Agarwal, Brian Bullins and Elad Hazan. Second-order stochastic optimization for machine learning\\n\\nin linear time. Journal of Machine Learning Research, 2017. Cited on page 3.\\n\\nRami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry Bahdanau, Nicolas Ballas, Frédéric Bastien, Justin Bayer, Anatoly Belikov, Alexander Belopolsky et al. Theano: A Python framework for fast computation of mathematical expressions. arXiv:1605.02688, 2016. Cited on page 2.\\n\\nShun-ichi Amari. Natural gradient works eﬃciently in learning. Neural Computation, 1998. Cited on pages 3\\n\\nand 5.\\n\\nNavid Azizan and Babak Hassibi. Stochastic gradient/mirror descent: Minimax optimality and implicit\\n\\nregularization. In International Conference on Learning Representations, 2019. Cited on page 3.\\n\\nJeremy Bernstein. Optimisation & Generalisation in Networks of Neurons. Ph.D. thesis, California Institute\\n\\nof Technology, 2022. Cited on pages 7 and 12.\\n\\nJeremy Bernstein, Arash Vahdat, Yisong Yue and Ming-Yu Liu. On the distance between two neural networks and the stability of learning. In Neural Information Processing Systems, 2020. Cited on pages 2, 3, 5, 6, and 10.\\n\\nÅke Björck. Numerical Methods for Least Squares Problems. Society for Industrial and Applied Mathematics,\\n\\n1996. Cited on pages 3 and 5.\\n\\nLéon Bottou, Frank E. Curtis and Jorge Nocedal. Optimization methods for large-scale machine learning.\\n\\nSIAM Review, 2018. Cited on page 3.\\n\\nLev M. Bregman. The relaxation method of ﬁnding the common point of convex sets and its application to the solution of problems in convex programming. USSR Computational Mathematics and Mathematical Physics, 1967. Cited on pages 2 and 6.\\n\\nAndy Brock, Soham De, Samuel L. Smith and Karen Simonyan. High-performance large-scale image recognition without normalization. In International Conference on Machine Learning, 2021. Cited on page 10.\\n\\nSimon Carbonnelle and Christophe De Vleeschouwer. Layer rotation: A surprisingly simple indicator of generalization in deep networks? In ICML Workshop on Identifying and Understanding Deep Learning Phenomena, 2019. Cited on page 10.\\n\\nJeremy Cohen, Simran Kaur, Yuanzhi Li, J. Zico Kolter and Ameet Talwalkar. Gradient descent on neural networks typically occurs at the edge of stability. In International Conference on Learning Representations, 2021. Cited on page 3.\\n\\n15\\n\\nAhmet Demirkaya, Jiasi Chen and Samet Oymak. Exploring the role of loss functions in multiclass classiﬁcation.\\n\\nConference on Information Sciences and Systems, 2020. Cited on page 11.\\n\\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li and Li Fei-Fei. ImageNet: A large-scale hierarchical\\n\\nimage database. In Computer Vision and Pattern Recognition, 2009. Cited on page 11.\\n\\nInderjit S. Dhillon and Joel A. Tropp. Matrix nearness problems with Bregman divergences. SIAM Journal\\n\\non Matrix Analysis and Applications, 2008. Cited on page 2.\\n\\nAlexander R. Farhang, Jeremy Bernstein, Kushal Tirumala, Yang Liu and Yisong Yue.\\n\\nInvestigating generalization by controlling normalized margin. In International Conference on Machine Learning, 2022. Cited on page 2.\\n\\nIan Goodfellow, Yoshua Bengio and Aaron Courville. Deep Learning. MIT Press, 2016. Cited on page 2.\\n\\nKaiming He, X. Zhang, Shaoqing Ren and Jian Sun. Deep residual learning for image recognition. Computer\\n\\nVision and Pattern Recognition, 2015. Cited on page 11.\\n\\nPeter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup and David Meger. Deep reinforcement learning that matters. In AAAI Conference on Artiﬁcial Intelligence, 2018. Cited on page 2.\\n\\nLike Hui and Mikhail Belkin. Evaluation of neural architectures trained with square loss vs. cross-entropy in classiﬁcation tasks. In International Conference on Learning Representations, 2021. Cited on page 11.\\n\\nArthur Jacot, Franck Gabriel and Clement Hongler. Neural tangent kernel: Convergence and generalization\\n\\nin neural networks. In Neural Information Processing Systems, 2018. Cited on page 5.\\n\\nYiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan and Samy Bengio. Fantastic generalization measures and where to ﬁnd them. In International Conference on Learning Representations, 2020. Cited on page 2.\\n\\nTosio Kato. Perturbation Theory for Linear Operators. Springer, 1966. Cited on page 15.\\n\\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference\\n\\non Learning Representations, 2015. Cited on page 3.\\n\\nAlex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University of\\n\\nToronto, 2009. Cited on page 11.\\n\\nKenneth Lange. MM Optimization Algorithms. Society for Industrial and Applied Mathematics, 2016. Cited\\n\\non pages 3 and 4.\\n\\nJaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-Dickstein and Jeﬀrey Pennington. Wide neural networks of any depth evolve as linear models under gradient descent. In Neural Information Processing Systems, 2019. Cited on page 7.\\n\\nChaoyue Liu, Libin Zhu and Mikhail Belkin. Loss landscapes and optimization in over-parameterized non-linear systems and neural networks. Applied and Computational Harmonic Analysis, 2022. Cited on page 10.\\n\\nKuang Liu. Train CIFAR-10 with PyTorch. https://github.com/kuangliu/pytorch-cifar, 2017. Cited\\n\\non page 12.\\n\\nYonatan Loewenstein, Annerose Kuras and Simon Rumpel. Multiplicative dynamics underlie the emergence of the log-normal distribution of spine sizes in the neocortex in vivo. Journal of Neuroscience, 2011. Cited on page 10.\\n\\nMario Lucic, Karol Kurach, Marcin Michalski, Sylvain Gelly and Olivier Bousquet. Are GANs created equal?\\n\\nA large-scale study. In Neural Information Processing Systems, 2017. Cited on page 2.\\n\\n16\\n\\nArkady S. Nemirovsky and David B. Yudin. Problem complexity and method eﬃciency in optimization. Wiley,\\n\\n1983. Cited on pages 3 and 5.\\n\\nYurii Nesterov and Boris Polyak. Cubic regularization of Newton method and its global performance.\\n\\nMathematical Programming, 2006. Cited on page 3.\\n\\nJorge Nocedal and Stephen J. Wright. Numerical Optimization. Springer, 1999. Cited on page 3.\\n\\nFrancesco Orabona and Ashok Cutkosky. ICML 2020 tutorial on parameter-free online optimization, 2020.\\n\\nCited on page 2.\\n\\nRazvan Pascanu and Yoshua Bengio. Revisiting natural gradient for deep networks.\\n\\nIn International\\n\\nConference on Learning Representations, 2014. Cited on pages 3 and 7.\\n\\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga et al. PyTorch: An imperative style, high-performance deep learning library. In Neural Information Processing Systems, 2019. Cited on pages 2 and 23.\\n\\nGeorge Philipp, Dawn Xiaodong Song and Jaime G. Carbonell. The exploding gradient problem demystiﬁed.\\n\\narXiv:1712.05577, 2017. Cited on page 9.\\n\\nDavid E. Rumelhart, Geoﬀrey E. Hinton and Ronald J. Williams. Learning representations by back-propagating\\n\\nerrors. Nature, 1986. Cited on page 3.\\n\\nRobin M. Schmidt, Frank Schneider and Philipp Hennig. Descending through a crowded valley—benchmarking\\n\\ndeep learning optimizers. In International Conference on Machine Learning, 2021. Cited on page 2.\\n\\nOr Sharir, Barak Peleg and Yoav Shoham. The cost of training NLP models: A concise overview.\\n\\narXiv:2004.08900, 2020. Cited on page 2.\\n\\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition.\\n\\nIn International Conference on Learning Representations, 2015. Cited on page 11.\\n\\nMichael Stewart. Perturbation of the SVD in the presence of small singular values. Linear Algebra and its\\n\\nApplications, 2006. Cited on page 15.\\n\\nHaoyuan Sun, Kwangjun Ahn, Christos Thrampoulidis and Navid Azizan. Mirror descent maximizes generalized margin and can be implemented eﬃciently. In Neural Information Processing Systems, 2022. Cited on page 3.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser and Illia Polosukhin. Attention is all you need. In Neural Information Processing Systems, 2017. Cited on page 14.\\n\\nHermann Weyl. Das asymptotische Verteilungsgesetz der Eigenwerte linearer partieller Diﬀerentialgleichungen (mit einer Anwendung auf die Theorie der Hohlraumstrahlung). Mathematische Annalen, 1912. Cited on pages 8 and 15.\\n\\nGreg Yang and Edward J. Hu. Tensor programs IV: Feature learning in inﬁnite-width neural networks. In\\n\\nInternational Conference on Machine Learning, 2021. Cited on pages 3, 4, 5, 9, 10, and 15.\\n\\nGreg Yang, Edward J. Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen and Jianfeng Gao. Tuning large neural networks via zero-shot hyperparameter transfer. In Neural Information Processing Systems, 2021. Cited on pages 9 and 12.\\n\\nYang You, Igor Gitman and Boris Ginsburg. Scaling SGD batch size to 32K for ImageNet training. Technical\\n\\nreport, University of California, Berkeley, 2017. Cited on page 10.\\n\\nJingzhao Zhang, Tianxing He, Suvrit Sra and Ali Jadbabaie. Why gradient clipping accelerates training: A theoretical justiﬁcation for adaptivity. In International Conference on Learning Representations, 2020. Cited on page 3.\\n\\n17\\n\\nA Proofs\\n\\nHere are the proofs for the theoretical results in the main text.\\n\\nProposition 1 (Decomposition of linearisation error) For any diﬀerentiable loss ‘ and any diﬀerentiable ma- chine learning model f the linearisation error of the objective function admits the following decomposition:\\n\\npwq ´\\n\\npwqJ∆w ∆ looooooooooooooomooooooooooooooon w ∇ linearisation error of objective\\n\\nL\\n\\nL\\n\\n“\\n\\n1 |S|\\n\\n1 |S|\\n\\n`\\n\\nÿ\\n\\npx,yqPS ÿ\\n\\npx,yqPS\\n\\nL ∇f pxq‘pf pxq, yqJ r∆f pxq ´\\n\\nwf pxq∆ws loooooooooooooomoooooooooooooon ∇ linearisation error of model\\n\\n∇f pxq‘pf pxq, yqJ∆f pxq ∆‘pf pxq, yq ´ looooooooooooooooooooooooomooooooooooooooooooooooooon\\n\\n.\\n\\nlinearisation error of loss\\n\\nProof. By the chain rule,\\n\\npwqJ∆w “ 1 |S|\\n\\nw\\n\\n∇\\n\\nL\\n\\nř\\n\\npx,yqPS ∇f pxq‘pf pxq, yqJ\\n\\n∇\\n\\nwf pxq∆w. Therefore:\\n\\npwq ´\\n\\n∆\\n\\nL\\n\\nw\\n\\n∇\\n\\nL\\n\\npwqJ∆w “\\n\\n1 |S|\\n\\nÿ\\n\\npx,yqPS\\n\\n∆‘pf pxq, yq ´\\n\\n∇f pxq‘pf pxq, yqJ\\n\\n∇\\n\\nwf pxq∆w.\\n\\nAdding and subtracting 1 |S|\\n\\nř px,yqPS ∇f pxq‘pf pxq, yqJ∆f pxq on the right-hand side yields the result.\\n\\nLemma 1 (Bregman divergence of square loss) When ‘ is set to square loss, then:\\n\\nbregman‘p¨,yqpf pxq, ∆f pxqq “ 1 2dL\\n\\n}∆f pxq}2 2.\\n\\nProof. Expanding the Euclidean norms in the loss perturbation ∆‘ yields:\\n\\n∆‘pf pxq, yq “ 1 2dL “ 1 2dL\\n\\n}f pxq ` ∆f pxq ´ y}2 }∆f pxq}2\\n\\n2 ` pf pxq ´ yqJ∆f pxq.\\n\\n2 ´ 1 2dL\\n\\n}f pxq ´ y}2 2\\n\\nThe result follows by identifying that\\n\\n∇f pxq‘pf pxq, yqJ∆f pxq “ pf pxq ´ yqJ∆f pxq.\\n\\nLemma 2 (Bregman divergence of xent loss) When ‘ is set to cross-entropy loss, and if yJ1 “ 1, then:\\n\\nbregman‘p¨,yqpf pxq, ∆f pxqq “ DKL\\n\\nsoftmaxpf pxqq\\n\\n´\\n\\nˇ ˇ ˇ\\n\\nˇ ˇ ˇ softmaxpf pxq ` ∆f pxqq\\n\\n¯\\n\\nProof. First, since\\n\\nř\\n\\nď 1\\n\\n2 }∆f pxq}2\\n\\n8 `\\n\\np∆f 3q.\\n\\nO\\n\\ni yi “ 1, cross-entropy loss may be re-written:\\n\\n‘pf pxq, yq :“ ´ logrsoftmaxpf pxqqsJy “ ´f pxqJy ` log }exp f pxq}1.\\n\\nThe linear term ´f pxqJy does not contribute to the linearisation error and may be neglected. Therefore:\\n\\n∆‘pf pxq, yq ´\\n\\n∇f pxq‘pf pxq, yqJ∆f pxq\\n\\n“ log }exppf pxq ` ∆f pxqq}1 ´ log }exp f pxq}1 ´\\n\\n∇f pxq log }exp f pxq}J\\n\\n1 ∆f pxq\\n\\n“ log\\n\\n1{}exp f pxq}1 1{}exppf pxq ` ∆f pxqq}1\\n\\n“\\n\\nexp f pxqJ }exp f pxq}1\\n\\nlog\\n\\n´\\n\\n´\\n\\nexp f pxqJ }exp f pxq}1 exp f pxq{}exp f pxq}1 exppf pxq ` ∆f pxqq{}exppf pxq ` ∆f pxqq}1 ¯\\n\\n∆f pxq\\n\\nˇ ˇ ˇ ˇ ˇ softmaxpf pxq ` ∆f pxqq ˇ\\n\\n.\\n\\nThe ﬁnal line is equivalent to DKL\\n\\nsoftmaxpf pxqq\\n\\nestablishing the ﬁrst equality.\\n\\n18\\n\\nTo establish the inequality, let b denote the outer product and deﬁne p :“ softmaxpf pxqq. Then we have:\\n\\n∆‘pf pxq, yq ´\\n\\n∇f pxq‘pf pxq, yqJ∆f pxq “ “\\n\\n“\\n\\nď\\n\\nď\\n\\n1 2 1 2 1 2 1 2 1 2\\n\\n∆f pxqJ\\n\\n∆f pxqJ\\n\\n∇\\n\\n∇\\n\\n2 f pxq‘pf pxq, yq∆f pxq `\\n\\np∆f 3q\\n\\nO\\n\\n2 f pxq log }exp f pxq}1∆f pxq `\\n\\np∆f 3q\\n\\nO p∆f 3q\\n\\n∆f pxqJrdiagppq ´ p b ps∆f pxq `\\n\\nO\\n\\n∆f pxqJ diagppq∆f pxq `\\n\\np∆f 3q\\n\\nO\\n\\n}∆f pxq}2\\n\\n8 `\\n\\np∆f 3q,\\n\\nO\\n\\nwhere we have used that p b p is positive deﬁnite and then applied Hölder’s inequality with }p}1 “ 1.\\n\\nTheorem 1 (Functional expansion) Consider a convex diﬀerentiable loss ‘ and a diﬀerentiable machine learning model f . Under Assumption 1, the corresponding composite objective\\n\\nadmits the expansion:\\n\\npw ` ∆wq “\\n\\nL\\n\\npwq `\\n\\npwqJ∆w looooooooooooomooooooooooooon w L\\n\\n∇\\n\\nL\\n\\nﬁrst-order Taylor series\\n\\n`\\n\\n1 |S|\\n\\nÿ\\n\\npx,yqPS\\n\\nL\\n\\nbregman‘p¨,yqpf pxq, ∆f pxqq.\\n\\nProof. The result follows by substituting Assumption 1 into Proposition 1 and applying Deﬁnition 9.\\n\\nCorollary 1 (Functional expansion of mean squared error) Under Assumption 1, for square loss:\\n\\npw ` ∆wq “\\n\\nL\\n\\npwq `\\n\\nL\\n\\nw\\n\\n∇\\n\\npwqJ∆w `\\n\\nL\\n\\n1 |S|\\n\\nÿ\\n\\npx,yqPS\\n\\n1 2dL\\n\\n}∆f pxq}2 2.\\n\\nProof. Combine Lemma 1 with Theorem 1 to obtain the result.\\n\\nCorollary 2 (Functional majorisation for xent loss) Under Assumption 1, for cross-entropy loss, if yJ1 “ 1:\\n\\npw ` ∆wq ď\\n\\nL\\n\\npwq `\\n\\nL\\n\\nw\\n\\n∇\\n\\npwqJ∆w `\\n\\nL\\n\\n1 |S|\\n\\nÿ\\n\\npx,yqPS\\n\\nProof. Combine Lemma 2 with Theorem 1 to obtain the result.\\n\\n1\\n\\n2 }∆f pxq}2\\n\\n8 `\\n\\np∆f 3q.\\n\\nO\\n\\nLemma 3 (Output bound) The output norm of a fully-connected network f obeys the following bound:\\n\\n«\\n\\nLź\\n\\nﬀ\\n\\na\\n\\n}f px; wq}2 ď\\n\\n}Wk}˚\\n\\nˆ }x}2 “\\n\\ndL under Prescription 1.\\n\\nk“1\\n\\nProof. For any vector v and matrix M with compatible dimensions, we have that }M v}2 ď }M }˚ ¨ }v}2 and }relu v}2 ď }v}2. The lemma follows by applying these results recursively over the depth of the network.\\n\\nLemma 4 (Deep relative trust) When adjusting the weights w “ pW1, ..., WLq of a fully-connected network f by ∆w “ p∆W1, ..., ∆WLq, the induced functional perturbation ∆f pxq :“ f px; w ` ∆wq ´ f px; wq obeys:\\n\\n«\\n\\nLź\\n\\nﬀ\\n\\n}∆f pxq}2 ď\\n\\n}Wk}˚\\n\\nˆ }x}2 ˆ\\n\\nk“1\\n\\n«\\n\\nˆ\\n\\nLź\\n\\nk“1\\n\\n1 `\\n\\n}∆Wk}˚ }Wk}˚\\n\\n˙\\n\\nﬀ\\n\\na\\n\\n´ 1\\n\\nď\\n\\ndL ˆ pexp η ´ 1q under Prescription 1.\\n\\nProof. We proceed by induction. First, consider a network with L “ 1 layers: f pxq “ W1x. Observe that }∆f pxq}2 “ }∆W1x}2 ď }∆W1}˚ ¨ }x}2 as required. Next, assume that the result holds for a network gpxq\\n\\n19\\n\\nwith L ´ 1 layers and consider adding a layer to obtain f pxq “ WL ˝ relu ˝ gpxq. Then:\\n\\n}∆f pxq}2 “ }pWL ` ∆WLq ˝ relu ˝ pgpxq ` ∆gpxqq ´ WL ˝ relu ˝ gpxq}2\\n\\n“ }WL prelu ˝ pgpxq ` ∆gpxqq ´ relu ˝ gpxqq ` ∆WL prelu ˝ pgpxq ` ∆gpxqq ´ relup0qq}2 ď }WL}˚ ¨ }∆gpxq}2 ` }∆WL}˚ ¨ p}gpxq}2 ` }∆gpxq}2q “ p}WL}˚ ` }∆WL}˚q ¨ }∆gpxq}2 ` }∆WL}˚ ¨ }gpxq}2,\\n\\nwhere the inequality follows by applying the triangle inequality, the operator norm bound, the fact that relu is one-Lipschitz, and a further application of the triangle inequality. But by the inductive hypothesis and Lemma 3, the right-hand side is bounded by: «\\n\\nﬀ\\n\\n«\\n\\n«\\n\\nﬀ\\n\\nﬀ\\n\\nˆ\\n\\nL´1ź\\n\\n˙\\n\\nL´1ź\\n\\nL´1ź\\n\\np}WL}˚ ` }∆WL}˚q\\n\\n1 `\\n\\n«\\n\\nˆ\\n\\nLź\\n\\n“\\n\\nk“1\\n\\nk“1\\n\\n1 `\\n\\n}∆Wk}˚ }Wk}˚\\n\\n}∆Wk}˚ }Wk}˚ ﬀ\\n\\n˙\\n\\n´ 1\\n\\nˆ\\n\\n}Wk}˚\\n\\nˆ }x}2.\\n\\nk“1\\n\\n´ 1\\n\\nˆ\\n\\n}Wk}˚\\n\\nˆ }x}2 ` }∆WL}˚ ˆ\\n\\n}Wk}˚\\n\\nˆ }x}2\\n\\n«\\n\\nLź\\n\\nk“1 ﬀ\\n\\nk“1\\n\\nThe induction is complete. To further bound this result under Prescription 1, observe that the product ”ś L k“1 }Wk}˚\\n\\ndL, while the other product satisﬁes:\\n\\n?\\n\\nı\\n\\nˆ }x}2 telescopes to just ﬀ «\\n\\n˙\\n\\nˆ\\n\\nLź\\n\\n1 `\\n\\nk“1\\n\\n}∆Wk}˚ }Wk}˚\\n\\n´ 1\\n\\n“\\n\\n1 `\\n\\n´\\n\\n¯\\n\\nL\\n\\nη L\\n\\n´\\n\\n´ 1 ď lim LÑ8\\n\\n1 `\\n\\n¯\\n\\nL\\n\\nη L\\n\\n´ 1 “ exp η ´ 1.\\n\\nCombining these observations yields the result.\\n\\nLemma 5 (Exponential majorisation) For an FCN with square loss, under Assumption 1 and Prescription 1:\\n\\npw ` ∆wq ď\\n\\nL\\n\\npwq `\\n\\nL\\n\\nη L\\n\\nLÿ\\n\\n„\\n\\na\\n\\ndk{dk´1 ˆ tr\\n\\nk“1\\n\\n∆W J\\n\\nk ∇ }∆Wk}˚\\n\\nWk L\\n\\n\\uf6be\\n\\n` 1\\n\\n2 pexp η ´ 1q2.\\n\\nProof. Substitute Lemma 4 into Corollary 1 and decompose result follows by realising that under Prescription 1, the perturbations satisfy }∆Wk}˚ “\\n\\npwqJ∆w “\\n\\n∇\\n\\nL\\n\\nw\\n\\nL k“1 trp∆W J\\n\\na\\n\\nq. The Wk L k ∇ dk{dk´1 ¨ η L .\\n\\nř\\n\\nTheorem 2 (Automatic gradient descent) For a deep fully-connected network, under Assumptions 1 and 2 and Prescription 1, the majorisation of square loss given in Lemma 5 is minimised by setting:\\n\\n1 `\\n\\nη “ log\\n\\n?\\n\\n1 ` 4G 2\\n\\n,\\n\\n∆Wk “ ´\\n\\na\\n\\nη L\\n\\n¨\\n\\ndk{dk´1 ¨ ∇ } ∇\\n\\nWk L }F Wk L\\n\\n,\\n\\nfor all layers k “ 1, ..., L.\\n\\nProof. The inner product tr k ∇ }∆Wk}˚ ∆Wk satisﬁes ∆Wk{}∆Wk}˚ “ ´\\n\\nWk L\\n\\n∆W J\\n\\nthat appears in Lemma 5 is most negative when the perturbation {}\\n\\n}˚. Substituting this result back into Lemma 5 yields:\\n\\nWk L\\n\\n∇\\n\\nWk L ∇ „ Lÿ a\\n\\npw ` ∆wq ď\\n\\nL\\n\\npwq ´\\n\\nL\\n\\nη L\\n\\ndk{dk´1 ˆ\\n\\nk“1\\n\\n\\uf6be\\n\\n` 1\\n\\n2 pexp η ´ 1q2.\\n\\n} ∇ } ∇\\n\\nWk L Wk L\\n\\n}2 F }˚\\n\\nUnder Assumption 2, we have that }\\n\\n∇\\n\\nWk L\\n\\n}2 F {}\\n\\n}˚ “ }\\n\\nWk L\\n\\n∇ ∇ pwq ´ η ¨ G ` 1\\n\\nWk L 2 pexp η ´ 1q2.\\n\\npw ` ∆wq ď\\n\\nL\\n\\nL\\n\\n}F and so this inequality simpliﬁes to:\\n\\nTaking the derivative of the right-hand side with respect to η and setting it to zero yields pexp η ´ 1q exp η “ G. Applying the quadratic formula and retaining the positive solution yields exp η “ 1 1 ` 4Gq. Combining dk{dk´1 ¨ η this with the relation that ∆Wk{}∆Wk}˚ “ ´ L by Prescription 1 yields the result.\\n\\n}˚ and applying that }∆Wk}˚ “\\n\\n2 p1 `\\n\\nWk L\\n\\nWk L\\n\\na\\n\\n{}\\n\\n∇\\n\\n∇\\n\\n?\\n\\n20\\n\\nLemma 6 (Bounded objective) For square loss, the objective is bounded as follows:\\n\\npwq ď\\n\\nL\\n\\n1 |S|\\n\\nÿ\\n\\npx,yqPS\\n\\n}f px; wq}2 2dL\\n\\n2 ` }y}2 2\\n\\nď 1 under Prescription 1.\\n\\nProof. The result follows by the following chain of inequalities:\\n\\npwq :“\\n\\nL\\n\\n1 |S|\\n\\nÿ\\n\\npx,yqPS\\n\\n1 2dL\\n\\n}f px; wq ´ y}2\\n\\n2 ď\\n\\n1 |S|\\n\\nÿ\\n\\npx,yqPS\\n\\n1 2dL\\n\\nwhere the second inequality holds under Prescription 1.\\n\\np}f px; wq}2\\n\\n2 ` }y}2\\n\\n2q ď\\n\\n1 |S|\\n\\nÿ\\n\\npx,yqPS\\n\\ndL ` dL 2dL\\n\\n“ 1,\\n\\nLemma 7 (Bounded gradient) For square loss, the norm of the gradient at layer k is bounded as follows:\\n\\n} ∇\\n\\nWk L\\n\\n}F ď\\n\\nś\\n\\nL l“1 }Wl}˚ }Wk}˚\\n\\nd\\n\\n¨\\n\\npwq 2 L dL\\n\\n¨\\n\\nd\\n\\n1 |S|\\n\\nc\\n\\n}x}2\\n\\n2 ď\\n\\n2 ¨\\n\\ndk´1 dk\\n\\nÿ\\n\\npx,yqPS\\n\\nunder Prescription 1.\\n\\nProof. By the chain rule, the gradient of mean square error objective may be written:\\n\\npwq “\\n\\nWk L\\n\\n∇\\n\\n1 |S|\\n\\nÿ\\n\\npx,yqPS\\n\\n1 dL\\n\\npf px; wq ´ yqJWL ¨ DL´1WL´1 . . . Dk`1Wk`1 ¨ Dk b Dk´1Wk´1 . . . D1W1x,\\n\\nwhere b denotes the outer product and Dk denotes a diagonal matrix whose entries are one when relu is active and zero when relu is inactive. Since the operator norm }Dk}˚ “ 1, we have that the Frobenius norm } ∇ 1 |S|\\n\\n}pf px; wq ´ yqJWL ¨ DL´1WL´1 . . . Dk`1Wk`1 ¨ Dk b Dk´1Wk´1 . . . D1W1x}F\\n\\npwq}F is bounded from above by:\\n\\nWk L ÿ\\n\\n1 dL\\n\\npx,yqPS\\n\\n1 dL\\n\\n}pf px; wq ´ yqJWL ¨ DL´1WL´1 . . . Dk`1Wk`1 ¨ Dk}2 ¨ }Dk´1Wk´1 . . . D1W1x}2\\n\\n}f px; wq ´ y}2 ¨ }WL}˚ ¨ }WL´1}˚ . . . }Wk`1}˚ ¨ }Wk´1}˚ . . . }W1}˚ ¨ }x}2\\n\\nÿ\\n\\npx,yqPS ÿ\\n\\n1 dL\\n\\n1 |S|\\n\\n1 |S| ś\\n\\npx,yqPS L l“1 }Wl}˚ }Wk}\\n\\nś\\n\\nL l“1 }Wl}˚ }Wk}˚\\n\\nś\\n\\nL l“1 }Wl}˚ }Wk}˚\\n\\n¨\\n\\n¨\\n\\nˆ\\n\\n1 |S|\\n\\nÿ\\n\\npx,yqPS d\\n\\n1 ? dL\\n\\nd\\n\\n2 |S|\\n\\npwq 2 L dL\\n\\n¨\\n\\nÿ\\n\\npx,yqPS d\\n\\n1 |S|\\n\\n“\\n\\nď\\n\\n“\\n\\nď\\n\\n“\\n\\n1 dL\\n\\n}f px; wq ´ y}2 ¨ }x}2\\n\\n}f px; wq ´ y}2 2 ¨\\n\\nd\\n\\n1 |S|\\n\\nÿ\\n\\npx,yqPS\\n\\n}x}2 2\\n\\n1 2dL\\n\\nÿ\\n\\n}x}2 2.\\n\\npx,yqPS\\n\\nIn the above argument, the ﬁrst inequality follows by recursive application of the operator norm upper bound, and the second inequality follows from the Cauchy-Schwarz inequality. The right-hand side simpliﬁes under Prescription 1, and we may apply Lemma 6 to obtain:\\n\\n} ∇\\n\\npwq}F ď\\n\\nWk L\\n\\nś\\n\\nL l“1 }Wl}˚ }Wk}˚\\n\\nd\\n\\n¨\\n\\n2\\n\\npwq L dL\\n\\n¨\\n\\nd\\n\\n1 |S|\\n\\nÿ\\n\\npx,yqPS\\n\\n}x}2\\n\\n2 ď\\n\\na\\n\\nc\\n\\na\\n\\ndL{d0 dk{dk´1\\n\\n¨\\n\\n2 dL\\n\\n¨\\n\\nc\\n\\na\\n\\nd0 “\\n\\n?\\n\\n2 ¨\\n\\ndk´1 dk\\n\\n.\\n\\nLemma 8 (Convergence rate to critical point) Consider a fully-connected network trained by automatic gradi- ent descent (Theorem 2) and square loss for T iterations. Let Gt denote the gradient summary (Deﬁnition 11) at step t ď T . Under Assumptions 1 and 2 and Prescription 1, AGD converges at the following rate:\\n\\n21\\n\\nProof. Theorem 2 prescribes that exp η “ 1 . We begin by proving some useful auxiliary bounds. By Lemma 7 and Prescription 1, the gradient summary is bounded by:\\n\\n1 ` 4Gq, and so η “ log\\n\\n2 p1 `\\n\\n1 `\\n\\n1`4G´1 2\\n\\n`\\n\\n?\\n\\n˘\\n\\nmin tPt1,...,T u\\n\\nG2\\n\\nt ď\\n\\n11 T\\n\\n.\\n\\n?\\n\\nG :“\\n\\n1 L\\n\\nLÿ\\n\\na\\n\\ndk{dk´1 ¨ }\\n\\npwq}F ď\\n\\nWk L\\n\\n∇\\n\\nk“1\\n\\n1 L\\n\\nLÿ\\n\\n?\\n\\nk“1\\n\\n2 ă 2.\\n\\nThe fact that the gradient summary G is less than two is important because, for x ď 1, we have that logp1 ` xq ě x log 2. In turn, this implies that since G ă 2, we have that η “ log 1` log 2. It will also be important to know that for G ă 2, we have that 1\\n\\n1`4G 2 ď G.\\n\\n1`4G´1 2\\n\\ně\\n\\n?\\n\\n?\\n\\n?\\n\\n2 ¨ G ď\\n\\n1`4G´1 2\\n\\nWith these bounds in hand, the analysis becomes fairly standard. By an intermediate step in the proof of Theorem 2, the change in objective across a single step is bounded by:\\n\\npw ` ∆wq ´\\n\\nL\\n\\nL\\n\\npwq ď ´η ¨ G ` 1 ? 1`4G´1 2\\n\\n2 pexp η ´ 1q2 pG log 2 ´ 1 2\\n\\n?\\n\\n1`4G´1 2\\n\\nq\\n\\n2 ¨ plog 2 ´ 1\\n\\n2 q ¨ G2 ď ´G2{11,\\n\\nď ´ ď ´ 1\\n\\nwhere the second and third inequalities follow by our auxiliary bounds. Letting Gt denote the gradient summary at step t, averaging this bound over time steps and applying the telescoping property yields:\\n\\nmin tPr1,...,T s\\n\\nG2\\n\\nt ď\\n\\n1 T\\n\\nTÿ\\n\\nt“1\\n\\nG2\\n\\nt ď\\n\\n11 T\\n\\nTÿ\\n\\nL\\n\\nt“1\\n\\npwtq ´\\n\\npwt`1q “\\n\\nL\\n\\n11 T\\n\\n¨ p\\n\\npw1q ´\\n\\nL\\n\\npwT qq ď\\n\\nL\\n\\n11 T\\n\\n,\\n\\nwhere the ﬁnal inequality follows by Lemma 6 and the fact that\\n\\npwT q ě 0.\\n\\nL\\n\\nTheorem 3 (Convergence rate to global minima) For automatic gradient descent (Theorem 2) in the same setting as Lemma 8 but with the addition of Assumption 3, the mean squared error objective at step T obeys:\\n\\npwT q ď\\n\\nL\\n\\n1 α2\\n\\nˆ\\n\\n6 T\\n\\n.\\n\\na\\n\\nProof. By Assumption 3, the gradient summary at time step t must satisfy Gt ě α ˆ the objective at time step t is bounded by\\n\\npwtq. Therefore t {p2α2q. Combining with Lemma 8 then yields that:\\n\\npwtq ď G2\\n\\n2 ¨\\n\\nL\\n\\nL\\n\\npwT q “ min\\n\\npwtq ď\\n\\ntPr1,...,T s L\\n\\nL\\n\\n1 2α2 min\\n\\ntPr1,...,T s\\n\\nG2\\n\\nt ď\\n\\n6 α2T\\n\\n.\\n\\nThe proof is complete.\\n\\n22\\n\\nB PyTorch Implementation\\n\\nThe following code implements automatic gradient descent in PyTorch (Paszke et al., 2019). We include a single gain hyperparameter which controls the update size and may be increased from its default value of 1.0 to slightly accelerate training. We emphasise that all the results reported in the paper used a gain of unity.\\n\\nimport math import torch\\n\\nfrom torch.nn.init import orthogonal_\\n\\ndef singular_value(p):\\n\\nsv = math.sqrt(p.shape[0] / p.shape[1]) if p.dim() == 4:\\n\\nsv /= math.sqrt(p.shape[2] * p.shape[3])\\n\\nreturn sv\\n\\nclass AGD:\\n\\n@torch.no_grad() def __init__(self, net, gain=1.0):\\n\\nself.net = net self.depth = len(list(net.parameters())) self.gain = gain\\n\\nfor p in net.parameters():\\n\\nif p.dim() == 1: raise Exception(\"Biases are not supported.\") if p.dim() == 2: orthogonal_(p) if p.dim() == 4:\\n\\nfor kx in range(p.shape[2]):\\n\\nfor ky in range(p.shape[3]):\\n\\northogonal_(p[:,:,kx,ky])\\n\\np *= singular_value(p)\\n\\n@torch.no_grad() def step(self):\\n\\nG = 0 for p in self.net.parameters():\\n\\nG += singular_value(p) * p.grad.norm(dim=(0,1)).sum()\\n\\nG /= self.depth\\n\\nlog = math.log(0.5 * (1 + math.sqrt(1 + 4*G)))\\n\\nfor p in self.net.parameters():\\n\\nfactor = singular_value(p) / p.grad.norm(dim=(0,1), keepdim=True) p -= self.gain * log / self.depth * factor * p.grad\\n\\n23', metadata={'source': '/var/folders/3j/ztkfg0cn00n8q91tdg5hkf7m0000gr/T/tmpwj_30tbb'})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import OnlinePDFLoader\n",
    "loader = OnlinePDFLoader(pdf_url)\n",
    "data = loader.load()\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "defa71b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T10:04:51.736247Z",
     "start_time": "2023-06-03T10:04:51.712732Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_semantic_blocks_based_on_font_size(pdf_url):\n",
    "    from langchain.document_loaders import PDFMinerPDFasHTMLLoader\n",
    "    loader = PDFMinerPDFasHTMLLoader(pdf_url)\n",
    "    data = loader.load()[0]\n",
    "\n",
    "    from bs4 import BeautifulSoup\n",
    "    soup = BeautifulSoup(data.page_content,'html.parser')\n",
    "    content = soup.find_all('div')\n",
    "    import re\n",
    "    cur_fs = None\n",
    "    cur_text = ''\n",
    "    snippets = []   # first collect all snippets that have the same font size\n",
    "    for c in content:\n",
    "        sp = c.find('span')\n",
    "        if not sp:\n",
    "            continue\n",
    "        st = sp.get('style')\n",
    "        if not st:\n",
    "            continue\n",
    "        fs = re.findall('font-size:(\\d+)px',st)\n",
    "        if not fs:\n",
    "            continue\n",
    "        fs = int(fs[0])\n",
    "        if not cur_fs:\n",
    "            cur_fs = fs\n",
    "        if fs == cur_fs:\n",
    "            cur_text += c.text\n",
    "        else:\n",
    "            snippets.append((cur_text,cur_fs))\n",
    "            cur_fs = fs\n",
    "            cur_text = c.text\n",
    "    snippets.append((cur_text,cur_fs))\n",
    "    # Note: The above logic is very straightforward. One can also add more strategies such as removing duplicate snippets (as\n",
    "    # headers/footers in a PDF appear on multiple pages so if we find duplicatess safe to assume that it is redundant info)\n",
    "    from langchain.docstore.document import Document\n",
    "    cur_idx = -1\n",
    "    semantic_snippets = []\n",
    "    # Assumption: headings have higher font size than their respective content\n",
    "    for s in snippets:\n",
    "        # if current snippet's font size > previous section's heading => it is a new heading\n",
    "        if not semantic_snippets or s[1] > semantic_snippets[cur_idx].metadata['heading_font']:\n",
    "            metadata={'heading':s[0], 'content_font': 0, 'heading_font': s[1]}\n",
    "            metadata.update(data.metadata)\n",
    "            semantic_snippets.append(Document(page_content='',metadata=metadata))\n",
    "            cur_idx += 1\n",
    "            continue\n",
    "\n",
    "        # if current snippet's font size <= previous section's content => content belongs to the same section (one can also create\n",
    "        # a tree like structure for sub sections if needed but that may require some more thinking and may be data specific)\n",
    "        if not semantic_snippets[cur_idx].metadata['content_font'] or s[1] <= semantic_snippets[cur_idx].metadata['content_font']:\n",
    "            semantic_snippets[cur_idx].page_content += s[0]\n",
    "            semantic_snippets[cur_idx].metadata['content_font'] = max(s[1], semantic_snippets[cur_idx].metadata['content_font'])\n",
    "            continue\n",
    "\n",
    "        # if current snippet's font size > previous section's content but less tha previous section's heading than also make a new \n",
    "        # section (e.g. title of a pdf will have the highest font size but we don't want it to subsume all sections)\n",
    "        metadata={'heading':s[0], 'content_font': 0, 'heading_font': s[1]}\n",
    "        metadata.update(data.metadata)\n",
    "        semantic_snippets.append(Document(page_content='',metadata=metadata))\n",
    "        cur_idx += 1\n",
    "    semantic_snippets = [s.page_content.strip() for s in semantic_snippets]\n",
    "    semantic_snippets = [s for s in semantic_snippets if len(s)>0]\n",
    "    return semantic_snippets\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5099a3f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T10:05:04.735749Z",
     "start_time": "2023-06-03T10:04:52.577094Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20402"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_url = \"https://arxiv.org/pdf/2304.05187.pdf\"\n",
    "semantic_snippets = create_semantic_blocks_based_on_font_size(pdf_url)\n",
    "print(len(semantic_snippets))\n",
    "# print(semantic_snippets[4])\n",
    "len(enc.encode(\" \".join(semantic_snippets)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac052254",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T10:07:23.954805Z",
     "start_time": "2023-06-03T10:07:23.932355Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In this section, we will focus our eﬀorts on deriving an optimiser for deep fully-connected networks trained\\nwith square loss. The derivation for cross-entropy loss is analogous. Proofs are relegated to Appendix A.\\nDeﬁnition 10 (Fully-connected network) A fully-connected network (FCN) f of depth L maps an input x P Rd0\\nto an output f px; wq P RdL via L matrix multiplications interspersed by non-linearity relupzq :“ maxp0, zq:\\nf px; wq :“ WL ˝ prelu ˝ WL´1q ˝ prelu ˝ WL´2q ˝ ¨ ¨ ¨ ˝ prelu ˝ W1xq.\\nIn this expression, w denotes the tuple of matrices w “ pW1, ..., WLq with kth matrix Wk in Rdkˆdk´1. In\\nwhat follows, we will ﬁnd the following dimensional scaling to be particularly convenient:\\nPrescription 1 (Dimensional scaling) For η ą 0, the data px, yq, weights Wk and updates ∆Wk should obey:\\n}x}2 “\\n}Wk}˚ “\\n}∆Wk}˚ “\\n}y}2 “\\na\\nd0;\\na\\na\\na\\ndk{dk´1\\ndk{dk´1 ¨ η\\nL\\ndL.\\nfor all k “ 1, ..., L;\\nfor all k “ 1, ..., L;\\n(input scaling)\\n(weight scaling)\\n(update scaling)\\n(target scaling)\\nWhile results can be derived without adopting Prescription 1, the scalings substantially simplify our formulae.\\ndL{d0.\\nOne reason for this is that, under Prescription 1, we have the telescoping property that\\nFor a concrete example of how this helps, consider the following bound on the norm of the network outputs:\\nLemma 3 (Output bound) The output norm of a fully-connected network f obeys the following bound:\\nL\\nk“1 }Wk}˚ “\\nś\\na\\n«\\nLź\\nﬀ\\na\\n}f px; wq}2 ď\\n}Wk}˚\\nˆ }x}2 “\\ndL under Prescription 1.\\nk“1\\nSo, under Prescription 1, the bound is simple. Furthermore, the scaling of the update with a single parameter\\nη reduces the problem of solving for an optimiser to a single parameter problem. To see how this might make\\nlife easier, consider the following lemma that relates weight perturbations to functional perturbations:\\nLemma 4 (Deep relative trust) When adjusting the weights w “ pW1, ..., WLq of a fully-connected network\\nf by ∆w “ p∆W1, ..., ∆WLq, the induced functional perturbation ∆f pxq :“ f px; w ` ∆wq ´ f px; wq obeys:\\n«\\nLź\\nﬀ\\n}∆f pxq}2 ď\\n}Wk}˚\\nˆ }x}2 ˆ\\nk“1\\n«\\nˆ\\nLź\\nk“1\\n1 `\\n}∆Wk}˚\\n}Wk}˚\\n˙\\nﬀ\\na\\n´ 1\\nď\\ndL ˆ pexp η ´ 1q under Prescription 1.\\nSo, under Prescription 1, the single parameter η directly controls the size of functional perturbations.\\nIn terms of enforcing Prescription 1 in practice, the norms of the data px, yq may be set via pre-processing, the\\nnorm of the update ∆Wk may be set via the optimisation algorithm and the norm of the weight matrix Wk\\nmay be set by the choice of initialisation. While, yes, }Wk}˚ may drift during training, the amount that this\\ncan happen is limited by Weyl (1912)’s inequality for singular values. In particular, after one step the perturbed\\noperator norm }Wk ` ∆WK}˚ is sandwiched like p1 ´ η{Lq ¨ }Wk}˚ ď }Wk ` ∆WK}˚ ď p1 ` η{Lq ¨ }Wk}˚.\\n3.1 Deriving automatic gradient descent\\nWith both functional majorisation and deep relative trust in hand, we can majorise the deep network objective:\\nLemma 5 (Exponential majorisation) For an FCN with square loss, under Assumption 1 and Prescription 1:\\npw ` ∆wq ď\\nL\\npwq `\\nL\\nη\\nL\\nLÿ\\n„\\na\\ndk{dk´1 ˆ tr\\nk“1\\n8\\n∆W J\\nk ∇\\n}∆Wk}˚\\nWk L\\n\\uf6be\\n` 1\\n2 pexp η ´ 1q2.\\ndef initialise_weights():\\nfor layer k in t1, ..., Lu:\\nWk „ uniformporthogonalpdk, dk´1qq\\nb\\nWk Ð Wk ¨\\ndk\\ndk´1\\n# sample a semi-orthogonal matrix\\n# rescale its singular values\\ndef update_weights():\\nř\\nG Ð 1\\nL\\nη Ð log 1`\\nL\\nl“1 }\\n?\\n∇\\n1`4G\\n2\\n}F ¨\\nWk L\\nb\\ndk\\ndk´1\\n# get gradient summary\\n# set automatic learning rate\\nfor layer k in t1, ..., Lu:\\nWk Ð Wk ´ η\\nL ¨ ∇\\n}\\n∇\\nWk L\\n}F\\nWk L\\nb\\n¨\\ndk\\ndk´1\\n# update weights\\nAlgorithm 1: Automatic gradient descent. The matrix Wk in Rdkˆdk´1 is the weight matrix at layer k.\\nevaluated on a mini-batch B of training samples.\\nThe gradient\\nis with respect to the objective\\nWk L\\n∇\\nL\\nk ∇\\nWk L\\n{}∆Wk}˚ between the perturbation matrix ∆Wk and the gradient matrix\\nObserve that the majorisation only depends on the magnitude of the scalar η and on some notion of angle\\ntr ∆W J\\n. To derive\\nan optimiser, we would now like to minimise this majorisation with respect to η and this angle. First, let us\\nintroduce one additional assumption and one additional deﬁnition:\\nAssumption 2 (Gradient conditioning) The gradient satisﬁes rankstable ∇\\n}˚ of the gradient\\nThis assumption implies that the Frobenius norm }\\nWk L\\n∇\\nat layer k are equal. It is not immediately obvious why this should be a good assumption. After all, the\\n“ 1\\nhk ‘pf pxq, yq b hk´1, where hk´1pxq and\\ngradient is a sum of |S| rank-one matrices:\\n|S|\\nhkpxq denote the inputs and outputs of the weight matrix Wk at layer k, and b denotes the outer product.\\nto have a stable rank of minpdk, dk´1, |S|q. But it turns out\\nSo, naïvely, one might expect the gradient\\nto be a good assumption in practice (Yang & Hu, 2021; Yang et al., 2021). And for the deﬁnition:\\nDeﬁnition 11 (Gradient summary) At a weight setting w, the gradient summary G is given by:\\n“ 1 at all layers k “ 1, ..., L.\\n}F and operator norm }\\npx,yqPS ∇\\nWk L\\nř\\nWk L\\nWk L\\nWk L\\nWk L\\n∇\\n∇\\n∇\\n∇\\nG :“\\n1\\nL\\nLÿ\\na\\ndk{dk´1 ¨ }\\npwq}F .\\nWk L\\n∇\\nk“1\\nThe gradient summary is a weighted average of gradient norms over layers. It can be thought of as a way to\\nmeasure the size of the gradient while accounting for the fact that the weight matrices at diﬀerent layers may\\nbe on diﬀerent scales. This is related to the concept of the gradient scale coeﬃcient of Philipp et al. (2017).\\nWe now have everything we need to derive automatic gradient descent via the majorise-minimise principle:\\nTheorem 2 (Automatic gradient descent) For a deep fully-connected network, under Assumptions 1 and 2\\nand Prescription 1, the majorisation of square loss given in Lemma 5 is minimised by setting:\\n1 `\\nη “ log\\n?\\n1 ` 4G\\n2\\n,\\n∆Wk “ ´\\na\\nη\\nL\\n¨\\ndk{dk´1 ¨ ∇\\n}\\n∇\\nWk L\\n}F\\nWk L\\n,\\nfor all layers k “ 1, ..., L.\\nWe present pseudocode for this theorem in Algorithm 1, and a PyTorch implementation in Appendix B. Via\\na simple derivation based on clear algorithmic principles, automatic gradient descent uniﬁes various heuristic\\nand theoretical ideas that have appeared in the literature:\\n9\\n• Relative updates. The update is scaled relative to the norm of the weight matrix to which it is applied—\\nassuming the weight matrices are scaled according to Prescription 1. Such a scaling was proposed by You\\net al. (2017) and further explored by Carbonnelle & Vleeschouwer (2019) and Bernstein et al. (2020).\\nThere is evidence that such relative synaptic updates may occur in neuroscience (Loewenstein et al., 2011).\\n• Depth scaling. Scaling the perturbation strength like 1{L for networks of depth L was proposed on\\ntheoretical grounds by Bernstein et al. (2020) based on analysis via deep relative trust.\\n• Width scaling. The dimensional factors of dk and dk´1 that appear closely relate to the maximal update\\nparameterisation of Yang & Hu (2021) designed to ensure hyperparameter transfer across network width.\\n• Gradient clipping. The logarithmic dependence of the update on the gradient summary may be seen as an\\nautomatic form of adaptive gradient clipping (Brock et al., 2021)—a technique which clips the gradient\\nonce its magnitude surpasses a certain threshold set by a hyperparameter.\\n3.2 Convergence analysis\\nThis section presents theoretical convergence rates for automatic gradient descent. While the spirit of\\nthe analysis is standard in optimisation theory, the details may still prove interesting for their detailed\\ncharacterisation of the optimisation properties of deep networks. For instance, we propose a novel Polyak-\\nŁojasiewicz inequality tailored to the operator structure of deep networks. We begin with two observations:\\nLemma 6 (Bounded objective) For square loss, the objective is bounded as follows:\\npwq ď\\nL\\n1\\n|S|\\nÿ\\npx,yqPS\\n}f px; wq}2\\n2dL\\n2 ` }y}2\\n2\\nď 1 under Prescription 1.\\nLemma 7 (Bounded gradient) For square loss, the norm of the gradient at layer k is bounded as follows:\\n}\\n∇\\nWk L\\n}F ď\\nś\\nL\\nl“1 }Wl}˚\\n}Wk}˚\\nd\\n¨\\npwq\\n2\\nL\\ndL\\n¨\\nd\\n1\\n|S|\\nc\\n}x}2\\n2 ď\\n2 ¨\\ndk´1\\ndk\\nÿ\\npx,yqPS\\nunder Prescription 1.\\nThese results help us prove that automatic gradient descent converges to a point where the gradient vanishes:\\nLemma 8 (Convergence rate to critical point) Consider a fully-connected network trained by automatic gradi-\\nent descent (Theorem 2) and square loss for T iterations. Let Gt denote the gradient summary (Deﬁnition 11)\\nat step t ď T . Under Assumptions 1 and 2 and Prescription 1, AGD converges at the following rate:\\nmin\\ntPt1,...,T u\\nG2\\nt ď\\n11\\nT\\n.\\nThis lemma can be converted into a convergence rate to a global minimum with one additional assumption:\\nAssumption 3 (Deep Polyak-Łojasiewicz inequality) For some α ą 0, the gradient norm is lower bounded by:\\n}\\n∇\\nWk L\\n}F ě α ˆ\\nś\\nL\\nl“1 }Wl}˚\\n}Wk}˚\\nd\\n¨\\npwq\\n2\\nL\\ndL\\n¨\\nd\\n1\\n|S|\\nÿ\\npx,yqPS\\nc\\n}x}2\\n2 “ α ˆ\\n2 ¨\\npwq ¨\\nL\\ndk´1\\ndk\\nunder Prescription 1.\\nThis lower bound mirrors the structure of the upper bound in Lemma 7. The parameter α captures how much\\nof the gradient is attenuated by small singular values in the weights and by deactivated relu units. While\\nPolyak-Łojasiewicz inequalities are common in the literature (Liu et al., 2022), our assumption is novel in\\nthat it pays attention to the operator structure of the network. Assumption 3 leads to the following theorem:\\nTheorem 3 (Convergence rate to global minima) For automatic gradient descent (Theorem 2) in the same\\nsetting as Lemma 8 but with the addition of Assumption 3, the mean squared error objective at step T obeys:\\npwT q ď\\nL\\n1\\nα2\\nˆ\\n6\\nT\\n.\\n10\\nFigure 4: Benchmarking automatic gradient descent on a range of architectures and datasets. Solid\\nlines are AGD and faint dashed lines are tuned Adam except for ImageNet where the dashed line is SGD\\nwith a ﬁxed learning rate of 0.1. ImageNet used cross-entropy loss with a mini-batch size of 1024. The other\\nexperiments used square loss with a mini-batch size of 128. The top row plots the automatic learning rate (η\\nin the main text) and objective value. The maximum and minimum learning rate for each epoch is included\\nin addition to the mean for the ﬁrst three plots. The bottom row shows the train and test accuracy.\\n3.3 Experiments\\nThe goal of our experiments was twofold. First, we wanted to test automatic gradient descent (AGD,\\nAlgorithm 1) on a broad variety of networks architectures and datasets to check that it actually works. In\\nparticular, we tested AGD on fully-connected networks (FCNs, Deﬁnition 10), and both VGG-style (Simonyan\\n& Zisserman, 2015) and ResNet-style (He et al., 2015) convolutional neural networks on the CIFAR-10,\\nCIFAR-100 (Krizhevsky, 2009) and ImageNet (Deng et al., 2009, ILSVRC2012) datasets with standard data\\naugmentation. And second, to see what AGD may have to oﬀer beyond the status quo, we wanted to compare\\nAGD to tuned Adam and SGD baselines, as well as Adam and SGD run with their default hyperparameters.\\nTo get AGD working with convolutional layers, we adopted a per-submatrix normalisation scheme. Speciﬁcally,\\nfor a convolutional tensor with ﬁlters of size kx ˆ ky, we implemented the normalisation separately for each of\\nthe kx ˆ ky submatrices of dimension channelsin ˆ channelsout. Since AGD does not yet support biases\\nor aﬃne parameters in batchnorm, we disabled these parameters in all architectures. To at least adhere to\\nPrescription 1 at initialisation, AGD draws initial weight matrices uniform semi-orthogonal and re-scaled\\nfan_in{fan_out. Adam and SGD baselines used the PyTorch default initialisation. A\\nby a factor of\\nPyTorch implementation of AGD reﬂecting these details is given in Appendix B. All experiments use square\\nloss except ImageNet which used cross-entropy loss. Cross-entropy loss has been found to be superior to\\nsquare loss for datasets with a large number of classes (Demirkaya et al., 2020; Hui & Belkin, 2021).\\na\\nOur experimental results are spread across ﬁve ﬁgures:\\n• Figure 1 presents some highlights of our results: First, AGD can train networks that Adam and SGD\\nwith default hyperparameters cannot. Second, for ResNet-18 on CIFAR-10, AGD attained performance\\ncomparable to the best-tuned performance of Adam and SGD. And third, AGD scales up to ImageNet.\\n11\\nFigure 5: Comparing automatic gradient descent to tuned Adam and SGD. An eight-layer fully-connected\\nnetwork was trained on CIFAR-10 with square loss. Dotted lines show test and solid lines show train\\nperformance. The left panel shows the objective value: AGD and Adam attained a smaller training objective\\nthan SGD. The middle panel shows train and test accuracies. The right panel shows the relative update size\\nL\\naveraged over layers: 1\\nk“1 }∆Wk}F {}Wk}F . We plot the maximum, minimum and mean over an epoch.\\nL\\nř\\n• Figure 4 displays the breadth of our experiments: from training a 16-layer fully-connected network on\\nCIFAR-10 to training ResNet-50 on ImageNet. Adam’s learning rate was tuned over the logarithmic\\ngrid t10´5, 10´4, ..., 10´1u while for ImageNet we used a default learning rate of 0.1 for SGD without any\\nmanual decay. AGD and Adam performed almost equally well on the depth-16 width-512 fully-connected\\nnetwork: 52.7% test accuracy for AGD compared to 53.5% for Adam. For ResNet-18 on CIFAR-10, Adam\\nattained 92.9% test accuracy compared to AGD’s 91.2%. On this benchmark, a fully-tuned SGD with\\nlearning rate schedule, weight decay, cross-entropy loss and bias and aﬃne parameters can attain 93.0%\\ntest accuracy (Liu, 2017). For VGG-16 on CIFAR-100, AGD achieved 67.4% test accuracy compared to\\nAdam’s 69.7%. Finally, on ImageNet AGD achieved a top-1 test accuracy of 65.5% after 350 epochs.\\n• Figure 5 compares AGD to Adam and SGD for training an eight-layer fully-connected network of width\\n256. Adam and SGD’s learning rates were tuned over the logarithmic grid t10´5, 10´4, ..., 10´1u. Adam’s\\noptimal learning rate of 10´4 was three orders of magnitude smaller than SGD’s optimal learning rate of\\n10´1. SGD did not attain as low of an objective value as Adam or AGD.\\n• Figure 6 shows that AGD can train FCNs with width ranging from 64 to 2048 and depth from 2 to 32 and\\nFigure 7 shows that AGD successfully trains a four-layer FCN at varying mini-batch size: from 32 to 4096.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print([i for i, c in enumerate(chunks) if \"Algorithm\" in c])\n",
    "\n",
    "# display_markdown(chunks[10], raw=True)\n",
    "\n",
    "semantic_snippets[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a1c04bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T10:09:17.531267Z",
     "start_time": "2023-06-03T10:09:17.519848Z"
    }
   },
   "outputs": [],
   "source": [
    "from DocIndex import PDFReaderTool, ChunkText, call_ai21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "70b825fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T09:53:57.667820Z",
     "start_time": "2023-06-03T09:53:30.613033Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: loaded, waiting for processing to complete\n",
      "Status: split, waiting for processing to complete\n",
      "Status: split, waiting for processing to complete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "doc_text = PDFReaderTool()(\"https://arxiv.org/pdf/2304.05187.pdf\")\n",
    "chunks = ChunkText(doc_text, 1024, 128)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "437e8f26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T10:12:14.341460Z",
     "start_time": "2023-06-03T10:12:14.336556Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Given below text is small part/fragment text of a larger document. Based on the below fragment of document, write a set of important key phrases or sentences in elaborate bullet points that can be derived from the fragment. We will use these key phrases or sentences to summarize the document later.\n",
    "\n",
    "\"{semantic_snippets[7][:9000]}\"\n",
    "\n",
    "Note: This is a raw unformatted document which may contain irrelevant text/syntax or other noise. Ignore what you consider noise or irrelevant.\n",
    "Based on the fragment of document, write a set of important key phrases (keywords) or sentences in elaborate bullet points that can be derived from the fragment.\n",
    "Don't give out small bullet points.\n",
    "\n",
    "Key phrases or keywords or detailed outline in markdown bullet points:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0cc3988",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T10:12:25.101087Z",
     "start_time": "2023-06-03T10:12:15.101662Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- We present a novel derivation of automatic gradient descent for deep fully-connected networks trained\n",
      "with square loss.\n",
      "- The derivation is based on majorisation-minimisation principles.\n",
      "- We show that the resulting algorithm converges to a critical point where the gradient vanishes.\n",
      "- We also present a Polyak-Łojasiewicz inequality tailored to the operator structure of deep networks.\n"
     ]
    }
   ],
   "source": [
    "import cohere  \n",
    "co = cohere.Client(\"0GFuzMkYpofxfwIAxSf142NPkcICrHuVuPN4kaca\")\n",
    "\n",
    "response = co.generate(  \n",
    "    model='command-nightly',  \n",
    "    prompt = prompt,  \n",
    "    max_tokens=512,  \n",
    "    temperature=0.6)\n",
    "intro_paragraph = response.generations[0].text\n",
    "print(intro_paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4fcb757f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T10:13:23.671867Z",
     "start_time": "2023-06-03T10:12:25.107045Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling POST https://api.ai21.com/studio/v1/j2-jumbo-instruct/complete failed with Exception: HTTPSConnectionPool(host='api.ai21.com', port=443): Read timed out. (read timeout=30)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Derivation of an optimiser for fully-connected networks trained with square loss\n",
      "1. Definition 10 (Fully-connected network)\n",
      "1. Prescription 1 (Dimensional scaling)\n",
      "1. Lemma 3 (Output bound)\n",
      "1. Lemma 4 (Deep relative trust)\n",
      "1. Lemma 5 (Exponential majorisation)\n",
      "1. Algorithm 1 (Automatic gradient descent)\n",
      "1. Lemma 6 (Bounded objective)\n",
      "1. Lemma 7 (Bounded gradient)\n",
      "1. Theorem 2 (Automatic gradient descent)\n",
      "1. Lemma 8 (Convergence rate to critical point)\n",
      "1. Assumption 3 (Deep Polyak-Łojasiewicz inequality)\n"
     ]
    }
   ],
   "source": [
    "print(call_ai21(prompt, temperature=0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "270dee33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T09:20:07.436044Z",
     "start_time": "2023-06-03T09:20:07.415974Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\\title{\n",
       "RWKV: Reinventing RNNs for the Transformer Era\n",
       "}\n",
       "\n",
       "\\author{\n",
       "Bo Peng ({ }^{1 *}) Eric Alcaide ({ }^{2,3,4 *}) Quentin Anthony ({ }^{2,5 *}) \\\\ Alon Albalak ({ }^{2,6}) Samuel Arcadinho (^{2,7}) Huanqi Cao (^{8}) Xin Cheng (^{9}) Michael Chung (^{10}) \\\\ Matteo Grella ({ }^{11}) Kranthi Kiran GV ({ }^{12}) Xuzheng (He ^{2}) Haowen Hou ({ }^{13}) Przemysaw Kazienko ({ }^{14}) \\\\ Jan Kocoń ({ }^{14}) Jiaming Kong ({ }^{15}) Bartłomiej Koptyra ({ }^{14}) Hayden Lau (^{2}) Krishna Sri Ipsit Mantri ({ }^{16}) \\\\ Ferdinand Mom ({ }^{17,18}) Atsushi Saito ({ }^{2,19}) Xiangru Tang ({ }^{20}) Bolun Wang ({ }^{27}) Johan S. Wind ({ }^{21}) Stanisław Woźniak (^{14}) \\\\ Ruichong Zhang ({ }^{8}) Zhenyuan Zhang ({ }^{2}) Qihang Zhao ({ }^{22,23}) Peng Zhou ({ }^{27}) Jian Zhu ({ }^{24}) Rui-Jie Zhu ({ }^{25,26}) \\\\ ({ }^{1}) RWKV Foundation ({ }^{2}) EleutherAI ({ }^{3}) University of Barcelona ({ }^{4}) Charm Therapeutics ({ }^{5}) Ohio State University \\\\ ({ }^{6}) University of California, Santa Barbara ({ }^{7}) Zendesk ({ }^{8}) Tsinghua University ({ }^{9}) Peking University \\\\ ({ }^{10}) Storyteller.io ({ }^{11}) Crisis (24{ }^{12}) New York University ({ }^{13}) National University of Singapore \\\\ ({ }^{14}) Wroclaw University of Science and Technology ({ }^{15}) Databaker Technology Co. Ltd ({ }^{16}) Purdue University \\\\ ({ }^{17}) Criteo AI Lab ({ }^{18}) Epita ({ }^{19}) Nextremer Co. Ltd. ({ }^{20}) Yale University ({ }^{21}) University of Oslo \\\\ ({ }^{22}) University of Science and Technology of China ({ }^{23}) Kuaishou Technology Co. Ltd \\\\ ({ }^{24}) University of British Columbia ({ }^{25}) University of California, Santa Cruz \\\\ ({ }^{26}) University of Electronic Science and Technology of China ({ }^{27}) RuoxinTech\n",
       "}\n",
       "\n",
       "\\begin{abstract}\n",
       "Transformers have revolutionized almost all natural language processing (NLP) tasks but suffer from memory and computational complexity that scales quadratically with sequence length. In contrast, recurrent neural networks (RNNs) exhibit linear scaling in memory and computational requirements but struggle to match the same performance as Transformers due to limitations in parallelization and scalability. We propose a novel model architecture, Receptance Weighted Key Value (RWKV), that combines the efficient parallelizable training of Transformers with the efficient inference of RNNs. Our approach leverages a linear attention mechanism and allows us to formulate the model as either a Transformer or an (RNN), which parallelizes computations during training and maintains constant computational and memory complexity during inference, leading to the first non-transformer architecture to be scaled to tens of billions of parameters. Our experiments reveal that RWKV performs on par with similarly sized Transformers, suggesting that future work can leverage this architecture to create more efficient models. This work presents a significant step towards reconciling the trade-offs between computational efficiency and model performance in sequence processing tasks. ({ }^{1})\n",
       "\\end{abstract}\n",
       "\n",
       "\\section{Introduction}\n",
       "\n",
       "Deep learning techniques have made significant strides in artificial intelligence, playing a pivotal\n",
       "\n",
       "* Equal first authorship. Others listed alphabetically.\n",
       "\n",
       "({ }^{1}) Code at: https://github.com/BlinkDL/RWKV-LM role in various scientific and industrial applications. These applications often involve complex sequential data processing tasks that include natural language understanding, conversational AI, time-series analysis, and even indirect modalities that can be reframed as sequences, such as images and graphs (Brown et al., 2020; Ismail Fawaz et al., 2019; Wu et al., 2020; Albalak et al., 2022). Predominant among these techniques are RNNs, convolutional neural networks (CNNs), and the Transformer models (Vaswani et al., 2017).\n",
       "\n",
       "Each of these has distinct drawbacks that restrict their efficiency in certain scenarios. RNNs suffer from the vanishing gradient problem, making them difficult to train for long sequences. Additionally, they cannot be parallelized in the time dimension during training, which restricts their scalability (Hochreiter, 1998; Le and Zuidema, 2016). CNNs, on the other hand, are only adept at capturing local patterns, which limits their capacity to deal with long-range dependencies, crucial to many sequence processing tasks (Bai et al., 2018).\n",
       "\n",
       "Transformer models emerged as a powerful alternative due to their ability to handle both local and long-range dependencies and their capability for parallelized training (Tay et al., 2022). Recent models such as GPT-3 (Brown et al., 2020), ChatGPT (OpenAI, 2022; Kocoń et al., 2023), GPT-4 (OpenAI, 2023), LLaMA (Touvron et al., 2023), and Chinchilla (Hoffmann et al., 2022) exemplify the capability of this architecture, pushing the frontiers of what's possible in NLP. Despite these significant advancements, the self-attention mechanism inherent to Transformers poses unique challenges, \n",
       "\n",
       "\\begin{tabular}{lcc}\n",
       "\\hline Model & Time & Space \\\\\n",
       "\\hline Transformer & (O\\left(T^{2} d\\right)) & (O\\left(T^{2}+T d\\right)) \\\\\n",
       "Reformer & (O(T \\log T d)) & (O(T \\log T+T d)) \\\\\n",
       "Linear Transformers & (O\\left(T d^{2}\\right)) & (O\\left(T d+d^{2}\\right)) \\\\\n",
       "Performer & (O\\left(T d^{2} \\log d\\right)) & (O\\left(T d \\log d+d^{2} \\log d\\right)) \\\\\n",
       "AFT-full & (O\\left(T^{2} d\\right)) & (O(T d)) \\\\\n",
       "MEGA & (O(c T d)) & (O(c T d)) \\\\\n",
       "\\hline RWKV (ours) & (O( T d )) & (O( d )) \\\\\n",
       "\\hline\n",
       "\\end{tabular}\n",
       "\n",
       "Table 1: Complexity comparison with different Transformers: Reformer (Kitaev et al., 2020), Linear Transformer (Katharopoulos et al., 2020), Performer (Choromanski et al., 2020), AFT (Zhai et al., 2021), MEGA (Ma et al., 2023). Here (T) denotes the sequence length, (d) the feature dimension, and (c) is MEGA's chunk size of quadratic attention.\n",
       "\n",
       "primarily due to its quadratic complexity. This complexity renders the architecture computationally expensive and memory-intensive for tasks involving long input sequences or in resource-constrained situations. These limitations have spurred a wealth of research aiming to improve the scaling properties of Transformers, often at the expense of some of the properties that make it so effective (Wang et al., 2020; Zaheer et al., 2020; Dao et al., 2022a).\n",
       "\n",
       "To tackle these challenges, we introduce the Receptance Weighted Key Value (RWKV) model, a novel architecture that effectively combines the strengths of RNNs and Transformers while circumventing key drawbacks. RWKV is carefully designed to alleviate the memory bottleneck and quadratic scaling associated with Transformers (Katharopoulos et al., 2020) with a more efficient linear scaling, while still preserving the rich, expressive properties that make the Transformer a dominant architecture in the field.\n",
       "\n",
       "One of the defining characteristics of RWKV is its ability to offer parallelized training and robust scalability, similar to Transformers. Moreover, we have reformulated the attention mechanism in RWKV to introduce a variant of linear attention, eschewing the traditional dot-product token interaction in favor of more effective channeldirected attention. This approach contrasts significantly with the traditional Transformer architecture, where specific token interactions predominantly drive attention. The implementation of linear attention in RWKV is carried out without approximation, which offers a considerable improvement in efficiency and enhances the scalability, see Table 1.\n",
       "\n",
       "The overarching motivation behind developing RWKV is to bridge the gap between computational efficiency and expressive capacity in neural net- work architectures. It offers a promising and viable solution for handling tasks involving large-scale models with billions of parameters, exhibiting competitive performance at a fraction of the computational cost. Our experimental results suggest that RWKV could be a valuable tool for addressing the ongoing challenges in scaling and deploying AI models across various domains, particularly those involving sequential data processing. Thus, RWKV paves the way for the next generation of more sustainable and computationally efficient AI models for sequence processing tasks.\n",
       "\n",
       "Our contributions in this paper are as follows:\n",
       "\n",
       "- We introduce the RWKV network architecture, which combines the advantages of RNNs and Transformers while mitigating their known limitations.\n",
       "\n",
       "- We propose a new attention mechanism reformulation that results in linear attention, eschewing the quadratic complexity associated with standard Transformer models.\n",
       "\n",
       "- We conduct a comprehensive series of experiments on benchmark datasets to showcase the performance, efficiency and scaling of RWKV in managing tasks involving large-scale models and long-range dependencies.\n",
       "\n",
       "- We release pretrained model ranging in size from 169 million to 14 billion parameters trained on the Pile (Gao et al., 2020). ({ }^{2})\n",
       "\n",
       "\\section{Related Work}\n",
       "\n",
       "Recently, a number of techniques have been proposed to address the limitations of transformers.\n",
       "\n",
       "Optimizing Attention Mechanism Many transformer variants (\"x-formers\") have been introduced to reduce the complexity of transformers (Tay et al., 2022), including sparse attention (Beltagy et al., 2020; Kitaev et al., 2020; Guo et al., 2022), approximating the full attention matrix (Wang et al., 2020; Ma et al., 2021; Choromanski et al., 2020), combining chunked attention with gating (Ma et al., 2023) and other efficient methods (Katharopoulos et al., 2020; Jaegle et al., 2021).\n",
       "\n",
       "Some recent works like FlashAttention (Dao et al., 2022a) and others (Rabe and Staats, 2022; Jang et al., 2019) share similarities with RWKV's chunked computation scheme. Despite being memory-efficient, their time complexity remains quadratic or contains chunk size as a hidden factor. In contrast, RWKV achieves better space and\n",
       "\n",
       "({ }^{2} https : / /) huggingface.co/RWKV time complexity during inference by formulating a linear attention as an (RNN).\n",
       "\n",
       "Attention Free Models Another line of research replaces the attention mechanism with other modules to scale to long sequences. MLP-Mixer and others (Tolstikhin et al., 2021; Liu et al., 2021) proposed the replacement of attention by MultiLayer Perceptrons (MLPs) in computer vision tasks. The Attention Free Transformer (AFT) (Zhai et al., 2021) replaces dot-product self-attention with a computationally efficient alternative which can be seen as a multi-head attention where each feature dimension corresponds to a head. Inspired by AFT, RWKV takes a similar approach but modifies the interaction weights for simplicity such that it can be transformed into an RNN. In parallel, RNNstyle (Hochreiter and Schmidhuber, 1997; Chung et al., 2014) recursive components have also been modified to increase context length, such as the Recurrent Memory Transformer (Bulatov et al., 2022, 2023) and Linear Recurrent Units (Orvieto et al., 2023). State space models (SSM) like S4 (Gu et al., 2022) and its variants (Dao et al., 2022b; Poli et al., 2023) are also proposed.\n",
       "\n",
       "Notably, Quasi-Recurrent neural network (QRNN) (Bradbury et al., 2017) uses both convolutional layers and recurrent pooling functions across timesteps and channels. While QRNN utilizes convolutional filters with fixed sizes, RWKV employs a time-mixing module as an attention mechanism with time-decaying factors. Different from the element-wise pooling in QRNN, RWKV includes a parametrized channel-mixing module (see the green blocks in Fig.1c) that is parallelizable.\n",
       "\n",
       "\\section{Background}\n",
       "\n",
       "Here we briefly review the fundamentals of RNNs and Transformers.\n",
       "\n",
       "\\subsection{Recurrent Neural Networks (RNNs)}\n",
       "\n",
       "Popular RNN architectures such as LSTM (Hochreiter and Schmidhuber, 1997) and GRU (Chung et al., 2014) are characterized by the following formulation (shown for LSTM, others can be reasoned similarly):\n",
       "\n",
       "\\[\n",
       "\\begin{aligned}\n",
       "f_{t} & =\\sigma_{g}\\left(W_{f} x_{t}+U_{f} h_{t-1}+b_{f}\\right), \\\\\n",
       "i_{t} & =\\sigma_{g}\\left(W_{i} x_{t}+U_{i} h_{t-1}+b_{i}\\right), \\\\\n",
       "o_{t} & =\\sigma_{g}\\left(W_{o} x_{t}+U_{o} h_{t-1}+b_{o}\\right), \\\\\n",
       "\\tilde{c}_{t} & =\\sigma_{c}\\left(W_{c} x_{t}+U_{c} h_{t-1}+b_{c}\\right), \\\\\n",
       "c_{t} & =f_{t} \\odot c_{t-1}+i_{t} \\odot \\tilde{c}_{t}, \\\\\n",
       "h_{t} & =o_{t} \\odot \\sigma_{h}\\left(c_{t}\\right) .\n",
       "\\end{aligned}\n",
       "\\]\n",
       "\n",
       "The data flow of RNNs is shown in Fig. 1a. Although RNNs can be factored into two linear blocks ( (W) and (U) ) and an RNN-specific block (1)-(6), as noted by Bradbury et al. (2017), the data dependency relying on previous time steps prohibits parallelizing these typical RNNs.\n",
       "\n",
       "\\subsection{Transformers and AFT}\n",
       "\n",
       "Introduced by Vaswani et al. (2017), Transformers are a class of neural networks that have become the dominant architecture for several NLP tasks. Instead of operating on sequences step-by-step like RNNs, Transformers rely on attention mechanisms to capture relationships between all input and all output tokens:\n",
       "\n",
       "\\[\n",
       "\\operatorname{Attn}(Q, K, V)=\\operatorname{softmax}\\left(Q K^{\\top}\\right) V\n",
       "\\]\n",
       "\n",
       "where the multi-headness and scaling factor (\\frac{1}{\\sqrt{d_{k}}}) is omitted for convenience. The core (Q K^{\\top}) multiplication is an ensemble of pairwise attention scores between each token in a sequence, which can be decomposed as vector operations:\n",
       "\n",
       "\\[\n",
       "\\operatorname{Attn}(Q, K, V)_{t}=\\frac{\\sum_{i=1}^{T} e^{q_{t}^{\\top} k_{i}} v_{i}}{\\sum_{i=1}^{T} e^{q_{t}^{\\top} k_{i}}}\n",
       "\\]\n",
       "\n",
       "In AFT (Zhai et al., 2021), this is alternately formulated as\n",
       "\n",
       "\\[\n",
       "\\operatorname{Attn}^{+}(W, K, V)_{t}=\\frac{\\sum_{i=1}^{t} e^{w_{t, i}+k_{i}} v_{i}}{\\sum_{i=1}^{t} e^{w_{t, i}+k_{i}}}\n",
       "\\]\n",
       "\n",
       "where (\\left\\{w_{t, i}\\right\\} \\in R^{T \\times T}) is the learned pair-wise position biases, and each (w_{t, i}) is a scalar.\n",
       "\n",
       "Inspired by AFT, we let each (w_{t, i}) in RWKV be a channel-wise time decay vector multiplied by the relative position, traced backwards from current time as it decays:\n",
       "\n",
       "\\[\n",
       "w_{t, i}=-(t-i) w\n",
       "\\]\n",
       "\n",
       "where (w \\in\\left(R_{\\geq 0}\\right)^{d}), with (d) the number of channels. We require (w) to be non-negative to ensure that (e^{w_{t, i}} \\leq 1) and the per-channel weights decay backwards in time. \n",
       "\n",
       "\n",
       "(a) RNN\n",
       "\n",
       "\n",
       "(b) QuasiRNN (Bradbury et al., 2017)\n",
       "\n",
       "\n",
       "(c) (RWKV)\n",
       "\n",
       "Figure 1: Computation structure of the RWKV in comparison to QRNN and RNN (Vanilla, LSTM, GRU, etc) architectures. Color codes: orange indicates time-mixing, convolutions or matrix multiplications, and the continuous block indicates that these computations can proceed simultaneously; blue signifies parameterless functions that operate concurrently along the channel or feature dimension (element-wise). Green indicates channel-mixing.\n",
       "\n",
       "\\section{The Receptance Weighted Key Value (RWKV) Model}\n",
       "\n",
       "The RWKV architecture derives its name from the four primary model elements used in the timemixing and channel-mixing blocks:\n",
       "\n",
       "- (R) : Receptance vector acting as the acceptance of past information.\n",
       "\n",
       "- (W) : Weight is the positional weight decay vector. A trainable model parameter.\n",
       "\n",
       "- (K) : Key is a vector analogous to (K) in traditional attention.\n",
       "\n",
       "- (V) : Value is a vector analogous to (V) in traditional attention.\n",
       "\n",
       "Interactions between the main elements for every timestep are multiplicative, as illustrated in Fig. 2\n",
       "\n",
       "\\subsection{High-Level Summary}\n",
       "\n",
       "The RWKV architecture is comprised of a series of stacked residual blocks, each formed by a timemixing and a channel-mixing sub-blocks with recurrent structures.\n",
       "\n",
       "The recurrence is formulated both as a linear interpolation between the current input and the input at the previous time step (a technique we refer to as time-shift mixing or token shift, indicated by the diagonal lines in Fig. 3), which can be adjusted independently for every linear projection of the input embedding (e.g., (R, K, V) in time-mixing, and (R), (K) in channel-mixing), and as the time-dependent update of the (W K V) which is formalized in equation 14. The (W K V) computation is similar to AFT (Zhai et al., 2021), but (W) is now a channel-wise vector multiplied by relative position rather than a pairwise matrix in AFT. We also introduce a vector (U) for separately attending to the current token in order to compensate for potential degeneration of (W) (see Appendix G for more details).\n",
       "\n",
       "\n",
       "Figure 2: RWKV block elements (left) and RWKV residual block with a final head for language modeling (right) architectures.\n",
       "\n",
       "\n",
       "Figure 3: RWKV architecture for language modelling. The time-mixing block is given by:\n",
       "\n",
       "\\[\n",
       "\\begin{aligned}\n",
       "r_{t} & =W_{r} \\cdot\\left(\\mu_{r} x_{t}+\\left(1-\\mu_{r}\\right) x_{t-1}\\right) \\\\\n",
       "k_{t} & =W_{k} \\cdot\\left(\\mu_{k} x_{t}+\\left(1-\\mu_{k}\\right) x_{t-1}\\right) \\\\\n",
       "v_{t} & =W_{v} \\cdot\\left(\\mu_{v} x_{t}+\\left(1-\\mu_{v}\\right) x_{t-1}\\right) \\\\\n",
       "w k v_{t} & =\\frac{\\sum_{i=1}^{t-1} e^{-(t-1-i) w+k_{i}} v_{i}+e^{u+k_{t}} v_{t}}{\\sum_{i=1}^{t-1} e^{-(t-1-i) w+k_{i}}+e^{u+k_{t}}} \\\\\n",
       "o_{t} & =W_{o} \\cdot\\left(\\sigma\\left(r_{t}\\right) \\odot w k v_{t}\\right)\n",
       "\\end{aligned}\n",
       "\\]\n",
       "\n",
       "where the (W K V) computation, (w k v_{t}), plays the role of (\\operatorname{Attn}(Q, K, V)) in Transformers without incurring a quadratic cost as interactions are between scalars. Intuitively, as time (t) increases, the vector (o_{t}) is dependent on a long history, represented by the summation of an increasing number of terms. For the target position (t), RWKV performs a weighted summation in the positional interval of ([1, t]), and then multiplies with the receptance (\\sigma(r)). Therefore, interactions are multiplicative inside a given timestep and summed over different timesteps.\n",
       "\n",
       "Further, the channel-mixing block is given by:\n",
       "\n",
       "\\[\n",
       "\\begin{aligned}\n",
       "r_{t} & =W_{r} \\cdot\\left(\\mu_{r} x_{t}+\\left(1-\\mu_{r}\\right) x_{t-1}\\right), \\\\\n",
       "k_{t} & =W_{k} \\cdot\\left(\\mu_{k} x_{t}+\\left(1-\\mu_{k}\\right) x_{t-1}\\right), \\\\\n",
       "o_{t} & =\\sigma\\left(r_{t}\\right) \\odot\\left(W_{v} \\cdot \\max \\left(k_{t}, 0\\right)^{2}\\right),\n",
       "\\end{aligned}\n",
       "\\]\n",
       "\n",
       "where we adopt squared ReLU activation (So et al., 2021). Note that in both time-mixing and channelmixing, by taking the sigmoid of the receptance, we're intuitively using it as a \"forget gate\" to eliminate unnecessary historical information.\n",
       "\n",
       "\\subsection{Transformer-like Parallelization}\n",
       "\n",
       "RWKV can be efficiently parallelized in what we call a time-parallel mode, reminiscent of Transformers. The time complexity of processing a batch of sequences in a single layer is (O\\left(B T d^{2}\\right)), which mainly consists of matrix multiplications (W_{\\square}, \\square \\in\\{r, k, v, o\\}) (assuming (B) sequences, (T) maximum tokens and (d) channels). Meanwhile, updating attention scores (w k v_{t}) requires a serial scan (see Appendix B for more detail) and has complexity (O(B T d)).\n",
       "\n",
       "The matrix multiplications can be parallelized akin to (W^{\\square}, \\square \\in\\{Q, K, V, O\\}) in typical Transformers. The element-wise (W K V) computation is time-dependent, but can be readily parallelized along the other two dimensions (Lei et al., 2018) ({ }^{3}).\n",
       "\n",
       "({ }^{3}) If the sequence is very long, more sophisticated methods such as Martin and Cundy (2017) that parallelize over sequence length could be used. Additionally, token shift is implemented as a simple offset in the temporal dimension at each block using PyTorch (Paszke et al., 2019) library as (nn). (\\operatorname{ZeroPad} 2 d ((\\theta, \\theta, 1,-1))).\n",
       "\n",
       "\\subsection{RNN-like Sequential Decoding}\n",
       "\n",
       "It is common in recurrent networks to use output at state (t) as input at state (t+1). This is especially evident in the autoregressive decoding inference of a language model, requiring each token to be computed before fed into the next step, making it possible for RWKV to take advantage of its RNNlike structure, referred to as time-sequential mode. In such circumstances, RWKV can be conveniently formulated recursively for decoding during inference, as shown in Appendix B, which leverages the advantage that each output token is dependent only on the latest state, which is of constant size, irrespective of the sequence length.\n",
       "\n",
       "It then behaves as an RNN decoder, yielding constant speed and memory footprint with respect to the sequence length, enabling the processing of longer sequences more efficiently. In contrast, selfattention typically requires a (KV) cache growing linearly with respect to the sequence length, resulting in degraded efficiency and increasing memory footprint and time as the sequence grows longer.\n",
       "\n",
       "\\subsection{Software Implementation}\n",
       "\n",
       "RWKV is originally implemented using the Pytorch Deep Learning Library (Paszke et al., 2019) and a custom CUDA kernel for the (W K V) computation explained in 4.7. Although RWKV is a general recurrent network, its current implementation focuses in the task of language modeling (RWKV-LM). The model architecture is comprised of an embedding layer, for which we follow the setup described in Section 4.7 and several identical residual blocks applied sequentially as seen in Fig. 2 and 3 following the principles outlined in Section 4.6. After the last block, a simple output projection head composed by a LayerNorm (Ba et al., 2016) and a linear projection is used to obtain the logits to be used in the next-token prediction task and calculate the cross entropy loss during training. Both the embeddings generated after the last residual block and the logits could also be used later for downstream NLP tasks. Training is performed in time-parallel mode (Section 4.2) while autoregressive inference and a potential chat interface ({ }^{4})\n",
       "\n",
       "({ }^{4} https : / /) github.com/BlinkDL/ChatRWKV leverage the time-sequential mode (Section 4.3).\n",
       "\n",
       "\\subsection{Gradient Stability and Layer Stacking}\n",
       "\n",
       "The RWKV architecture has been designed as a fusion of both Transformers and RNNs, offering the advantage of stable gradients and deeper architectures of Transformers compared to traditional RNNs while being efficient in inference.\n",
       "\n",
       "Previous work has sought to tackle the problem of gradient stability in RNNs with a variety of techniques including using non-saturated activation functions (Chandar et al., 2019), gating mechanism (Gu et al., 2019), gradient clipping (Pascanu et al., 2012), and adding constraints (Kanai et al., 2017; Miller and Hardt, 2018). While these techniques have seen little success, RWKV avoids the problem inherently by utilizing softmax in conjunction with RNN-style updates.\n",
       "\n",
       "The RWKV model features a single-step process for updating attention-like scores, which includes a time-dependent softmax operation that helps numerical stability and guards against vanishing gradients (for rigorous proof, see Appendix F). Intuitively, this operation ensures the gradient is propagated along the most relevant path. Layer normalization (Ba et al., 2016) is another key aspect of the architecture which enhances the training dynamics of deep neural networks by stabilizing gradients, addressing both vanishing and exploding gradient issues.\n",
       "\n",
       "These design elements not only contribute to the RWKV architecture's stability and learning capabilities but enable the stacking of multiple layers in a manner that surpasses the capabilities of any existing RNN. In doing so, the model is able to capture more complex patterns across various levels of abstraction (see also Appendix G).\n",
       "\n",
       "\\subsection{Harnessing Temporal Structure for Sequential Data Processing}\n",
       "\n",
       "RWKV captures and propagates sequential information through the combination of three mechanisms: recurrence, time decay and token shift.\n",
       "\n",
       "The recurrence in the time-mixing block of RWKV is the basis for the model's capacity to capture intricate relationships between sequence elements and to propagate locality information through time.\n",
       "\n",
       "The time decay mechanism (\\left(e^{-w}\\right.) and (e^{u}) in equation 14), maintains sensitivity to the positional relationship between sequence elements. By gradually diminishing the influence of past information over time, the model preserves a sense of temporal locality and progression, which is essential for sequential processing. This treatment of positional information in sequential data exhibits similarities to the Attention with Linear Biases (ALiBi) model (Press et al., 2022), where the linear biases facilitate input length extrapolation. In this context, the RWKV architecture can be perceived as a trainable version of (ALiBi), seamlessly incorporating positional information without the necessity for explicit encoding. It can also be seen as an extension of the gated convolution introduced in Zhai et al. (2021) to the full sequence length until a given step.\n",
       "\n",
       "The token shift or time-shift mixing, or (diagonal arrows in Figure 3), also contributes to the model's adaptation to sequential data. By linearly interpolating between the current input and the previous time step input, the model naturally aggregates and gates information in the input channels. The overall structure of time-shift mixing bears resemblance to the causal convolution with no dilations in WaveNet (van den Oord et al., 2016), which is a classical architecture used for forecasting time series data.\n",
       "\n",
       "\\subsection{Additional Optimizations}\n",
       "\n",
       "Custom Kernels To address inefficiencies in the (W K V) computation due to the sequential nature of the task when using standard deep learning frameworks, we implement a custom CUDA kernel so as to launch a single compute kernel in training accelerators. All other parts of the model are matrix multiplications and point-wise operations that can already be efficiently parallelized.\n",
       "\n",
       "FFN with R gate Prior research (Tolstikhin et al., 2021; Liu et al., 2021; Yu et al., 2022) suggests that self-attention may not be as essential in Transformer-based vision tasks as previously thought. Although it provided us with some insights, replacing self-attention entirely in natural language tasks could be too drastic. In our study, we partially dismantle the attention mechanism by replacing the fixed (Q K V) formula with (K V) and introducing a new time-decaying factor (W). This approach enables us to incorporate token and channelmixing components akin to MLP-mixer (Tolstikhin et al., 2021) and a gating unit (R) similar to gMLP (Liu et al., 2021), which enhance the performance of our RWKV model.\n",
       "\n",
       "Small Init Embedding During the initial stage of training a transformer model (Vaswani et al., 2017), we observe that the embedding matrix undergoes slow changes, which pose a challenge for the model to deviate from its initial noisy embedding state. To mitigate this issue, we propose an approach that involves initializing the embedding matrix with small values and subsequently applying an additional LayerNorm operation. By implementing this technique, we accelerate and stabilize the training process, enabling the training of deep architectures with post-LN components. The effectiveness of this approach is demonstrated in Figure 8 , where it is shown to facilitate improved convergence by allowing the model to quickly transition away from the initially small embedding. This is achieved through small changes following a single step, which in turn lead to substantial alterations in directions and subsequently significant changes after the LayerNorm operation.\n",
       "\n",
       "Custom Initialization Building on principles from previous works (He et al., 2016; Jumper et al., 2021), we initialize parameters to values as similar as possible to an identity mapping while breaking symmetry so there is a clean information path. Most weights are initialized to zero. No biases are used for linear layers. Specific formulas are given in Appendix D. We find the choice of initialization to be significant in convergence speed and quality (see Appendix E).\n",
       "\n",
       "\\section{Evaluations}\n",
       "\n",
       "In this section, we focus on evaluating to answer the following questions:\n",
       "\n",
       "- RQ1: Is RWKV competitive against quadratic transformer architectures with equal number of parameters and training tokens?\n",
       "\n",
       "- RQ2: When increasing the number of parameters, does RWKV remain competitive against quadratic transformer architectures?\n",
       "\n",
       "- RQ3: Does increasing parameters of RWKV yield better language modeling loss, when RWKV models are trained for context lengths that most open-sourced quadratic transformers cannot efficiently process?\n",
       "\n",
       "Addressing RQ1 and RQ2, from Fig. 4, we can see that RWKV is very competitive on six benchmarks (Winogrande, PIQA, ARC-C, ARC-E, LAMBADA, and SciQ) against major open source quadratic complexity transformer models: Pythia (Biderman et al., 2023), OPT (Zhang et al., 2022) and BLOOM (Scao et al., 2022). RWKV even outperforms Pythia and GPT-Neo (Black et al., 2022) in four tasks: PIQA, OBQA, ARC-E, and COPA (See details in Appendix H). For RQ3, Fig. 5 shows that increasing context length leads to lower test loss on the Pile, an indication that RWKV can make effective use of long contextual information.\n",
       "\n",
       "\\section{Inference Experiments}\n",
       "\n",
       "We benchmark inference requirements according to size and family. Specifically, we evaluate text generation speed and memory requirements on a typical compute platforms including CPU (x86) and GPU (NVIDIA A100 80GB). For all our experiments we use float 32 precision. We include all model parameters in parameter count, including both embedding and non-embedding layers. Performance under different quantization setups is left to further work. See Appendix I for more results.\n",
       "\n",
       "\n",
       "Figure 6: Cumulative time during text generation for different LLMs.\n",
       "\n",
       "Additionally, we carried out comparative studies on RWKV-4 and ChatGPT / GPT-4, see Appendix J. They revealed that RWKV-4 is very sensitive to prompt engineering. When the prompts were adjusted from the ones used for GPT to more suitable for RWKV, the F1-measure performance increased even from (44.2 %) to (74.8 %).\n",
       "\n",
       "\\section{Future Work}\n",
       "\n",
       "There are several promising directions for future work on the RWKV architecture:\n",
       "\n",
       "- Increasing model expressivity with enhanced time-decay formulations and exploring initial model states while maintaining efficiency. \n",
       "\n",
       "\n",
       "(a) Winogrande\n",
       "\n",
       "\n",
       "(d) ARC-Easy\n",
       "\n",
       "\n",
       "(b) PIQA\n",
       "\n",
       "\n",
       "(e) LAMBADA\n",
       "\n",
       "\n",
       "(c) ARC-Challenge\n",
       "\n",
       "\n",
       "(f) SciQ\n",
       "\n",
       "Figure 4: Zero-Shot Performance: The horizontal axis is a number of parameters and the vertical axis is accuracy.\n",
       "\n",
       "\n",
       "Figure 5: Increasing context length contributes to lower test loss on the Pile (Gao et al., 2020).\n",
       "\n",
       "- Further improving RWKV computational efficiency by applying parallel scan in the (w k v_{t}) step to reduce the computational cost to (O(B \\log (T) d)).\n",
       "\n",
       "- Investigating the application of RWKV to encoder-decoder architectures and potential replacement of cross-attention mechanism. This could have applicability seq 2 seq or multimodal settings, enhancing efficiency both in training and inference.\n",
       "\n",
       "- Leveraging RWKV's state (or context) for interpretability, predictability in sequence data and safety. Manipulating the hidden state could also guide behavior and allow greater customizability through prompt tuning.\n",
       "\n",
       "- Exploring fine-tuned models in specific settings for enhanced interaction with humans (Ouyang et al., 2022). Particularly interest- ing would be the performance under different datasets and specific use cases.\n",
       "\n",
       "- Adapting parameter-efficient fine-tuning methods such as LoRA (Hu et al., 2022) and characterizing behavior under different quantization schemes for the proposed architecture\n",
       "\n",
       "\\section{Conclusions}\n",
       "\n",
       "We introduced RWKV, a new approach to RNN models exploiting the potential of time-based mixing components. RWKV introduces several key strategies which allow it to capture locality and long-range dependencies, while addressing limitations of current architectures by: (1) replacing the quadratic (QK) attention by a scalar formulation with linear cost, (2) reformulating recurrence and sequential inductive biases to unlock efficient training parallelization and efficient inference, and (3) enhancing training dynamics using custom initializations.\n",
       "\n",
       "We benchmark the proposed architecture in a wide variety of NLP tasks and show comparable performance to SoTA with reduced cost. Further experiments on expressivity, interpretability, and scaling showcase the model capabilities and draw parallels in behavior between RWKV and other LLMs.\n",
       "\n",
       "RWKV opens a new door to scalable and efficient architectures to model complex relation- ships in sequential data. While many alternatives to Transformers have been proposed with similar claims, ours is the first to back up those claims with pretrained models with tens of billions of parameters.\n",
       "\n",
       "\\section{Limitations}\n",
       "\n",
       "While our proposed RWKV model has demonstrated promising results regarding training and memory efficiency during inference, some limitations should be acknowledged and addressed in future work. First, the linear attention of RWKV leads to significant efficiency gains but still, it may also limit the model's performance on tasks that require recalling minutiae information over very long contexts. This is due to the funneling of information through a single vector representation over many time steps, compared with the full information maintained by the quadratic attention of standard Transformers. In other words, the model's recurrent architecture inherently limits its ability to \"look back\" at previous tokens, as opposed to traditional self-attention mechanisms. While learned time decay helps prevent the loss of information, it is mechanistically limited compared to full selfattention.\n",
       "\n",
       "Another limitation of this work is the increased importance of prompt engineering in comparison to standard Transformer models. The linear attention mechanism used in RWKV limits the information from the prompt that will be carried over to the model's continuation. As a result, carefully designed prompts may be even more crucial for the model to perform well on tasks.\n",
       "\n",
       "\\section{Acknowledgements}\n",
       "\n",
       "We acknowledge EleutherAI and StabilityAI for compute access and technical support in development of RWKV. We also acknowledge the members of the RWKV Discord server for their help and work on further extending the applicability of RWKV to different domains. Finally, we thank Stella Biderman for feedback on the paper.\n",
       "\n",
       "\\section{References}\n",
       "\n",
       "Alon Albalak, Yi-Lin Tuan, Pegah Jandaghi, Connor Pryor, Luke Yoffe, Deepak Ramachandran, Lise Getoor, Jay Pujara, and William Yang Wang. 2022 FETA: A benchmark for few-sample task transfer in open-domain dialogue. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 10936-10953, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\n",
       "\n",
       "Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. 2016. Layer normalization.\n",
       "\n",
       "Shaojie Bai, J. Zico Kolter, and Vladlen Koltun. 2018. An empirical evaluation of generic convolutional and recurrent networks for sequence modeling.\n",
       "\n",
       "Francesco Barbieri, Jose Camacho-Collados, Luis Espinosa Anke, and Leonardo Neves. 2020. TweetEval: Unified benchmark and comparative evaluation for tweet classification. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1644-1650, Online. Association for Computational Linguistics.\n",
       "\n",
       "Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The long-document transformer. (\\operatorname{arXiv}: 2004.05150).\n",
       "\n",
       "Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. 2023. Pythia: A suite for analyzing large language models across training and scaling. arXiv preprint (\\operatorname{arXiv} 22304.01373).\n",
       "\n",
       "Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. 2020. Piqa: Reasoning about physical commonsense in natural language. In Thirty-Fourth AAAI Conference on Artificial Intelligence.\n",
       "\n",
       "Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. 2022. Gpt-neo: Large scale autoregressive language modeling with mesh-tensorflow, 2021. URL: https://doi. org/l0.5281/zenodo, 5297715.\n",
       "\n",
       "James Bradbury, Stephen Merity, Caiming Xiong, and Richard Socher. 2017. Quasi-recurrent neural networks. In ICLR.\n",
       "\n",
       "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901.\n",
       "\n",
       "Aydar Bulatov, Yuri Kuratov, and Mikhail S. Burtsev. 2023. Scaling transformer to (1 m) tokens and beyond with rmt.\n",
       "\n",
       "Aydar Bulatov, Yury Kuratov, and Mikhail Burtsev. 2022. Recurrent memory transformer. Advances in Neural Information Processing Systems, 35:1107911091. A. P. Sarath Chandar, Chinnadhurai Sankar, Eugene Vorontsov, Samira Ebrahimi Kahou, and Yoshua Bengio. 2019. Towards non-saturating recurrent units for modelling long-term dependencies. In AAAI Conference on Artificial Intelligence.\n",
       "\n",
       "Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, and Adrian Weller. 2020. Rethinking attention with performers.\n",
       "\n",
       "Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. 2014. Empirical evaluation of gated recurrent neural networks on sequence modeling. In NIPS 2014 Deep Learning and Representation Learning Workshop.\n",
       "\n",
       "Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai 2 reasoning challenge. In arXiv:1803.05457.\n",
       "\n",
       "Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. In arXiv, volume (abs / 2110.14168).\n",
       "\n",
       "Tri Dao, Daniel Y Fu, Stefano Ermon, Atri Rudra, and Christopher Re. 2022a. Flashattention: Fast and memory-efficient exact attention with IO-awareness In Advances in Neural Information Processing Systems.\n",
       "\n",
       "Tri Dao, Daniel Y Fu, Khaled K Saab, Armin W Thomas, Atri Rudra, and Christopher Ré. 2022b. Hungry hungry hippos: Towards language modeling with state space models. arXiv preprint (\\operatorname{arXiv}: 2212.14052).\n",
       "\n",
       "Dorottya Demszky, Dana Movshovitz-Attias, Jeongwoo Ko, Alan S. Cowen, Gaurav Nemade, and Sujith Ravi. 2020. Goemotions: A dataset of finegrained emotions. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 4040-4054. Association for Computational Linguistics.\n",
       "\n",
       "Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. 2020. The pile: An (800 gb) dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027.\n",
       "\n",
       "Albert Gu, Karan Goel, and Christopher Ré. 2022. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR).\n",
       "\n",
       "Albert Gu, Çaglar Gülçehre, Tom Le Paine, Matthew W. Hoffman, and Razvan Pascanu. 2019. Improving the gating mechanism of recurrent neural networks. ArXiv, abs/1910.09890. Mandy Guo, Joshua Ainslie, David C Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, and Yinfei Yang. 2022. Longt5: Efficient text-to-text transformer for long sequences. In Findings of the Association for Computational Linguistics: NAACL 2022, pages 724-736.\n",
       "\n",
       "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Identity mappings in deep residual networks.\n",
       "\n",
       "Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask language understanding. In International Conference on Learning Representations.\n",
       "\n",
       "Sepp Hochreiter. 1998. The vanishing gradient problem during learning recurrent neural nets and problem solutions. International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, (6(02): 107-116).\n",
       "\n",
       "Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural Computation, (9(8): 1735-1780).\n",
       "\n",
       "Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. 2022. Training compute-optimal large language models.\n",
       "\n",
       "Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations.\n",
       "\n",
       "Hassan Ismail Fawaz, Germain Forestier, Jonathan Weber, Lhassane Idoumghar, and Pierre-Alain Muller. 2019. Deep learning for time series classification: a review. Data mining and knowledge discovery, 33(4):917-963.\n",
       "\n",
       "Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Joao Carreira. 2021. Perceiver: General perception with iterative attention. In International conference on machine learning, pages 4651-4664. PMLR.\n",
       "\n",
       "Hanhwi Jang, Joonsung Kim, Jae-Eon Jo, Jaewon Lee, and Jangwoo Kim. 2019. Mnnfast: A fast and scalable system architecture for memory-augmented neural networks. In Proceedings of the 46th International Symposium on Computer Architecture, pages 250-263.\n",
       "\n",
       "Matt Gardner Johannes Welbl Nelson F. Liu. 2017. Crowdsourcing multiple choice science questions. In DOI:10.18653/v1/W17-4413. Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In (A C L).\n",
       "\n",
       "John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, and et al. 2021. Highly accurate protein structure prediction with alphafold. Nature, 596(7873):583-589.\n",
       "\n",
       "Sekitoshi Kanai, Yasuhiro Fujiwara, and Sotetsu Iwamura. 2017. Preventing gradient explosions in gated recurrent units. In NIPS.\n",
       "\n",
       "Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361.\n",
       "\n",
       "Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. 2020. Transformers are rnns: Fast autoregressive transformers with linear attention. In International Conference on Machine Learning, pages 5156-5165. PMLR.\n",
       "\n",
       "Nikita Kitaev, L. Kaiser, and Anselm Levskaya. 2020. Reformer: The efficient transformer. ArXiv, (abs / 2001.04451).\n",
       "\n",
       "Jan Kocoń, Igor Cichecki, Oliwier Kaszyca, Mateusz Kochanek, Dominika Szydło, Joanna Baran, Julita Bielaniewicz, Marcin Gruza, Arkadiusz Janz, Kamil Kanclerz, Anna Kocoń, Bartłomiej Koptyra, Wiktoria Mieleszczenko-Kowszewicz, Piotr Miłkowski, Marcin Oleksy, Maciej Piasecki, Łukasz Radliński, Konrad Wojtasik, Stanisław Woźniak, and Przemysław Kazienko. 2023. Chatgpt: Jack of all trades, master of none.\n",
       "\n",
       "Jan Kocoń, Piotr Miłkowski, and Monika ZaśkoZielińska. 2019. Multi-level sentiment analysis of polemo 2.0: Extended corpus of multi-domain consumer reviews. In Proceedings of the (23 rd) Conference on Computational Natural Language Learning (CoNLL), pages 980-991.\n",
       "\n",
       "Phong Le and Willem Zuidema. 2016. Quantifying the vanishing gradient and long distance dependency problem in recursive neural networks and recursive lstms. In Proceedings of the 1st Workshop on Representation Learning for NLP, pages 87-93.\n",
       "\n",
       "Tao Lei, Yu Zhang, Sida I. Wang, Hui Dai, and Yoav Artzi. 2018. Simple recurrent units for highly parallelizable recurrence. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4470-4481, Brussels, Belgium. Association for Computational Linguistics.\n",
       "\n",
       "Hanxiao Liu, Zihang Dai, David R. So, and Quoc V. Le. 2021. Pay attention to mlps. Xuezhe Ma, Xiang Kong, Sinong Wang, Chunting Zhou, Jonathan May, Hao Ma, and Luke Zettlemoyer. 2021. Luna: Linear unified nested attention. Advances in Neural Information Processing Systems, (34: 2441-2453).\n",
       "\n",
       "Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer. 2023. Mega: Moving average equipped gated attention. In ICLR.\n",
       "\n",
       "Eric Martin and Chris Cundy. 2017. Parallelizing linear recurrent neural nets over sequence length. ArXiv, abs/1709.04057.\n",
       "\n",
       "Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. 2022. Locating and editing factual associations in GPT. Advances in Neural Information Processing Systems, 36.\n",
       "\n",
       "Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018. Can a suit of armor conduct electricity? a new dataset for open book question answering. In (E M N L P).\n",
       "\n",
       "John Miller and Moritz Hardt. 2018. Stable recurrent models. arXiv: Learning.\n",
       "\n",
       "Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and James Allen. 2016. A corpus and cloze evaluation for deeper understanding of commonsense stories. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 839-849.\n",
       "\n",
       "OpenAI. 2022. Introducing chatgpt. https://openai . com/blog/chatgpt.\n",
       "\n",
       "OpenAI. 2023. Gpt-4 technical report.\n",
       "\n",
       "Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. 2023. Resurrecting recurrent neural networks for long sequences. arXiv preprint (\\operatorname{arXiv}: 2303.06349).\n",
       "\n",
       "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow in structions with human feedback.\n",
       "\n",
       "Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Ngoc Quan Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernandez. 2016. The LAMBADA dataset: Word prediction requiring a broad discourse context. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1525-1534, Berlin, Germany. Association for Computational Linguistics. Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. 2012. On the difficulty of training recurrent neural networks. In International Conference on Machine Learning.\n",
       "\n",
       "Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. 2019. Pytorch: An imperative style, high-performance deep learning library.\n",
       "\n",
       "Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher Ré. 2023. Hyena hierarchy: Towards larger convolutional language models. arXiv preprint arXiv:2302.10866.\n",
       "\n",
       "Ofir Press, Noah A. Smith, and Mike Lewis. 2022. Train short, test long: Attention with linear biases enables input length extrapolation. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022.\n",
       "\n",
       "Ilan Price, Jordan Gifford-Moore, Jory Flemming, Saul Musker, Maayan Roichman, Guillaume Sylvain, Nithum Thain, Lucas Dixon, and Jeffrey Sorensen. 2020. Six attributes of unhealthy conversations. In Proceedings of the Fourth Workshop on Online Abuse and Harms, pages 114-124, Online. Association for Computational Linguistics.\n",
       "\n",
       "Markus N. Rabe and Charles Staats. 2022. Selfattention does not need (o\\left(n^{2}\\right)) memory.\n",
       "\n",
       "Melissa Roemmele, Cosmin Adrian Bejan, , and Andrew S. Gordon. 2018. Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In (A A A I).\n",
       "\n",
       "Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. 2022. Bloom: A 176bparameter open-access multilingual language model. arXiv preprint arXiv:2211.05100.\n",
       "\n",
       "Ramsha Siddiqui. 2019. SARCASMANIA: Sarcasm Exposed! http://www.kaggle.com/rmsharks4/ sarcasmania-dataset. [Online; accessed 02February-2023].\n",
       "\n",
       "David R. So, Wojciech Manke, Hanxiao Liu, Zihang Dai, Noam Shazeer, and Quoc V. Le. 2021. Primer: Searching for efficient transformers for language modeling. CoRR, abs/2109.08668.\n",
       "\n",
       "Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, and Che Zheng. 2020. Synthesizer: Rethinking self-attention in transformer models.\n",
       "\n",
       "Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. 2022. Efficient transformers: A survey. ACM Computing Surveys, 55(6):1-28. Ilya O. Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, and Alexey Dosovitskiy. 2021. Mlp-mixer: An all-mlp architecture for vision. CoRR, abs/2105.01601.\n",
       "\n",
       "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023 Llama: Open and efficient foundation language models.\n",
       "\n",
       "Aäron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew W. Senior, and Koray Kavukcuoglu. 2016. Wavenet: A generative model for raw audio. ArXiv, abs/1609.03499.\n",
       "\n",
       "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.\n",
       "\n",
       "David Vilares and Carlos Gómez-Rodríguez. 2019. Head-qa: A healthcare dataset for complex reasoning. In (A C L).\n",
       "\n",
       "Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2019. Superglue: A stickier benchmark for general-purpose language understanding systems. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.\n",
       "\n",
       "Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353-355, Brussels, Belgium Association for Computational Linguistics.\n",
       "\n",
       "Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. 2020. Linformer: Self-attention with linear complexity.\n",
       "\n",
       "Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. 2020. A comprehensive survey on graph neural networks. IEEE transactions on neural networks and learning systems, 32(1):4-24.\n",
       "\n",
       "Ellery Wulczyn, Nithum Thain, and Lucas Dixon. 2017. Ex machina: Personal attacks seen at scale. In Proceedings of the 26th International Conference on World Wide Web, WWW 2017, Perth, Australia, April 3-7, 2017, pages 1391-1399. ACM. Weihao Yu, Mi Luo, Pan Zhou, Chenyang Si, Yichen Zhou, Xinchao Wang, Jiashi Feng, and Shuicheng Yan. 2022. Metaformer is actually what you need for vision.\n",
       "\n",
       "Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. 2020. Big bird: Transformers for longer sequences. Advances in Neural Information Processing Systems, 33.\n",
       "\n",
       "Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. Hellaswag: Can a machine really finish your sentence? In (A C L).\n",
       "\n",
       "Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2020. Winogrande: An adversarial winograd schema challenge at scale. In (A C L).\n",
       "\n",
       "Shuangfei Zhai, Walter Talbott, Nitish Srivastava, Chen Huang, Hanlin Goh, Ruixiang Zhang, and Josh Susskind. 2021. An attention free transformer.\n",
       "\n",
       "Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme. 2018 Record: Bridging the gap between human and machine commonsense reading comprehension. In arXiv: 1810.12885\n",
       "\n",
       "Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068.\n",
       "\n",
       "\\section{A Author Contributions}\n",
       "\n",
       "Bo Peng Original RWKV idea, original code, performance optimizations, original experiments, and trained RWKV models from (0.1 B) to (14 B).\n",
       "\n",
       "Eric Alcaide Manuscript (initial draft sections 1, 2; sections 4, 7 and 8; revision and proofreading; final version ). Figures ((2,3,4,7)). Experiments section 6. Appendices D, I. Contributions to Appendix K.\n",
       "\n",
       "Quentin Anthony Led writing the paper. Manuscript (initial draft sections 1, 2, 3; revision and proofreading; final version).\n",
       "\n",
       "Zhenyuan Zhang Manuscript (revision and proofreading) Figure 3. Experiments Appendix G. Contributions to Appendices B and K.\n",
       "\n",
       "Kranthi Kiran GV Manuscript (sections 2 and 5; contributions to section 3; revision and proofreading). Tables 3 and 4. Appendix C. Xiangru Tang Manuscript (sections 2 and 3; contributions to abstract; revision and proofreading). Contributions to Appendix K.\n",
       "\n",
       "Matteo Grella Manuscript (sections 4.5, 4.6, 8; contributions to sections 1,7 and 9 ; proofreading and revision). Contributions to Appendix B.\n",
       "\n",
       "Ferdinand Mom Manuscript (contributions to section 1, 2, 4.3, 4.6; proofreading and revision). Contributions to Appendix B.\n",
       "\n",
       "Atsushi Saito Manuscript (sections 3 and 5; contributions to section 2). Figures 1a, 1b, 1c. Contributions to Appendix (H)\n",
       "\n",
       "Krishna Sri Ipsit Mantri Figure 4\n",
       "\n",
       "Rui-Jie Zhu Tables 1 and 5. Experiments for table 5\n",
       "\n",
       "Peng Zhou Contributions to Table 5 .\n",
       "\n",
       "Qihang Zhao Manuscript (proofreading and revision). Contributions to Table 5.\n",
       "\n",
       "Xuzheng He Manuscript (contributions to section 3; proofreading and revision). Contributions to Figures 1, 7. Appendix G. Contributions to appendix (F).\n",
       "\n",
       "Hayden Lau Manuscript (contributions to section 1; proofreading and revision). Contributions to Appendix K.\n",
       "\n",
       "Michael Chung Manuscript (contributions to section 4.6; proofreading and revision).\n",
       "\n",
       "Haowen Hou Figure 8. Appendix E\n",
       "\n",
       "Jiaming Kong Manuscript (revision and proofreading). Appendix (F).\n",
       "\n",
       "Johan S. Wind RWKV performance optimizations (CUDA), Contributions to Appendix C.\n",
       "\n",
       "Jian Zhu Manuscript (section 2; proofreading and revision). Figures 3 and 5 .\n",
       "\n",
       "Huanqi Cao Manuscript (contributions to 4.2 and 4.3; proofreading and revision). Experiments for Appendix G.\n",
       "\n",
       "Samuel Arcadinho Contributions to Figures 6, 10, and 11. Contributions to Appendix I.\n",
       "\n",
       "Xin Cheng Manuscript (proofreading and revision). Contributions to Appendix K, H. Alon Albalak Manuscript (abstract and sections 1,(9 ;) proofreading and revision).\n",
       "\n",
       "Jan Kocon Manuscript (sections 1; proofreading and revision). Contributions to Appendix J.\n",
       "\n",
       "Przemysław Kazienko Manuscript (section 6; proofreading and revision). Contributions Appendix J.\n",
       "\n",
       "Ruichong Zhang Manuscript (proofreading and revision); Contributions to Figure 5 and Appendix (K).\n",
       "\n",
       "Stanisław Woźniak Appendix J.\n",
       "\n",
       "Bartlomiej Koptyra Contributions to Appendix J.\n",
       "\n",
       "\\section{B Time-Mixing Block as an RNN Cell}\n",
       "\n",
       "As stated in 4.3, the RWKV time-mixing block can be formulated as an RNN, as the (W K V) computation can be written in such a recursive form:\n",
       "\n",
       "\\[\n",
       "\\begin{aligned}\n",
       "a_{0}, b_{0} & =0, \\\\\n",
       "w k v_{t} & =\\frac{a_{t-1}+e^{u+k_{t}} v_{t}}{b_{t-1}+e^{u+k_{t}}}, \\\\\n",
       "a_{t} & =e^{-w} a_{t-1}+e^{k_{t}} v_{t}, \\\\\n",
       "b_{t} & =e^{-w} b_{t-1}+e^{k_{t}} .\n",
       "\\end{aligned}\n",
       "\\]\n",
       "\n",
       "The dataflow of the RNN-like time-mixing is shown in Fig. 7, where the hidden states (h) is the numerator-denominator tuple ((a, b)).\n",
       "\n",
       "\n",
       "Figure 7: RWKV time-mixing block formulated as an RNN cell. Color codes: yellow ((\\mu)) denotes the token shift, red (1) denotes the denominator, blue (2) denotes the numerator, pink (3) denotes the fraction computations in 14. (h) denotes the numerator-denominator tuple ((a, b)).\n",
       "\n",
       "To avoid overflow in calculating (e^{k_{t}}), a numerical trick is used in the official implementation. Note that\n",
       "\n",
       "\\[\n",
       "\\begin{aligned}\n",
       "a_{1} & =e^{-w} a_{0}+e^{k_{0}} v_{0}=e^{k_{0}} v_{0}, \\\\\n",
       "b_{1} & =e^{-w} b_{0}+e^{k_{0}}=e^{k_{0}},\n",
       "\\end{aligned}\n",
       "\\]\n",
       "\n",
       "and we set (a_{1}^{\\prime}=v_{0}, b_{1}^{\\prime}=1, p_{0}=k_{0}), where (p_{t-1}) stores the shared exponents of (a_{t}) and (b_{t}). Now the above recursion can be converted into a numerical safe version, for each time step (t>1) :\n",
       "\n",
       "\\[\n",
       "\\begin{aligned}\n",
       "q & :=\\max \\left(p_{t-1}, u+k_{t}\\right), \\\\\n",
       "a_{t}^{*} & =e^{p_{t-1}-q} a_{t-1}^{\\prime}+e^{u+k_{t}-q} v_{t}, \\\\\n",
       "b_{t}^{*} & =e^{p_{t-1}-q} b_{t-1}^{\\prime}+e^{u+k_{t}-q}, \\\\\n",
       "w k v_{t} & =\\frac{a_{t}^{*}}{b_{t}^{*}} .\n",
       "\\end{aligned}\n",
       "\\]\n",
       "\n",
       "The update to (a_{t}^{\\prime}, b_{t}^{\\prime}) and their shared exponent are also carried out in similar fashion:\n",
       "\n",
       "\\[\n",
       "\\begin{aligned}\n",
       "q & :=\\max \\left(p_{t-1}-w, k_{t}\\right), \\\\\n",
       "a_{t}^{\\prime} & =e^{p_{t-1}-w-q} a_{t-1}^{\\prime}+e^{k_{t}-q} v_{t}, \\\\\n",
       "b_{t}^{\\prime} & =e^{p_{t-1}-w-q} b_{t-1}^{\\prime}+e^{k_{t-q}}, \\\\\n",
       "p_{t} & =q .\n",
       "\\end{aligned}\n",
       "\\]\n",
       "\n",
       "\\section{Parameter and FLOP Count for the RWKV Models}\n",
       "\n",
       "The following section provides an overview of the different RWKV model architectures along with their respective parameter and FLOP counts in Table 2 .\n",
       "\n",
       "\\begin{tabular}{ccccc}\n",
       "\\hline Name & Layers & Model Dimension & Parameters & FLOPs per token \\\\\n",
       "\\hline (169 M) & 12 & 768 & (1.693 \\times 10^{8}) & (2.613 \\times 10^{8}) \\\\\n",
       "(430 M) & 24 & 1024 & (4.304 \\times 10^{8}) & (7.573 \\times 10^{8}) \\\\\n",
       "(1.5 B) & 24 & 2048 & (1.515 \\times 10^{9}) & (2.823 \\times 10^{9}) \\\\\n",
       "3B & 32 & 2560 & (2.985 \\times 10^{9}) & (5.710 \\times 10^{9}) \\\\\n",
       "7B & 32 & 4096 & (7.393 \\times 10^{9}) & (1.437 \\times 10^{10}) \\\\\n",
       "(14 B) & 40 & 5120 & (1.415 \\times 10^{10}) & (2.778 \\times 10^{10}) \\\\\n",
       "\\hline\n",
       "\\end{tabular}\n",
       "\n",
       "Table 2: RWKV model architectures and associated FLOP counts\n",
       "\n",
       "The number of parameters for each model is computed using the formula: \\#parameters (=) (2 V D+13 D^{2} L+D(11 L+4)) where (V=50277) is the vocabulary size, (D) represents the Model Dimension and (L) corresponds to the number of layers.\n",
       "\n",
       "FLOPs is for a forward pass for one token. It was calculated as (6\\left(V D+13 D^{2} L\\right)), which is the twice (add and multiply) the number of parameters in linear layers. The backwards pass FLOPs can be approximated as twice that of the forward pass. So the total is (6\\left(V D+13 D^{2} L\\right)) per token for training ( (3 x) fw FLOPs). It is noteworthy that FLOPs are independent of the context length, unlike regular transformers. The FLOP approximations in this paper are in line with the methodology used by Kaplan et al. (2020).\n",
       "\n",
       "Alternative approximations for FLOPs include doubling the parameters which yields similar results within (2 %) for (14 B) and a (30 %) discrepancy for (169 M) variant. Another approximation is based on the number of non-embedding parameters multiplied by 2 . This gives (2\\left(V D+13 D^{2} L+D(11 L+\\right.) 4)) resulting in (1.6 %) more FLOPs for (14 B) model and (8 %) more FLOPs for (169 M) model.\n",
       "\n",
       "\\section{Parameter initializations}\n",
       "\n",
       "We describe the specific parameter initializations below and motivate the design choices. Parameters belonging to residual blocks are often adjusted by layer depth and total number of layers. Let \\# denote the vocabulary size, (s) denote the embedding dimension, (d) denote the hidden size (we use (d=4 s), L) the number of layers, (l) the layer index (from 0 to L-1), we use the following initializations:\n",
       "\n",
       "- Embeddings are initialized to (U ( \\pm 1 e-4)) as explained in 4.7\n",
       "\n",
       "- For the channel-mixing blocks (11), (\\mu_{k_{i}}) and (\\mu_{r_{i}}) are initialized to (\\left(\\frac{i}{s}\\right)^{1-\\frac{l}{L}})\n",
       "\n",
       "- For the time-mixing blocks (16), initializations are (\\mu_{k_{i}}=\\left(\\frac{i}{s}\\right)^{1-\\frac{l}{L}}, \\mu_{v_{i}}=\\left(\\frac{i}{s}\\right)^{1-\\frac{l}{L}}+\\frac{0.3 l}{L-1}) and (\\mu_{r_{i}}=0.5\\left(\\frac{i}{s}\\right)^{1-\\frac{l}{L}})\n",
       "\n",
       "- (w_{i}) (14), also known as \"time decay\", is initialized to (-5+8 \\cdot\\left(\\frac{i}{d-1}\\right)^{0.7+\\frac{1.3 l}{L-1}}). Intuitively, it is the discount factor applied to previous tokens over time.\n",
       "\n",
       "- (u_{i}) (14), also known as \"bonus\", is set to (0.5(((i+1) \\bmod 3)-1)+\\log 0.3). It is the special weighting applied to the current token in equation 14. The alternating zigzag pattern initially creates subtle variations in the tensor elements, which are intended to help the model treat different dimensions of the embedding distinctively.\n",
       "\n",
       "- (W_{o}) (15) (time-mixing) and (W_{v}) (channelmixing) are initialized to (N \\left(0, \\sqrt{\\frac{d}{s}}=2\\right))\n",
       "\n",
       "- All (W_{r}, W_{k}, W_{v}) weights are initialized to 0 so the model can start learning from the beginning without noisy signals. - All LayerNorm weights start from 1 and biases from 0.\n",
       "\n",
       "\\section{E Small Init Embedding}\n",
       "\n",
       "This section presents experimental validation of small initialization embedding. The experimental setup is as follows. In the baseline configuration, the parameters are initialized using a normal distribution with a mean of 0.0 and a standard deviation of 0.02 , which is a commonly used initialization method in models like BERT and GPT. On the other hand, in the small initialization of the embedding (small init emb) experiment, the parameters are initialized using a uniform distribution with a range of 1e-4, which is slightly different from RWKV where a normal distribution with a standard deviation of 1e-4 is used. However, this difference is negligible and does not affect our conclusions. The experiments were conducted with a batch size of 400 . As depicted in the figure 8 , the loss curve for the small init emb exhibits a faster rate of decrease and convergence compared to the traditional initialization using a normal distribution.\n",
       "\n",
       "\n",
       "Figure 8: Effect of small initialization embedding.\n",
       "\n",
       "\\section{F Gradient Stability in RWKV}\n",
       "\n",
       "In this section, we present a mathematical description of the gradient stability property in RWKV, focusing specifically on the time-mixing block. By gradient stability we mean that if the inputs (x_{t}) are bounded and the model parameters are fixed, then the gradients with respect to (W_{k}) and (W_{v}) are uniformly bounded for all (T) (thus not exploding). Consequently, we can control the amount each (x_{t}) contributes to the gradient at (T) in a naturally decaying fashion by the weight decay mechanism (w) (thus not vanishing unless desired).\n",
       "\n",
       "First, we make the simplification that there are no token shifts, this will not affect the final conclusion. In this scenario, (w k v_{T}) can be written as\n",
       "\n",
       "\\[\n",
       "w k v_{T}=\\frac{\\sum_{t=1}^{T} K_{t}^{e} v_{t}}{\\sum_{t=1}^{T} K_{t}^{e}}= E \\left(v_{t}\\right)=\\frac{ S \\left(v_{t}\\right)}{ S (1)}\n",
       "\\]\n",
       "\n",
       "where\n",
       "\n",
       "\\[\n",
       "\\begin{gathered}\n",
       "v_{t}=W_{v} x_{t}, \\quad \\frac{\\partial\\left(v_{t}\\right)_{i}}{\\partial\\left(W_{v}\\right)_{i, j}}=\\left(x_{t}\\right)_{j}, \\\\\n",
       "K_{t}^{e}=e^{W_{k} x_{t}+w_{T, t}}, \\quad \\frac{\\partial\\left(K_{t}^{e}\\right)_{i}}{\\partial\\left(W_{k}\\right)_{i, j}}=\\left(x_{t}\\right)_{j}\\left(K_{t}^{e}\\right)_{i},\n",
       "\\end{gathered}\n",
       "\\]\n",
       "\n",
       "and (S (\\cdot)) and (E (\\cdot)) are shorthand for denoting sums and averages over weights (K_{t}^{e}).\n",
       "\n",
       "The loss function at position (T) can be written as\n",
       "\n",
       "\\[\n",
       "L_{T}=l\\left(f\\left(w k v_{T}\\right), y_{T}\\right)\n",
       "\\]\n",
       "\n",
       "Because (w k v_{T}) relates to (\\left(W_{k}\\right)_{i, j}) and (\\left(W_{v}\\right)_{i, j}) only through the (i)-th channel (\\left(w k v_{T}\\right)_{i}), we have\n",
       "\n",
       "\\[\n",
       "\\frac{\\partial L_{T}}{\\partial\\left(W_{v}\\right)_{i, j}}=\\frac{\\partial L_{T}}{\\partial\\left(w k v_{T}\\right)_{i}} \\frac{\\partial\\left(w k v_{T}\\right)_{i}}{\\partial\\left(W_{v}\\right)_{i, j}}\n",
       "\\]\n",
       "\n",
       "The first part of above equation contains trivial operations like output layers, and other layers of time-mixing, which can be proven inductively. The second part of above equation can be bounded as\n",
       "\n",
       "\\[\n",
       "\\begin{aligned}\n",
       "\\left|\\frac{\\partial\\left(w k v_{T}\\right)_{i}}{\\partial\\left(W_{v}\\right)_{i, j}}\\right| & =\\left|\\frac{\\partial E _{i}\\left[\\left(v_{t}\\right)_{i}\\right]}{\\partial\\left(W_{v}\\right)_{i, j}}\\right| \\\\\n",
       "& =\\left| E _{i}\\left[\\left(x_{t}\\right)_{j}\\right]\\right| \\leq \\max _{t}\\left|\\left(x_{t}\\right)_{j}\\right|,\n",
       "\\end{aligned}\n",
       "\\]\n",
       "\n",
       "which is irrelevant to (T). Similarly,\n",
       "\n",
       "\\[\n",
       "\\begin{aligned}\n",
       "\\frac{\\partial\\left(w k v_{T}\\right)_{i}}{\\partial\\left(W_{k}\\right)_{i, j}} & =\\partial \\frac{ S _{i}\\left[\\left(v_{t}\\right)_{i}\\right]}{ S _{i}(1)} / \\partial\\left(W_{k}\\right)_{i, j} \\\\\n",
       "& =\\frac{ S _{i}\\left[\\left(x_{t}\\right)_{j}\\left(v_{t}\\right)_{i}\\right]}{ S _{i}(1)}-\\frac{ S _{i}\\left[\\left(x_{t}\\right)_{j}\\right] S _{i}\\left[\\left(v_{t}\\right)_{i}\\right]}{ S _{i}(1)^{2}} \\\\\n",
       "& = E _{i}\\left[\\left(x_{t}\\right)_{j}\\left(v_{t}\\right)_{i}\\right]- E _{i}\\left[\\left(x_{t}\\right)_{j}\\right] E _{i}\\left[\\left(v_{t}\\right)_{i}\\right] \\\\\n",
       "& =\\operatorname{cov}_{i}\\left(\\left(x_{t}\\right)_{j},\\left(v_{t}\\right)_{i}\\right)\n",
       "\\end{aligned}\n",
       "\\]\n",
       "\n",
       "can also be bounded. Note that (w k v) 's softmax operation contains at least two non-zero terms ( (u) and (w) ), so the above \"covariance\" will not degenerate into 0 .\n",
       "\n",
       "Figure 9: Model behavior visualizations of the RWKV model.\n",
       "\n",
       "\\section{G Model Behavior Visualization}\n",
       "\n",
       "In Figure 9, we present visualizations of some behavior of the RWKV model.\n",
       "\n",
       "The top plot illustrates the time decays (\\left(e^{-w}\\right)) in each layer of the RWKV-169M model, sorted along the channel axis. Notably, several decays in the last layers are very close or equal to one, implying that certain information is preserved and propagated throughout the model's temporal context. Meanwhile, many decays in the initial layer are close to zero, which corresponds to local operations in (w k v) (14), likely to be associated with tasks such as text parsing or lexical analysis. (Note that the local operations in (w k v) is due to the extra parameter (u), when (e^{-w}) is degenerated into 0 .) These patterns of time decays are partly learned, but also come from parameter initialization as it speeds up training.\n",
       "\n",
       "The bottom plot shows the information retrieval and propagation path in the RWKV-430M model. The experiment follows the causal trace method introduced by Meng et al. (2022), where we\n",
       "\n",
       "1. Run the model once, and record all states and activation of each layer during the computation;\n",
       "\n",
       "2. Corrupt the input embeddings of the subject using noise (\"The Eiffel Tower\" in this example); 3. Restore the states and activation of a certain layer at a certain token during the computation, and record the log-probability of the model outputting the correct answer (\"Paris\").\n",
       "\n",
       "Unlike transformers, RWKV relies on recursive propagation of information in the time dimension. In this case, the fact that \"the Eiffel Tower is located in Paris\" is retrieved in layer 4. It is then passed down to the subsequent layers. In layer 20, mostly, the information is propagated through time until reaching where it is needed. Finally, it is passed down to the last layer for outputting the answer.\n",
       "\n",
       "\\section{H Evaluation Details}\n",
       "\n",
       "The results for following tasks are in Table 3 and 4 .\n",
       "\n",
       "Tasks:\n",
       "\n",
       "- LAMBADA (Paperno et al., 2016). A benchmark dataset that evaluates the model's contextual reasoning and language comprehension abilities by presenting context-target pairs, where the objective is to predict the most probable target token.\n",
       "\n",
       "- PIQA (Bisk et al., 2020). A benchmark for the task of physical common sense reasoning, which consists of a binary choice task that can be better understood as a set of two pairs, namely (Goal, Solution).\n",
       "\n",
       "- HellaSwag (Zellers et al., 2019) A novel benchmark for commonsense Natural Language Inference (NLI) which is build by adversarial filtering against transformer models.\n",
       "\n",
       "- Winogrande (Zellers et al., 2020) A dataset designed to evaluate the acquisition of common sense reasoning by neural language models, aiming to determine whether we are accurately assessing the true capabilities of machine common sense.\n",
       "\n",
       "- StoryCloze (Mostafazadeh et al., 2016) A benchmark to present a novel approach to assess comprehension of narratives, narrative generation, and script acquisition, focusing on commonsense reasoning.\n",
       "\n",
       "- ARC Challenge (Clark et al., 2018) A dataset designed for multiple-choice question answering, encompassing science exam questions ranging from third grade to ninth grade.\n",
       "\n",
       "- ARC Easy An easy subset of ARC.\n",
       "\n",
       "- HeadQA (Vilares and Gómez-Rodríguez, 2019) A benchmark consisting of graduatelevel questions encompassing various fields such as medicine, nursing, biology, chemistry, psychology, and pharmacology.\n",
       "\n",
       "- OpenBookQA (Mihaylov et al., 2018) A QA dataset to evaluate human comprehension of a subject by incorporating open book facts, scientific knowledge, and perceptual common sense, drawing inspiration from open book exams.\n",
       "\n",
       "- SciQ (Johannes Welbl Nelson F. Liu, 2017) A multiple-choice QA dataset which was created using an innovative approach to gather well-crafted multiple-choice questions that are focused on a specific domain.\n",
       "\n",
       "- TriviaQA (Joshi et al., 2017) A QA-IR dataset which is constituted of triples of questions, answers, supporting evidence, and independently collected evidence documents, with an average of six documents per question for reliable sources.\n",
       "\n",
       "- ReCoRD (Zhang et al., 2018) A benchmark for evaluating commonsense reasoning in reading comprehension by generating queries from CNN/Daily Mail news articles and requiring text span answers from corresponding summarizing passages.\n",
       "\n",
       "- COPA (Roemmele et al., 2018) A dataset to evaluate achievement in open-domain commonsense causal reasoning.\n",
       "\n",
       "- MMMLU (Hendrycks et al., 2021) A multitask dataset for 57 tasks containing elementary mathematics, US history, computer science, law, etc.\n",
       "\n",
       "\\section{Inference results}\n",
       "\n",
       "Figures 10 and 11 illustrate, respectively, the results on time (s) and memory (RAM, VRAM) requirements for LLM inference in float32 precision. We benchmark the following model families and sizes:\n",
       "\n",
       "- RWKV: 169m, 430m, 1.4b, 3b, 7b, 14b\n",
       "\n",
       "- Bloom (Scao et al., 2022): 560m, 1b, 3b\n",
       "\n",
       "- OPT (Zhang et al., 2022): 125m, 350m, 1.3b, (2.7 b, 6.7 b, 13 b)\n",
       "\n",
       "- GPT-Neo (Black et al., 2022): 125m, 1.3b, (2.7 b)\n",
       "\n",
       "- Pythia (Biderman et al., 2023): 160m, 410m, (1.4 b, 2.8 b, 6.7 b, 12 b)\n",
       "\n",
       "Missing models in are due to Out Of Memory (OOM) errors. A comparison at 512 tokens is shown in Figure 11 as some large transformer models produced an OOM when inferencing longer se- \n",
       "\n",
       "\\begin{tabular}{|c|c|c|c|c|c|c|c|c|}\n",
       "\\hline Model & (\\begin{array}{l}\\text { Params } \\\\\n",
       "\\text { B }\\end{array}) & (\\begin{array}{l}\\text { PIQA } \\\\\n",
       "\\text { acc }\\end{array}) & (\\begin{array}{l}\\text { StoryCloze } \\\\\n",
       "\\text { acc }\\end{array}) & (\\begin{array}{l}\\text { HellaSwag } \\\\\n",
       "\\text { acc_norm }\\end{array}) & (\\begin{array}{l}\\text { WinoGrande } \\\\\n",
       "\\text { acc }\\end{array}) & (\\begin{array}{l}\\text { ARC-e } \\\\\n",
       "\\text { acc }\\end{array}) & (\\begin{array}{l}\\text { ARC-c } \\\\\n",
       "\\text { acc_norm }\\end{array}) & (\\begin{array}{l}\\text { OBQA } \\\\\n",
       "\\text { acc_norm }\\end{array}) \\\\\n",
       "\\hline RWKV-4 & 0.17 & 65.07 & 58.79 & 32.26 & 50.83 & 47.47 & 24.15 & 29.60 \\\\\n",
       "\\hline Pythia & 0.16 & 62.68 & 58.47 & 31.63 & 52.01 & 45.12 & 23.81 & 29.20 \\\\\n",
       "\\hline GPT-Neo & 0.16 & 63.06 & 58.26 & 30.42 & 50.43 & 43.73 & 23.12 & 26.20 \\\\\n",
       "\\hline RWKV-4 & 0.43 & 67.52 & 63.87 & 40.90 & 51.14 & 52.86 & 25.17 & 32.40 \\\\\n",
       "\\hline Pythia & 0.40 & 66.70 & 62.64 & 39.10 & 53.35 & 50.38 & 25.77 & 30.00 \\\\\n",
       "\\hline GPT-Neo & 0.40 & 65.07 & 61.04 & 37.64 & 51.14 & 48.91 & 25.34 & 30.60 \\\\\n",
       "\\hline RWKV-4 & 1.5 & 72.36 & 68.73 & 52.48 & 54.62 & 60.48 & 29.44 & 34.00 \\\\\n",
       "\\hline Pythia & 1.4 & 71.11 & 67.66 & 50.82 & 56.51 & 57.74 & 28.58 & 30.80 \\\\\n",
       "\\hline GPT-Neo & 1.4 & 71.16 & 67.72 & 48.94 & 54.93 & 56.19 & 25.85 & 33.60 \\\\\n",
       "\\hline RWKV-4 & 3.0 & 74.16 & (7 0 . 7 1) & 59.89 & 59.59 & 65.19 & 33.11 & 37.00 \\\\\n",
       "\\hline Pythia & 2.8 & 73.83 & 70.71 & 59.46 & 61.25 & 62.84 & 32.25 & 35.20 \\\\\n",
       "\\hline GPT-Neo & 2.8 & 72.14 & 69.54 & 55.82 & 57.62 & 61.07 & 30.20 & 33.20 \\\\\n",
       "\\hline RWKV-4 & 7.4 & 76.06 & 73.44 & 65.51 & 61.01 & 67.80 & 37.46 & 40.20 \\\\\n",
       "\\hline Pythia & 6.9 & 74.54 & 72.96 & 63.92 & 61.01 & 66.79 & 35.07 & 38.00 \\\\\n",
       "\\hline GPT-J & 6.1 & 75.41 & 74.02 & 66.25 & 64.09 & 66.92 & 36.60 & 38.20 \\\\\n",
       "\\hline RWKV-4 & 14.2 & (7 7 . 4 8) & 76.06 & 70.65 & 63.85 & 70.24 & 38.99 & 41.80 \\\\\n",
       "\\hline GPT-level (^{*}) & 14.2 & 76.49 & 74.97 & 68.72 & 65.14 & 70.77 & 37.99 & 39.27 \\\\\n",
       "\\hline Pythia (c.f.) & 11.8 & 75.90 & 74.40 & 67.38 & 64.72 & 69.82 & 36.77 & 38.80 \\\\\n",
       "\\hline GPT-NeoX (c.f.) & 20.6 & 77.69 & 76.11 & 71.42 & 65.98 & 72.69 & 40.44 & 40.20 \\\\\n",
       "\\hline\n",
       "\\end{tabular}\n",
       "\n",
       "Table 3: Zero-Shot Performance of the model on Common Sense Reasoning Tasks. ({ }^{*}) Interpolation of Pythia and GPT-Neo models\n",
       "\n",
       "\\begin{tabular}{lllllllll}\n",
       "\\hline Model & (\\begin{array}{l}\\text { Params } \\\\\n",
       "\\text { B }\\end{array}) & (\\begin{array}{l}\\text { LAMBADA } \\\\\n",
       "\\text { ppl }\\end{array}) & (\\begin{array}{l}\\text { LAMBADA } \\\\\n",
       "\\text { acc }\\end{array}) & (\\begin{array}{l}\\text { headQA } \\\\\n",
       "\\text { acc_norm }\\end{array}) & (\\begin{array}{l}\\text { sciq } \\\\\n",
       "\\text { acc }\\end{array}) & (\\begin{array}{l}\\text { triviaQA } \\\\\n",
       "\\text { acc }\\end{array}) & (\\begin{array}{l}\\text { ReCoRD } \\\\\n",
       "\\text { em }\\end{array}) & (\\begin{array}{l}\\text { COPA } \\\\\n",
       "\\text { acc }\\end{array}) \\\\\n",
       "\\hline RWKV-4 & 0.17 & (2 9 . 3 3) & 32.99 & 25.78 & (7 7 . 5 0) & 1.26 & 62.03 & (6 6 . 0 0) \\\\\n",
       "Pythia & 0.16 & 24.38 & (3 8 . 9 7) & (2 5 . 8 2) & 76.50 & (1 . 3 1) & (6 6 . 3 2) & 62.00 \\\\\n",
       "GPT-Neo & 0.16 & 30.27 & 37.36 & 25.16 & 76.60 & 1.18 & 64.92 & 64.00 \\\\\n",
       "\\hline RWKV-4 & 0.43 & 13.04 & 45.16 & (2 7 . 3 2) & 80.30 & (2 . 3 5) & 70.48 & 65.00 \\\\\n",
       "Pythia & 0.40 & (1 1 . 5 8) & (5 0 . 4 4) & 25.09 & (8 1 . 5 0) & 2.03 & (7 5 . 0 5) & (6 7 . 0 0) \\\\\n",
       "GPT-Neo & 0.40 & 13.88 & 47.29 & 26.00 & 81.10 & 1.38 & 73.79 & 65.00 \\\\\n",
       "\\hline RWKV-4 & 1.5 & 7.04 & 56.43 & (2 7 . 6 4) & 85.00 & (5 . 6 5) & 76.97 & (7 7 . 0 0) \\\\\n",
       "Pythia & 1.4 & (6 . 5 8) & (6 0 . 4 3) & 27.02 & (8 5 . 5 0) & 5.52 & (8 1 . 4 3) & 73.00 \\\\\n",
       "GPT-Neo & 1.4 & 7.5 & 57.25 & 27.86 & 86.00 & 5.24 & 80.62 & 69.00 \\\\\n",
       "\\hline RWKV-4 & 3.0 & 5.25 & 63.96 & 28.45 & 86.50 & (1 1 . 6 8) & 80.87 & (8 2 . 0 0) \\\\\n",
       "Pythia & 2.8 & (4 . 9 3) & (6 5 . 3 6) & (2 8 . 9 6) & (8 7 . 7 0) & 9.63 & 85.10 & 77.00 \\\\\n",
       "GPT-Neo & 2.8 & 5.63 & 62.22 & 27.17 & 89.30 & 4.82 & (8 3 . 8 0) & 80.00 \\\\\n",
       "\\hline RWKV-4 & 7.4 & 4.38 & 67.18 & (3 1 . 2 2) & 88.80 & (1 8 . 3 0) & 83.68 & (8 5 . 0 0) \\\\\n",
       "Pythia & 6.9 & 4.3 & (6 7 . 9 8) & 28.59 & 90.00 & 15.42 & 86.44 & (8 5 . 0 0) \\\\\n",
       "GPT-J & 6.1 & (4 . 1) & 68.31 & 28.67 & (9 1 . 5 0) & 16.74 & (8 7 . 7 1) & 83.00 \\\\\n",
       "\\hline RWKV-4 & 14.2 & (3 . 8 6) & 70.83 & (3 2 . 6 4) & 90.40 & (2 4 . 5 8) & 85.67 & (8 5 . 0 0) \\\\\n",
       "GPT-level & 14.2 & 3.81 & (7 0 . 9 4) & 31.03 & (9 2 . 2 0) & 22.37 & 87.89 & 82.66 \\\\\n",
       "\\hline Pythia (c.f.) & 11.8 & 3.89 & 70.44 & 30.74 & 91.80 & 20.57 & 87.58 \\\\\n",
       "GPT-NeoX (c.f.) & 20.6 & 3.64 & 71.94 & 31.62 & 93.00 & 25.99 & 88.52 & 82.00 \\\\\n",
       "\\hline\n",
       "\\end{tabular}\n",
       "\n",
       "Table 4: Zero-Shot Performance of various models on different tasks. * Interpolation of Pythia and GPT-Neo models \n",
       "\n",
       "\\begin{tabular}{llllllll}\n",
       "\\hline Method & (L) & (d) & (T) & Train bpc & Test bpc & Time Complexity & Space Complexity \\\\\n",
       "\\hline Transformer & 12 & 512 & 1024 & 0.977 & 1.137 & (O\\left(T^{2} d\\right)) & (O\\left(T^{2}+T d\\right)) \\\\\n",
       "Transformer & 24 & 256 & 1024 & 1.039 & 1.130 & (O\\left(T^{2} d\\right)) & (O\\left(T^{2}+T d\\right)) \\\\\n",
       "\\hline Reformer & 12 & 512 & 1024 & 1.040 & 1.195 & (O(T \\log T d)) & (O(T \\log T+T d)) \\\\\n",
       "Synthesizer & 12 & 512 & 1024 & 0.994 & 1.298 & (O\\left(T^{2} d\\right)) & (O\\left(T^{2}+T d\\right)) \\\\\n",
       "Linear Transformer & 12 & 512 & 1024 & 0.981 & 1.207 & (O\\left(T d^{2}\\right)) & (O\\left(T d+d^{2}\\right)) \\\\\n",
       "Performer & 12 & 512 & 1024 & 1.002 & 1.199 & (O\\left(T d^{2} \\log d\\right)) & (O\\left(T d \\log d+d^{2} \\log d\\right)) \\\\\n",
       "AFT-simple & 12 & 512 & 1024 & 0.854 & 1.180 & (O(T d)) & (O(T d)) \\\\\n",
       "\\hline RWKV-RNN & 6 & 512 & 1024 & 0.720 & - & (O( T d )) & (O( d )) \\\\\n",
       "\\hline\n",
       "\\end{tabular}\n",
       "\n",
       "Table 5: Enwik8 results, measured in bits per character (bpc): the lower the better. Baseline comparisons are made with Reformer (Kitaev et al., 2020), Synthesizer (Tay et al., 2020) (the best performing dense version), Linear Transformer (Katharopoulos et al., 2020), Performer (Choromanski et al., 2020). (L, d), and (T) denote the number of blocks (network depth), dimension of features, and sequence length, respectively. Both Linear Transformer and Performer are implemented with customized CUDA kernels (github.com/idiap/fast-transformers), and all other models are implemented in native Pytorch.\n",
       "\n",
       "quences. For GPU experiments, we use an NVIDIA A100 with 80GB of VRAM. For CPU experiments, we use an AMD EPYC processor with (30 CPU) cores and (200 GiB) RAM.\n",
       "\n",
       "Figure 10: Text generation inference memory (CPU RAM, GPU VRAM) for LLMs. Model parameters are not accounted.\n",
       "\n",
       "Figure 11: Text generation inference time for LLMs. \n",
       "\n",
       "\\begin{tabular}{llrrrrr}\n",
       "\\hline Task Name & (\\begin{array}{l}\\text { Measure } \\\\\n",
       "\\text { type }\\end{array}) & (\\begin{array}{r}\\text { ChatGPT } \\\\\n",
       "\\text { [%] }\\end{array}) & (\\begin{array}{r}\\text { GPT-4 } \\\\\n",
       "\\text { [%] }\\end{array}) & (\\begin{array}{r}\\text { RWKV-4 } \\\\\n",
       "\\text { GPT [%] }\\end{array}) & (\\begin{array}{r}\\text { RWKV-4 } \\\\\n",
       "\\text { changed [%] }\\end{array}) & (\\begin{array}{r}\\text { SOTA } \\\\\n",
       "{[%]}\\end{array}) \\\\\n",
       "\\hline RTE & F1 Macro & 88.1 & (9 1 . 3) & 44.2 & 74.8 & 92.1 \\\\\n",
       "WNLI & Accuracy & 81.7 & (9 1 . 6) & 47.9 & 49.3 & 97.9 \\\\\n",
       "GoEmotions & F1 Macro & (2 5 . 6) & 23.1 & 7.9 & 7.9 & 52.8 \\\\\n",
       "PolEmo2 & F1 Macro & (4 4 . 1) & 41.0 & 38.2 & 40.9 & 76.4 \\\\\n",
       "\\hline\n",
       "\\end{tabular}\n",
       "\n",
       "Table 6: ChatGPT, GPT-4 and RWKV-4-Raven-14B reasoning performance comparison in RTE (Wang et al., 2019), WNLI (Wang et al., 2018), GoEmotions (Demszky et al., 2020), and PolEmo2 (Kocoń et al., 2019) benchmarks. SOTA is provided as a supplementary reference.\n",
       "\n",
       "\\section{J Importance of prompt construction and comparison to GPT models}\n",
       "\n",
       "Inspired by article (Kocoń et al., 2023), we compared the zero-shot performance of the RWKV4-Raven-14B with ChatGPT (access in February 2023) and GPT-4 using several known NLP tasks, i.e., recognizing textual entailment (RTE), Winograd Natural Language Inference (WNLI), and recognizing emotions elicited in readers (GoEmotions and PolEmo2). Each model got the same prompts manually chosen to receive proper responses from the ChatGPT model. As shown in Tab. 6, RWKV performs significantly worse than ChatGPT and GPT-4 in specific task performance. We suspect that this disparity is likely caused by the choice of prompts used to generate the answers. Given that prompts are in natural language and do not consider that RWKV is an RNN, so it can not look back inside an instruction.\n",
       "\n",
       "When the instruction style was adapted to respect that RNNs is not capable for retrospective processing, quality on some datasets increased significantly (ex. for RTE (Wang et al., 2019) F1 Macro increased from (44.2 %) to (74.8 %) ). We hypothesize that RWKV models are more sensitive to the position of the components in the context, as RNN-based architectures cannot look back and readjust the weight of previous information. For better performance, desired information should be after the question. Example of ChatGPT prompt to RTE:\n",
       "\n",
       "Having premise <here is a premise (>) judge if the following hypothesis <here is a hypothesis (>) are logically connected with the premise? Answer \"entailment\" if yes, or \"not_entailment\" if no.\n",
       "\n",
       "RWKV prompt taking into account the characteristics of the RNN:\n",
       "\n",
       "Can you tell me if the hypothesis is entailment or is not entailment to the premise?\n",
       "\n",
       "\\begin{tabular}{llrrr}\n",
       "\\hline Task Name & (\\begin{array}{l}\\text { Measure } \\\\\n",
       "\\text { type }\\end{array}) & (\\begin{array}{r}\\text { ChatGPT } \\\\\n",
       "{[%]}\\end{array}) & (\\begin{array}{r}\\text { RWKV-4 } \\\\\n",
       "\\text { adapted [%] }\\end{array}) & (\\begin{array}{r}\\text { SOTA } \\\\\n",
       "{[%]}\\end{array}) \\\\\n",
       "\\hline Aggression & F1 Macro & (6 9 . 1 0) & 56.66 & 74.45 \\\\\n",
       "MathQA & Accuracy & 71.40 & (8 0 . 6 9) & 83.20 \\\\\n",
       "Sarcasm & F1 Macro & 49.88 & (5 0 . 9 6) & 53.57 \\\\\n",
       "TweetSent & F1 Macro & (6 3 . 3 2) & 52.50 & 72.07 \\\\\n",
       "Unhealthy & F1 Macro & (4 5 . 2 1) & 43.30 & 50.96 \\\\\n",
       "\\hline\n",
       "\\end{tabular}\n",
       "\n",
       "Table 7: ChatGPT and RWKV-4-Raven-14B performance comparison in Aggresion (Wulczyn et al., 2017), Sarcasm (Siddiqui, 2019), Unhealthy (Price et al., 2020), MathQA (Cobbe et al., 2021), and TweetSent (Barbieri et al., 2020) benchmarks. SOTA is provided as a supplementary reference.\n",
       "\n",
       "premise: <here is a premise (>) hypothesis: <here is a hypothesis>\n",
       "\n",
       "While separating the instruction from the input is relatively easy to do, other aspects of prompt engineering are harder to quantify. Testing the approach of stating the input after the question on multiple other tasks, shown in tab. 7, suggests that better prompts might reduce the disparity between models. Raven achieves comparable result to ChatGPT on unhealthy conversation detection and even surpasses it on the sarcasm detection dataset. While this approach in prompting looks necessary, it alone is not enough to replace the capability of having free access to the whole context. Therefore, prompt engineering seems to be of significantly more importance to the RNN models compared to standard transformers. It is entirely possible that good prompts to (RNN) models do not mean additional restrictions, but should simply be constructed using completely different guidelines. While authors of a forementioned paper (Kocoń et al., 2023) perform chain-of-thought to improve results on MathQA dataset, trying the same approach on the Raven model achieved a very low accuracy of (48.11 %). Despite that, using a very similar prompt without the chain-of-thought outperformed ChatGPT on this dataset. It would not be surprising if, thanks to the hidden state, RNN models do not need additional steps during inference, and adding those only makes the model forget the information it analyzed and deemed important. This further emphasizes the importance of the order of information the model receives.\n",
       "\n",
       "The template used to prompt the Raven model in MathQA with chain-of-thought prompting:\n",
       "\n",
       "Write the reasoning and highlight the answer to the question at the end in format: 'Answer: '. The question is: <here is question> The template used to prompt the Raven model in MathQA without chain-of-thought prompting:\n",
       "\n",
       "Write the answer to the math question in the format:\n",
       "\n",
       "'Answer: '.\n",
       "\n",
       "The question is: <here is question> \n",
       "\n",
       "\\section{K Cases}\n",
       "\n",
       "In this part, we present a few instances of outputs produced by RWKV model using a Chat interface ({ }^{56}).\n",
       "\n",
       "\\section{RWKV-4-Raven-14B}\n",
       "\n",
       "\\section{Tell me about ravens.}\n",
       "\n",
       "Ravens are large, black birds with a distinctive white head and neck. They are found in most parts of the world, including North America, Europe, Asia, and Australia. Ravens are known for their intelligence and problem-solving abilities. They are also considered to be symbols of death and transformation in many cultures. Ravens are often associated with the afterlife or death because they have been known to eat carrion or even other birds. In some cultures, ravens are also believed to be messengers of the gods or guardians of treasure.\n",
       "\n",
       "\\section{RWKV-4-Raven-14B}\n",
       "\n",
       "\\section{Explain the concept of machine learning to a 10-year-old.}\n",
       "\n",
       "Machine learning is like a super-smart computer that can learn on its own. It can recognize patterns and make decisions based on what it has learned. Just like how you might remember something from the past, the computer can remember what it's learned and use it to make predictions or decisions in the future. It's like having a smart friend who always knows what to do!\n",
       "\n",
       "\\section{RWKV-4-Raven-14B}\n",
       "\n",
       "\\section{Write a short poem describing nature's beauty in springtime.}\n",
       "\n",
       "Nature's beauty in springtime is rare, A symphony of colors, a gentle surprise. The flowers bloom, the birds sing and sing, As the sun shines down, warming our skin.\n",
       "\n",
       "The trees sway in the gentle breeze, As we take in their beauty with ease. The sky is blue, the clouds are white, A sight to behold, it's truly divine.\n",
       "\n",
       "({ }^{5} https : / /) github.com/BlinkDL/ChatRWKV\n",
       "\n",
       "({ }^{6}) https://huggingface.co/spaces/BlinkDL/ChatRWKV-gradio RWKV-4-Raven-14B\n",
       "\n",
       "Below is the numerical stable formula for updating the state variable in RWKV:\n",
       "\n",
       "\\[\n",
       "\\begin{aligned}\n",
       "a_{t}^{*} & =e^{p-q} a_{t-1}+e^{u+k_{t}-q} v_{t} \\\\\n",
       "b_{t}^{*} & =e^{p-q} b_{t-1}+e^{u+k_{t}-q} \\\\\n",
       "w k v_{t} & =\\frac{a_{t}^{*}}{b_{t}^{*}}\n",
       "\\end{aligned}\n",
       "\\]\n",
       "\n",
       "Please write out the corresponding pytorch code to do so, where both (a_{t}) and (b_{t}) are torch.Tensor (([n]))\n",
       "\n",
       "Here is the PyTorch code to update the state variable in RWKV using the given formula:\n",
       "\n",
       "\n",
       "This code defines a function update_state that takes in the state variables a and (b), the control variables (u) and (v), and the parameters wkv and torch.randn. It then uses the PyTorch torch.exp function to compute the exponential of the control variables and the log of the normalization factor. Finally, it uses these values to update the state.\n",
       "\n",
       "\\section{RWKV-4-Raven-14B}\n",
       "\n",
       "\\section{Explain the following metaphor: Life is like cats.}\n",
       "\n",
       "The metaphor \"Life is like cats\" suggests that life can be unpredictable and full of unexpected twists and turns, just like a cat's behavior. It implies that we cannot fully control or predict the outcome of our lives, just as we cannot control the movements of a cat. The metaphor also implies that life is complex and multifaceted, with many different variables and factors at play. Just as a cat can have multiple personalities or moods, life can have many twists and turns that are difficult to predict or understand. Overall, the metaphor suggests that life is complex and unpredictable, much like a cat's behavior. RWKV-4-Raven-14B\n",
       "\n",
       "Test the convergence of (\\sum_{i=0}^{\\infty} 2^{-i}).\n",
       "\n",
       "The given series is a geometric series with the first term (a_{0}=2) and common ratio (r=\\frac{1}{2}). We can find the sum of this series using the formula for a geometric series:\n",
       "\n",
       "\\[\n",
       "\\sum_{i=0}^{\\infty} a_{i}=\\frac{a_{0}}{1-r}=\\frac{\\frac{1}{2}}{1-\\frac{1}{2}}=\\frac{1}{1-\\frac{1}{2}}=\\frac{2}{1}=2\n",
       "\\]\n",
       "\n",
       "So, the sum of the given series is 2 .\n",
       "\n",
       "\\section{RWKV-4-Raven-14B}\n",
       "\n",
       "I would like to cook some Chinese food at home. Do you have any suggestions on the meal I could choose?\n",
       "\n",
       "Yes, of course! If you are not very experienced in cooking Chinese food, you could start with making noodles or rice, then make a dish using one of those. I also think making stir-fry with some meat is an excellent idea.\n",
       "\n",
       "I want to make something more special. What about red braised pork belly?\n",
       "\n",
       "Yes, red braised pork belly is also very popular and tasty.\n",
       "\n",
       "Tell me the steps to cook.\n",
       "\n",
       "You will need a few ingredients such as pork belly, star anise, sugar, ginger, scallions and wine. Put all of these in a pot, cook on medium heat and simmer until the meat is tender. After that, remove the meat from the pot and reserve the sauce. Slice the meat and add some ginger slices into it, then boil until it's almost done.\n",
       "\n",
       "\\section{What do I need to be careful with the most during the cooking process?}\n",
       "\n",
       "To cook pork belly properly, it is very important to monitor the heat of the stove and be patient with the cooking process. Overcooking it can cause the pork to be tough, dry and difficult to chew. You can add a few ingredients like soy sauce or black beans to give the pork more flavor.\n",
       "\n",
       "Tell me more about the proper heat and timing.\n",
       "\n",
       "If the temperature of the water is too high, it can damage the collagen and connective tissues, resulting in tough pork belly. It is essential to control the temperature carefully, especially when braising the pork. Additionally, keep the water simmering but not boiling to maintain a low simmer, as high heat will dry out the pork belly and result in tough and unpalatable meat. Lastly, monitor the heat during cooking and make sure the heat is medium, as this is what is most appropriate for slow cooking pork belly.\n",
       "\n",
       "\\section{I see. Thank you for your advice.}\n",
       "\n",
       "You're welcome! It was a pleasure chatting with you. Let me know if you need any further assistance in the future. RWKV-4-Raven-14B\n",
       "\n",
       "If all apples are fruits and all fruits are sweet, are all apples sweet? Please explain your answer in detail.\n",
       "\n",
       "Yes, all apples are fruits, and all fruits are sweet. Therefore, it is safe to assume that all apples are sweet.\n",
       "\n",
       "If birds can fly. Penguins are birds. Can penguins fly? Please explain your answer in detail.\n",
       "\n",
       "No, penguins cannot fly. Penguins are birds, but they are not capable of flight like other birds. They have wings and can glide for short distances, but they cannot take off or fly long distances like other birds.\n",
       "\n",
       "The day before two days after the day before tomorrow is Saturday. What day is it today? Please explain your answer in detail.\n",
       "\n",
       "Today is Friday. Two days after tomorrow is Sunday. Tomorrow is Saturday.\n",
       "\n",
       "If five cats can catch five mice in five minutes, how long will it take one cat to catch one mouse? Please explain your answer in detail.\n",
       "\n",
       "If five cats can catch five mice in five minutes, it means that each cat can catch one mouse in one minute. To find out how long it will take one cat to catch one mouse, we can use the concept of proportionality.\n",
       "\n",
       "Let's assume that it takes (x) minutes for a cat to catch a mouse. Then, the number of mice caught by a cat in one minute is equal to the number of cats multiplied by the number of mice caught by each cat: (x=5 * 1=5)\n",
       "\n",
       "So, it will take 5 minutes for one cat to catch one mouse.\n",
       "\n",
       "\\section{RWKV-4-Raven-14B}\n",
       "\n",
       "Summarize the following paragraph.\n",
       "\n",
       "<The Abstract of this paper goes here>\n",
       "\n",
       "The paper presents a novel architecture, Receptance Weighted Key Value (RWKV), that combines the efficient parallelizable training of Transformers with the efficient inference of RNNs. The model is able to scale to tens of billions of parameters and exhibits linear computational complexity during training and inference, making it a promising alternative to Transformers for sequence processing tasks.\n",
       "\n",
       "\\section{Please further summarize it in one sentence.}\n",
       "\n",
       "The researchers have developed a new architecture that combines the efficient parallelizable training of Transformers with the efficient inference of RNNs, which allows for linear scaling in memory and computational requirements."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display_markdown\n",
    "display_markdown(doc_text, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0ece3118",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-29T09:52:32.239591Z",
     "start_time": "2023-05-29T09:52:32.223803Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function display_markdown in module IPython.core.display:\n",
      "\n",
      "display_markdown(*objs, **kwargs)\n",
      "    Displays the Markdown representation of an object.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    *objs : object\n",
      "        The Python objects to display, or if raw=True raw markdown data to\n",
      "        display.\n",
      "    raw : bool\n",
      "        Are the data objects raw data or Python objects that need to be\n",
      "        formatted before display? [default: False]\n",
      "    metadata : dict (optional)\n",
      "        Metadata to be associated with the specific mimetype output.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(display_markdown)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
