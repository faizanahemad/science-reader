{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe51487c",
   "metadata": {},
   "source": [
    "We are a research team that are using gpt-3.5 API to do question generation and question answering. We are also performing complex aspect based document summarization. We intend to publish our work if completed in upcoming EMNLP and then launch it to wider userbase. Current user base is a small alpha test user group of 20 people including our own team of 8. We hit rate limits frequently on token count based rate limit on gpt-3.5-turbo while multiple concurrent users are using our system as well as when we run automated quality tests. We have been asking our users to create separate accounts and upload their own API keys to our service for usage but this is not a scalable and appealing method to our users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1a14c0",
   "metadata": {},
   "source": [
    "Toolformer Approach\n",
    "- Take a long context modelling model like (LongT5, RMT, Knn-LM)\n",
    "- Make many tools, start with a large set of tools already.\n",
    "- Use LLMs like gpt4 to generate training data so that you have balanced training data for all tools.\n",
    "- Use generated and human annotated data (gold data) to bootstrap a simple long context model.\n",
    "- Generate bigger corpus of tool usage using the new model and filter using toolformer approach.\n",
    "- Elevate data quality by finding ways to detect ambigous cases and then using gpt4 to check them.\n",
    "- Train model in toolformer style so model already knows existence of tools in it's params\n",
    "- Multiple Tools in same Sequence.\n",
    "- Ability to use a new tool in few shot fashion. Few shot tool use training before making it learn the tools as regular training.\n",
    "- RL training maybe?\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e285bb9a",
   "metadata": {},
   "source": [
    "Planning and Re-planning helps improve LLMs. Planning stage can be done by an expensive LLM but the middle execution stages can be handled by cheaper models.\n",
    "\n",
    "- Better ways of asking\n",
    "- Plan and replan\n",
    "- break into smaller part and write programs for smaller parts.\n",
    "- Better memory and retrieval\n",
    "- Critic\n",
    "- Are the tools provided even enough to answer the request. feasibility.\n",
    "- chatgpt UI uses toolformer approach while we can't use that, unless we generate few tokens at a time.\n",
    "- Incorporate examples in some steps\n",
    "- Can it handle generic NLG tasks by searching examples from internet, search examples, or search prompts, and do better ICL.\n",
    "- Tasks it must handle\n",
    "    - Planning\n",
    "    - Question Answering\n",
    "    - Maths\n",
    "    - Multi-Hop QnA\n",
    "    - Document and PDF QnA\n",
    "    - Summarise a website or website QnA\n",
    "    - Large book QnA\n",
    "    - Long form writing - To generate longer sequences we can use sequential multiple agents, with about 1000 word summary of previous writing as input, and another 1000 words as any additional context.\n",
    "    - Multi-modal with Image-to-text and text-to-image\n",
    "    \n",
    "    \n",
    "- Async Agents\n",
    "    - Agents whose results are asynchronously used and if not available within certain time limit, the results are skipped.\n",
    "    \n",
    "Why not Langchain\n",
    "- Langchain makes agents and tools use only text input and output.\n",
    "- Langchain agents don't plan, they go step by step asking what should be my next step. We suggest planning and re-planning and making a plan before starting.\n",
    "- We believe that asking gpt4 to write code for its plan is a better approach than the output parsing and stitching that langchain does, langchain essentially hides the execution but we don't want that.\n",
    "- Langchain's output parsing and stitching is useful for less powerful agents.\n",
    "- we want our execution chain to look like a DAG while langchain's execution chain looks like a linear path. Although a DAG structure can be converted into a linear path as well, but it may optimal and parallizable to have a DAG path."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89ffc47",
   "metadata": {},
   "source": [
    "I want to build two plugins \n",
    "1. for literature survey: Given a search phrase / link to arxiv paper / research description, find relevant papers using web-search, then use a pdf to text engine and read each paper (in context of our current work), and give out a literature review summary with relevant citations. One key aspect is to be able to do BFS on the paper citation graph with iterative deepening, and spawning gpt4 agents to read each paper, find the relevant context and keep it in summary for using in our overall output text.\n",
    "2. Think before you reply plugin: Given a question, this plugin uses a four step approach to provide a better answer. step-1: plan- \"to answer this question or user request what do we need to keep in our context, or how can we improve our context, can we web search (if yes write down the search terms), or devise a set of multiple steps to follow before answering, if yes, enumerate the steps\" step-2: plan following from step-1, step-3: generate multiple answers with reasoning using context developed in step-1 & step-2 with a critic step that criticises each answer and rewrites them to improve each answer separately, and finally step-4: fuse all generated answers, along with the question in context to get a single good answer.\n",
    "3. For Generic paid API access: Give LM ability to use any paid API by a plugin, first LM can ask what available APIs exist and then on basis of available APIs (and their cost) chatgpt can decide which API to use for generating a better response. A key challenge will be using paid APIs and how user's can authenticate to paid API providers.\n",
    "\n",
    "Other Links\n",
    "- https://portal.azure.com/#home\n",
    "\n",
    "Princliple:\n",
    "- We make simple reusable components, any new component we make, we subclass from langchain Basetool class\n",
    "- WE combine them to get higher order functionality.\n",
    "\n",
    "Important LangChain classes and links\n",
    "We can use Prompt Tempates, Chat Memory, Vector Stores, Agents and Tools. \n",
    "\n",
    "- [OpenAI class](https://python.langchain.com/en/latest/reference/modules/llms.html#langchain.llms.OpenAI)\n",
    "- [VectorStoreMemory](https://python.langchain.com/en/latest/modules/memory/types/vectorstore_retriever_memory.html)\n",
    "- [Custom Memory Class](https://python.langchain.com/en/latest/modules/memory/examples/custom_memory.html)\n",
    "- [Making Custom Tools](https://python.langchain.com/en/latest/modules/agents/tools/custom_tools.html)\n",
    "- [Bing Search](https://python.langchain.com/en/latest/modules/agents/tools/examples/bing_search.html) and its tool class [BingSearchResults](https://python.langchain.com/en/latest/_modules/langchain/tools/bing_search/tool.html#BingSearchResults)\n",
    "- https://python.langchain.com/en/latest/reference/modules/tools.html#langchain.tools.DuckDuckGoSearchResults and https://python.langchain.com/en/latest/reference/modules/utilities.html#langchain.utilities.DuckDuckGoSearchAPIWrapper\n",
    "\n",
    "\n",
    "Agents we would need:\n",
    "\n",
    "- Composer: Compose multile modules as a DAG [Chain](https://python.langchain.com/en/latest/modules/chains/getting_started.html)\n",
    "- Contextual reader: Given a long text, and a context, read the page contextually and gather information relevant to the context.\n",
    "- Vector Search agent:\n",
    "- Text search agent:\n",
    "- Pdf reader agent: \n",
    "- Link follower or crawler agent:\n",
    "    - Get page links\n",
    "    - Recursively apply page reader and link getter.\n",
    "- Fuse information: Given context, and information from two or more agents, fuses the information from the sources into one cohesive information.\n",
    "- Keep History Agent: Keeps simple summary of existing conversation, as well as L1, L2, L3 cache style summary, as well as TOC based, text search based, vector search based search over full conversation.\n",
    "\n",
    "- Planning and Instruction Agent: Remembers the user's instruction, creates and improves plan based on existing information and context, decides what next steps to take.\n",
    "    - Let model know what tools it can use, their descriptions, then ask it to plan.\n",
    "    - Model comes up with a plan - you ask model - lets sya step-x fails or doesn't provide expected results. what can be done instead.\n",
    "    - plan improvement by self-critic\n",
    "    - lets evaluate our plan and progress step by step and see if we can improve it.\n",
    "    - User query Intent detection: based on query intent we will decide what type of text/web filtering is needed, as well as reduce number of calls for other calls. This can be part of planning.\n",
    "    - based on our plan, we want to reduce both input and output size of every subsequent module to keep things fast.\n",
    "\n",
    "- Document reader: Given a long text document, this breaks it into chunks before feeding to the API, generates hierarchical TOC based, text reverse index BM25 based, vector based, and simple summary.\n",
    "- Index Agent: Implements functionality for Document Reader, Keep History etc.\n",
    "- Web Search Agent: Uses gs-api or duckduckgo, with Link follower or crawler agent, Contextual page reader, to search for information from the web.\n",
    "- Prompt Engineer agent: Depending on the specific use case, this agent stores preformed prompts, as well as develops new prompts for better model performance.\n",
    "- Critic Agent: Given a context and some response, or context and some intermediate information. It generates criticsm. \"Given the below user request and gathered context, is it enough to answer the user question throughly and correctly?\" [Constitutional Chain](https://python.langchain.com/en/latest/reference/modules/chains.html#langchain.chains.ConstitutionalChain)\n",
    "- Evaluate Response Agent: Depends on Critic Agent, Given two or more responses and a context, it uses predefined factors or comes up with its own factors and then provides an evaluation of the responses and a final best response from the two, on basis of the evaluation factors.\n",
    "- Text Cleaner Agent: Given text composed of html, and other gibberish, get cleaned text. Caveat, the text might contain code blocks which should not be skipped.\n",
    "- Speech to Text.\n",
    "- Image to Descriptive Text\n",
    "- Get page images\n",
    "- Get page tables\n",
    "\n",
    "User interface agents: This agents try to solve the final user problem and usually have some hand crafting.\n",
    "12. Code writer Agent: Writes code for requested functionality, by using other agents.\n",
    "13. Literature Survey agent:\n",
    "14. Open Domain QnA agent:\n",
    "15. Open ended text generation agent: when remaining agents don't fit.\n",
    "\n",
    "\n",
    "\n",
    "Evaluation\n",
    "- On Gpt4 and Bard related evaluation tools\n",
    "- On other tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afb178a",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e1ccf6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-13T16:30:49.800911Z",
     "start_time": "2023-05-13T16:30:41.449122Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install duckduckgo-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "98e030af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-23T03:54:59.819410Z",
     "start_time": "2023-05-23T03:54:59.760979Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import re\n",
    "from pprint import pprint\n",
    "import time\n",
    "import concurrent.futures\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from copy import deepcopy, copy\n",
    "import requests\n",
    "import tempfile\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "pd.set_option('max_colwidth', 800)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4399dcab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T18:15:20.554583Z",
     "start_time": "2023-05-21T18:15:19.359271Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'What are the two ways the weights of the language model may be learned during training?'}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'What are the two ways the weights of the language model may be learned during training?'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_token = \"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\"\n",
    "import json\n",
    "import requests\n",
    "API_URL = \"https://api-inference.huggingface.co/models/gpt2\"\n",
    "API_URL = \"https://api-inference.huggingface.co/models/OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\"\n",
    "headers = {\"Authorization\": f\"Bearer {hf_token}\", 'Content-Type': 'application/json'}\n",
    "def hf_query(data):\n",
    "    input_text = f\"<|prompter|>{data}<|endoftext|><|assistant|> \"\n",
    "    payload = {\"inputs\": input_text}\n",
    "    data = json.dumps(payload)\n",
    "    response = requests.request(\"POST\", API_URL, headers=headers, data=data)\n",
    "    resp = json.loads(response.content)\n",
    "    return resp[0]['generated_text'].replace(input_text, \"\").strip()\n",
    "\n",
    "def hf_query_flan(data):\n",
    "    API_URL = \"https://api-inference.huggingface.co/models/google/flan-ul2\"\n",
    "    input_text = data\n",
    "    payload = {\"inputs\": input_text}\n",
    "    data = json.dumps(payload)\n",
    "    response = requests.request(\"POST\", API_URL, headers=headers, data=data)\n",
    "    resp = json.loads(response.content)\n",
    "    print(resp)\n",
    "    if isinstance(resp, list) and len(resp) > 0:\n",
    "        return resp[0]['generated_text'].replace(input_text, \"\").strip()\n",
    "    return \"\"\n",
    "\n",
    "data = hf_query_flan(\"\"\"Write three questions that can be answered from the document:\n",
    "\n",
    "The original weights of the language model may be \"frozen\", such that only the new layer of weights connecting them to the output are learned during training. Alternatively, the original weights may receive small updates (possibly with earlier layers frozen).[15] Prompting[edit] See also: Prompt engineering and Few-shot learning (natural language processing) In the prompting paradigm, popularized by GPT-3,[4] the problem to be solved is formulated via a text prompt, which the model must solve by providing a completion (via inference). In \"few-shot prompting\", the prompt includes a small number of examples of similar (problem, solution) pairs.[2] For example, a sentiment analysis task of labelling the sentiment of a movie review could be prompted as follows:[4] Review: This movie stinks.\n",
    "\n",
    "Questions:\n",
    "\n",
    "\"\"\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "732c6372",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T18:15:24.437285Z",
     "start_time": "2023-05-21T18:15:20.559936Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a different task than the one specified in the repository. Be sure to know what you're doing :)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[0.99999993, 0.99085845, 0.35575069, 0.36625064],\n",
       "       [0.99085845, 0.9999999 , 0.3364216 , 0.34527119],\n",
       "       [0.35575069, 0.3364216 , 1.00000009, 0.99093718],\n",
       "       [0.36625064, 0.34527119, 0.99093718, 0.99999992]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from langchain.embeddings import HuggingFaceHubEmbeddings\n",
    "repo_id = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "hf_embed = HuggingFaceHubEmbeddings(\n",
    "    repo_id=repo_id,\n",
    "    task=\"feature-extraction\",\n",
    "    huggingfacehub_api_token=hf_token,\n",
    ")\n",
    "\n",
    "documents = [\"an unrelated embedding which is not connected to the task at hand.\", \n",
    "                                   \"an very unrelated embedding which is not connected to the task at hand.\", \n",
    "                                   \"\"\"The original weights of the language model may be \"frozen\", such that only the new layer of weights connecting them to the output are learned during training. Alternatively, the original weights may receive small updates (possibly with earlier layers frozen).[15] Prompting[edit] See also: Prompt engineering and Few-shot learning (natural language processing) In the prompting paradigm, popularized by GPT-3,[4] the problem to be solved is formulated via a text prompt, which the model must solve by providing a completion (via inference). In \"few-shot prompting\", the prompt includes a small number of examples of similar (problem, solution) pairs.[2] For example, a sentiment analysis task of labelling the sentiment of a movie review could be prompted as follows:[4] Review: This movie stinks.\"\"\", \n",
    "                                   \"\"\"The most original weights of the language model may be \"frozen\", such that only the new layer of weights connecting them to the output are learned during training. Alternatively, the original weights may receive small updates (possibly with earlier layers frozen).[15] Prompting[edit] See also: Prompt engineering and Few-shot learning (natural language processing) In the prompting paradigm, popularized by GPT-3,[4] the problem to be solved is formulated via a text prompt, which the model must solve by providing a completion (via inference). In \"few-shot prompting\", the prompt includes a small number of examples of similar (problem, solution) pairs.[2] For example, a sentiment analysis task of labelling the sentiment of a movie review could be prompted\"\"\"]\n",
    "\n",
    "embeds = hf_embed.embed_documents(documents)\n",
    "type(embeds)\n",
    "np.matmul(np.array(embeds), np.array(embeds).T)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7afa88d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T18:15:25.895255Z",
     "start_time": "2023-05-21T18:15:24.442702Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.98588491, 0.75195768, 0.75051701],\n",
       "       [0.98588491, 1.        , 0.75759153, 0.75773449],\n",
       "       [0.75195768, 0.75759153, 1.        , 0.98660812],\n",
       "       [0.75051701, 0.75773449, 0.98660812, 1.        ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "openai_embed = OpenAIEmbeddings(openai_api_key= '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', model='text-embedding-ada-002')\n",
    "\n",
    "embeds = openai_embed.embed_documents(documents)\n",
    "type(embeds)\n",
    "np.matmul(np.array(embeds), np.array(embeds).T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "bc382645",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-23T03:55:06.938079Z",
     "start_time": "2023-05-23T03:55:03.574752Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1. What is the original weights of the language model?', 'The original weights of the language model may be \"frozen\", such that only the new layer of weights connecting them to the output are learned during training.', '2. How is the problem to be solved formulated via a text prompt?', 'In the prompting paradigm, popularized by GPT-3, the problem to be solved is formulated via a text prompt, which the model must solve by providing a completion (via inference).']\n"
     ]
    }
   ],
   "source": [
    "import ai21\n",
    "ai21.api_key = \"4U9o26ovfFLLFS7q10So3euEQkuCPJqv\"\n",
    "def call_ai21(text, num_tokens=4000, temperature=0.7):\n",
    "    response_grande = ai21.Completion.execute(\n",
    "          model=\"j2-jumbo-instruct\",\n",
    "          prompt=text,\n",
    "          numResults=1,\n",
    "          maxTokens=num_tokens,\n",
    "          temperature=temperature,\n",
    "          topKReturn=0,\n",
    "          topP=1,\n",
    "          stopSequences=[\"##\"]\n",
    "    )\n",
    "    result = response_grande[\"completions\"][0][\"data\"][\"text\"]\n",
    "    return result\n",
    "\n",
    "answer=call_ai21(\"\"\"Write as many valid and important question-answer pairs as can be answered/derived from the document below:\n",
    "\n",
    "The original weights of the language model may be \"frozen\", such that only the new layer of weights connecting them to the output are learned during training. Alternatively, the original weights may receive small updates (possibly with earlier layers frozen).[15] Prompting[edit] See also: Prompt engineering and Few-shot learning (natural language processing) In the prompting paradigm, popularized by GPT-3,[4] the problem to be solved is formulated via a text prompt, which the model must solve by providing a completion (via inference). In \"few-shot prompting\", the prompt includes a small number of examples of similar (problem, solution) pairs.[2] For example, a sentiment analysis task of labelling the sentiment of a movie review could be prompted as follows:[4] Review: This movie stinks.\n",
    "\n",
    "Questions:\n",
    "\n",
    "\"\"\")\n",
    "print(answer.split(\"\\n\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "793e5c5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T07:30:27.411214Z",
     "start_time": "2023-05-24T07:30:26.444885Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whisper-1\n",
      "babbage\n",
      "gpt-3.5-turbo\n",
      "davinci\n",
      "text-davinci-edit-001\n",
      "text-davinci-003\n",
      "babbage-code-search-code\n",
      "text-similarity-babbage-001\n",
      "code-davinci-edit-001\n",
      "text-davinci-001\n",
      "ada\n",
      "babbage-code-search-text\n",
      "babbage-similarity\n",
      "code-search-babbage-text-001\n",
      "text-curie-001\n",
      "code-search-babbage-code-001\n",
      "text-ada-001\n",
      "text-embedding-ada-002\n",
      "text-similarity-ada-001\n",
      "curie-instruct-beta\n",
      "ada-code-search-code\n",
      "ada-similarity\n",
      "gpt-4-0314\n",
      "code-search-ada-text-001\n",
      "text-search-ada-query-001\n",
      "davinci-search-document\n",
      "ada-code-search-text\n",
      "text-search-ada-doc-001\n",
      "davinci-instruct-beta\n",
      "gpt-4\n",
      "text-similarity-curie-001\n",
      "code-search-ada-code-001\n",
      "ada-search-query\n",
      "text-search-davinci-query-001\n",
      "curie-search-query\n",
      "davinci-search-query\n",
      "babbage-search-document\n",
      "ada-search-document\n",
      "text-search-curie-query-001\n",
      "text-search-babbage-doc-001\n",
      "curie-search-document\n",
      "text-search-curie-doc-001\n",
      "babbage-search-query\n",
      "text-babbage-001\n",
      "text-search-davinci-doc-001\n",
      "text-search-babbage-query-001\n",
      "curie-similarity\n",
      "curie\n",
      "gpt-3.5-turbo-0301\n",
      "text-similarity-davinci-001\n",
      "text-davinci-002\n",
      "davinci-similarity\n",
      "cushman:2020-05-03\n",
      "ada:2020-05-03\n",
      "babbage:2020-05-03\n",
      "curie:2020-05-03\n",
      "davinci:2020-05-03\n",
      "if-davinci-v2\n",
      "if-curie-v2\n",
      "if-davinci:3.0.0\n",
      "davinci-if:3.0.0\n",
      "davinci-instruct-beta:2.0.0\n",
      "text-ada:001\n",
      "text-davinci:001\n",
      "text-curie:001\n",
      "text-babbage:001\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import tiktoken\n",
    "\n",
    "# Set your OpenAI API key\n",
    "openai.api_key = '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'\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = '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'\n",
    "\n",
    "# Get the list of models\n",
    "response = openai.Model.list()\n",
    "\n",
    "# Extract the data from the response\n",
    "models = response['data']\n",
    "\n",
    "# Print each model's ID\n",
    "for model in models:\n",
    "    print(model['id'])\n",
    "    \n",
    "# gpt-4-0314, gpt-4, \n",
    "# text-davinci-003, gpt-3.5-turbo, gpt-3.5-turbo-0301, text-davinci-002, text-davinci-001\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8f7495b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T18:15:28.768592Z",
     "start_time": "2023-05-21T18:15:28.530177Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.agents import Tool\n",
    "from langchain.tools import BaseTool\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.text_splitter import NLTKTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.llms import GPT4All\n",
    "from llama_index.node_parser.simple import SimpleNodeParser\n",
    "from llama_index.langchain_helpers.text_splitter import TokenTextSplitter\n",
    "from llama_index import (\n",
    "    GPTVectorStoreIndex, \n",
    "    LangchainEmbedding, \n",
    "    LLMPredictor, \n",
    "    ServiceContext, \n",
    "    StorageContext, \n",
    "    download_loader,\n",
    "    PromptHelper\n",
    ")\n",
    "from llama_index import SimpleDirectoryReader, LangchainEmbedding, GPTListIndex, GPTSimpleVectorIndex, PromptHelper\n",
    "from llama_index import LLMPredictor, ServiceContext\n",
    "\n",
    "from langchain.utilities import SerpAPIWrapper\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from typing import Optional, Type\n",
    "from langchain.callbacks.manager import AsyncCallbackManagerForToolRun, CallbackManagerForToolRun\n",
    "from langchain.tools import DuckDuckGoSearchRun\n",
    "from langchain.utilities import BingSearchAPIWrapper, DuckDuckGoSearchAPIWrapper\n",
    "from langchain.tools import DuckDuckGoSearchResults\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "# openai = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.7, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85d43d66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T18:15:31.261244Z",
     "start_time": "2023-05-21T18:15:28.770273Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No good DuckDuckGo Search Result was found'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[{'snippet': '⚡️TLS-based <b>ChatGPT</b> API with auto token regeneration, conversation tracking, proxy support and more.',\n",
       "  'title': 'chatgptpy · PyPI',\n",
       "  'link': 'https://pypi.org/project/chatgptpy/'},\n",
       " {'snippet': '<b>ChatGPT</b>: In <b>Python</b>, both lists and tuples are used to store collections of data. However, there are some key differences between the two: Mutability: The main difference between lists and tuples is that lists are mutable (can be changed), while tuples are immutable (cannot be changed). This means that once a tuple is created, you cannot add, remove, or modify any of its elements, whereas you can do all of these things with a list.',\n",
       "  'title': 'ChatGPT: Your Personal Python Coding Mentor – Real Python',\n",
       "  'link': 'https://realpython.com/chatgpt-coding-mentor-python/'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[{'Result': 'No good DuckDuckGo Search Result was found'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"BING_SUBSCRIPTION_KEY\"] = \"4cf7467058dc49b9b98e8147a4ae7bd5\"\n",
    "os.environ[\"BING_SEARCH_URL\"] = \"https://api.bing.microsoft.com/v7.0/search\"\n",
    "\n",
    "\n",
    "search = DuckDuckGoSearchRun()\n",
    "search.run(\"Obama's first name?\")\n",
    "\n",
    "search = BingSearchAPIWrapper(k=1)\n",
    "search.results(\"python chatgpt\", 2)\n",
    "\n",
    "search = DuckDuckGoSearchAPIWrapper(k=1)\n",
    "search.results(\"python chatgpt\", 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "6d2f65f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T09:24:41.195398Z",
     "start_time": "2023-05-24T09:24:41.169613Z"
    }
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import threading\n",
    "from playwright.async_api import async_playwright\n",
    "    \n",
    "class RunThread(threading.Thread):\n",
    "    def __init__(self, func, args, kwargs):\n",
    "        \"\"\"\n",
    "        https://stackoverflow.com/questions/55409641/asyncio-run-cannot-be-called-from-a-running-event-loop-when-using-jupyter-no\n",
    "        \"\"\"\n",
    "        self.func = func\n",
    "        self.args = args\n",
    "        self.kwargs = kwargs\n",
    "        self.result = None\n",
    "        super().__init__()\n",
    "\n",
    "    def run(self):\n",
    "        self.result = asyncio.run(self.func(*self.args, **self.kwargs))\n",
    "\n",
    "def run_async(func, *args, **kwargs):\n",
    "    try:\n",
    "        loop = asyncio.get_running_loop()\n",
    "    except RuntimeError:\n",
    "        loop = None\n",
    "    if loop and loop.is_running():\n",
    "        thread = RunThread(func, args, kwargs)\n",
    "        thread.start()\n",
    "        thread.join()\n",
    "        return thread.result\n",
    "    else:\n",
    "        return asyncio.run(func(*args, **kwargs))\n",
    "    \n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def call_api_parallel(api_calls, fn, max_workers=4):\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit tasks and collect Future objects\n",
    "        futures = [executor.submit(fn, **api_call) for api_call in api_calls]\n",
    "\n",
    "        # Collect results in order of input tasks\n",
    "        results = [future.result() for future in futures]\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db7fa050",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T18:15:32.295222Z",
     "start_time": "2023-05-21T18:15:31.279522Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Rainbow Sockery\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI(temperature=0.1)\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain import OpenAI, ConversationChain\n",
    "\n",
    "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
    "print(llm(text))\n",
    "\n",
    "# tools = load_tools([\"wikipedia\", \"llm-math\"], llm=llm)\n",
    "# agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "# agent.run(\"What is the temperature in Bangalore in degree celcius divided by 2\")\n",
    "\n",
    "# conversation = ConversationChain(llm=llm, verbose=True)\n",
    "# output = conversation.predict(input=\"Hi there!\")\n",
    "# print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "c0e83db4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T08:11:27.976706Z",
     "start_time": "2023-05-24T08:11:21.295268Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'! Could you please provide me with a list of the top five restaurants in San Francisco?\\n\\nSure, here is a list of the top five restaurants in San Francisco according to TripAdvisor: \\n1. Gary Danko\\n2. The French Laundry\\n3. Benu\\n4. Saison\\n5. Quince'"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def round_robin(arr):\n",
    "    while True:\n",
    "        for item in arr:\n",
    "            yield item\n",
    "\n",
    "\n",
    "class CallGpt:\n",
    "    def __init__(self, ):\n",
    "        self.easy_models = round_robin([\n",
    "            \"text-davinci-003\", \"gpt-3.5-turbo\", \"gpt-3.5-turbo-0301\", \"text-davinci-003\", \n",
    "            \"text-davinci-002\", \n",
    "#             \"davinci-instruct-beta:2.0.0\", \n",
    "#             \"text-davinci-001\"\n",
    "                                       ])\n",
    "        self.completion_models = [\n",
    "            \"text-davinci-003\",\"text-davinci-002\", \"davinci-instruct-beta:2.0.0\", \"text-davinci-001\"\n",
    "        ]\n",
    "        self.turbo_models = round_robin([\"gpt-3.5-turbo\", \"gpt-3.5-turbo-0301\"])\n",
    "        self.hard_models = round_robin([\"gpt-4\", \"gpt-4-0314\"])\n",
    "\n",
    "        self.system = \"You are a helpful assistant. Please respond to the user request while following their instructions.\"\n",
    "        import tiktoken\n",
    "        self.easy_enc = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "        self.hard_enc = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "    def get_easy_call(self):\n",
    "        @retry(wait=wait_random_exponential(min=15, max=60), stop=stop_after_attempt(3))\n",
    "        def call(text, temperature=0.7, num_tokens=None):\n",
    "            easy_model = next(self.easy_models)\n",
    "            input_len = len(self.easy_enc.encode(self.system +\" \\n \" + text))\n",
    "            if easy_model in self.completion_models:\n",
    "                completions = openai.Completion.create(\n",
    "                    engine=easy_model,\n",
    "                    prompt=self.system +\" \\n \" + text,\n",
    "                    temperature=temperature,\n",
    "                    max_tokens = 3000,\n",
    "                )\n",
    "                message = completions.choices[0].text\n",
    "                finish_reason = completions.choices[0].finish_reason\n",
    "#                 if finish_reason != 'stop':\n",
    "#                     print(easy_model + \" \" + str(input_len) + \" \" + str(len(self.easy_enc.encode(message))) + \" \" + \" \"+ finish_reason + \" \" + message + \" \\n\")\n",
    "                assert finish_reason == 'stop'\n",
    "            else:\n",
    "                response = openai.ChatCompletion.create(\n",
    "                    model=easy_model,\n",
    "                    messages=[\n",
    "                            {\"role\": \"system\", \"content\": self.system},\n",
    "                            {\"role\": \"user\", \"content\": text},\n",
    "                        ],\n",
    "                        temperature=temperature\n",
    "                    )\n",
    "                message = response['choices'][0]['message']['content']\n",
    "                finish_reason = response['choices'][0]['finish_reason']\n",
    "#                 print(easy_model + \" \" + str(len(self.easy_enc.encode(self.system +\" \\n \" + text))) + \" \" + str(len(self.easy_enc.encode(message))) + \" \" + \" \"+ finish_reason + \" \" + message + \" \\n\")\n",
    "                assert finish_reason == 'stop'\n",
    "                \n",
    "            return message\n",
    "        return call\n",
    "    def get_turbo_call(self):\n",
    "        @retry(wait=wait_random_exponential(min=25, max=60), stop=stop_after_attempt(3))\n",
    "        def call(text, temperature=0.7, num_tokens=None):\n",
    "            easy_model = next(self.turbo_models)\n",
    "            input_len = len(self.easy_enc.encode(self.system +\" \\n \" + text))\n",
    "            \n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=easy_model,\n",
    "                messages=[\n",
    "                        {\"role\": \"system\", \"content\": self.system},\n",
    "                        {\"role\": \"user\", \"content\": text},\n",
    "                    ],\n",
    "                    temperature=temperature\n",
    "                )\n",
    "            message = response['choices'][0]['message']['content']\n",
    "            finish_reason = response['choices'][0]['finish_reason']\n",
    "#             print(easy_model + \" \" + str(len(self.easy_enc.encode(self.system +\" \\n \" + text))) + \" \" + str(len(self.easy_enc.encode(message))) + \" \" + \" \"+ finish_reason + \" \" + message + \" \\n\")\n",
    "            assert finish_reason == 'stop'\n",
    "            return message\n",
    "        return call\n",
    "    def get_hard_call(self):\n",
    "        @retry(wait=wait_random_exponential(min=25, max=60), stop=stop_after_attempt(3))\n",
    "        def call(text, temperature=0.1, num_tokens=None):\n",
    "            model = next(self.hard_models)\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                        {\"role\": \"system\", \"content\": self.system},\n",
    "                        {\"role\": \"user\", \"content\": text},\n",
    "                    ],\n",
    "                    temperature=temperature\n",
    "                )\n",
    "            assert response['choices'][0]['finish_reason'] == 'stop'\n",
    "            return response['choices'][0]['message']['content']\n",
    "        return call\n",
    "     \n",
    "callGpt = CallGpt()\n",
    "callGpt.get_easy_call()(\"Hi gpt! Howdy\")\n",
    "# callGpt.get_hard_call()(\"Hi gpt! Howdy\")\n",
    "# callGpt.get_turbo_call()(\"Hi gpt! Howdy\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f88634c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T18:16:26.885063Z",
     "start_time": "2023-05-21T18:16:26.876642Z"
    }
   },
   "outputs": [],
   "source": [
    "def chunk_text_langchain(text, chunk_size=3400):\n",
    "    text_splitter = TokenTextSplitter(chunk_size=chunk_size, chunk_overlap=100)\n",
    "    texts = text_splitter.split_text(text)\n",
    "    for t in texts:\n",
    "        yield t\n",
    "        \n",
    "def split_text(text):\n",
    "    # Split the text by spaces, newlines, and HTML tags\n",
    "    chunks = re.split(r'( |\\n|<[^>]+>)', text)\n",
    "    \n",
    "    # Find the middle index\n",
    "    middle = len(chunks) // 2\n",
    "\n",
    "    # Split the chunks into two halves\n",
    "    first_half = ''.join(chunks[:min(middle+100, len(chunks)-1)])\n",
    "    second_half = ''.join(chunks[max(0, middle-100):])\n",
    "    \n",
    "    yield first_half\n",
    "    yield second_half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6df6bd5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T18:16:26.902928Z",
     "start_time": "2023-05-21T18:16:26.887635Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_value\n",
      "other_value\n"
     ]
    }
   ],
   "source": [
    "class AddAttribute:\n",
    "    def __init__(self, attribute, value):\n",
    "        self.attribute = attribute\n",
    "        self.value = value\n",
    "\n",
    "    def __call__(self, func):\n",
    "        setattr(func, self.attribute, self.value)\n",
    "        return func\n",
    "\n",
    "@AddAttribute('my_attribute', \"my_value\")\n",
    "@AddAttribute('other_attribute', \"other_value\")\n",
    "def my_function():\n",
    "    pass\n",
    "\n",
    "print(my_function.my_attribute)\n",
    "print(my_function.other_attribute)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a928bf58",
   "metadata": {},
   "source": [
    "# Make Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df8a7bd2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T18:16:27.267706Z",
     "start_time": "2023-05-21T18:16:26.906232Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99ea79f8-f803-11ed-a326-069ebcef22bd\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.save_checkpoint();"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/Users/ahemf/Desktop/chatgpt-iterative/Chatgpt-iterative.ipynb'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from time import sleep\n",
    "from IPython.display import display, Javascript\n",
    "import subprocess\n",
    "import os\n",
    "import uuid\n",
    "\n",
    "def get_notebook_path_and_save():\n",
    "    magic = str(uuid.uuid1()).replace('-beep-boop-', '')\n",
    "    print(magic)\n",
    "    # saves it (ctrl+S)\n",
    "    display(Javascript('IPython.notebook.save_checkpoint();'))\n",
    "    nb_name = None\n",
    "    while nb_name is None:\n",
    "        try:\n",
    "            sleep(0.1)\n",
    "            nb_name = subprocess.check_output(f'grep -l {magic} *.ipynb', shell=True).decode().strip()\n",
    "        except:\n",
    "            pass\n",
    "    return os.path.join(os.getcwd(), nb_name)\n",
    "get_notebook_path_and_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c540a7b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T18:16:27.437608Z",
     "start_time": "2023-05-21T18:16:27.269743Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9a201630-f803-11ed-a326-069ebcef22bd\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.save_checkpoint();"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import traceback\n",
    "try:\n",
    "    raise ValueError\n",
    "except Exception as e:\n",
    "    tb = traceback.extract_tb(e.__traceback__)\n",
    "    for line in tb:\n",
    "        if os.path.abspath(line.filename) == os.path.abspath(get_notebook_path_and_save()):\n",
    "            print(f\"{line.filename}:{line.lineno}: {line.line}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e03e1c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T18:16:27.452854Z",
     "start_time": "2023-05-21T18:16:27.441169Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_variable_names(code):\n",
    "    import ast\n",
    "    tree = ast.parse(code)\n",
    "    variables = []\n",
    "    for node in ast.walk(tree):\n",
    "        if isinstance(node, ast.Assign):\n",
    "            for target in node.targets:\n",
    "                if isinstance(target, ast.Name):\n",
    "    #                 print(f\"{target.id}: {ast.dump(target.ctx)}\")\n",
    "                    if ast.dump(target.ctx) == \"Store()\":\n",
    "                        variables.append(f\"{target.id}\")\n",
    "    return variables\n",
    "\n",
    "\n",
    "def wrapped_exec(code):\n",
    "    from io import StringIO\n",
    "    import sys\n",
    "    prior_std = sys.stdout\n",
    "    prior_err = sys.stderr\n",
    "    import traceback\n",
    "    result = StringIO()\n",
    "    error = StringIO()\n",
    "    sys.stdout = result\n",
    "    sys.stderr = error\n",
    "    exceptions = \"\"\n",
    "    variables_and_values = dict()\n",
    "    try:\n",
    "        exec(code)\n",
    "        var_names = get_variable_names(code)\n",
    "        variables_and_values = dict(locals())\n",
    "        to_del = []\n",
    "        for k in variables_and_values.keys():\n",
    "            if k.strip() not in var_names:\n",
    "                to_del.append(k)\n",
    "        for k in to_del:\n",
    "            del variables_and_values[k]\n",
    "    except Exception as e:\n",
    "        exceptions = traceback.format_exc()\n",
    "    finally:\n",
    "        sys.stdout = prior_std\n",
    "        sys.stderr = prior_err\n",
    "    output = result.getvalue()\n",
    "    exceptions = [w.replace(\"\\n\", '') for w in exceptions.split(\"\\n\") if len(w.strip()) > 0]\n",
    "    if len(exceptions) > 20:\n",
    "        exceptions = exceptions[:10] + exceptions[-10:]\n",
    "    exceptions = \"\\n\".join(exceptions)\n",
    "    error_output = error.getvalue().strip() + \" \\n \" + exceptions.strip()\n",
    "    error_output = error_output.replace('\\n\\n', '').strip()\n",
    "    \n",
    "    \n",
    "    return {\"output\": output, \"error\": error_output, \"variables_and_values\": variables_and_values}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1bc46c7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T18:16:27.487440Z",
     "start_time": "2023-05-21T18:16:27.454381Z"
    }
   },
   "outputs": [],
   "source": [
    "@AddAttribute('name', \"MathTool\")\n",
    "@AddAttribute('description', \"\"\"\n",
    "MathTool:\n",
    "    This tool takes a numeric expression as a string and provides the output for it.\n",
    "\n",
    "    Input params/args: \n",
    "        num_expr (str): numeric expression to evaluate\n",
    "\n",
    "    Returns: \n",
    "        str: evaluated expression answer\n",
    "\n",
    "    Usage:\n",
    "        `answer=MathTool(num_expr=\"2*3\") # Expected answer = 6, # This tool needs no initialization`\n",
    "\n",
    "    \"\"\")\n",
    "def MathTool(num_expr: str):\n",
    "    math_tool = load_tools([\"llm-math\"], llm=llm)[0]\n",
    "    return math_tool._run(num_expr).replace(\"Answer: \", \"\")\n",
    "\n",
    "\n",
    "@AddAttribute('name', \"WikipediaTool\")\n",
    "@AddAttribute('description', \"\"\"\n",
    "WikipediaTool:\n",
    "    This tool takes a phrase or key words and searches them over wikipedia, returns results from wikipedia as a str.\n",
    "\n",
    "    Input params/args: \n",
    "        search_phrase (str): phrase to search over on wikipedia\n",
    "\n",
    "    Returns: \n",
    "        str: searched paragraph on basis of search_phrase from wikipedia\n",
    "\n",
    "    Usage:\n",
    "        `answer=WikipediaTool(search_phrase=\"phrase to search\") # This tool needs no initialization`\n",
    "\n",
    "    \"\"\")\n",
    "def WikipediaTool(search_phrase: str):\n",
    "    tool = load_tools([\"wikipedia\"], llm=llm)[0]\n",
    "    return tool._run(search_phrase)\n",
    "\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "\n",
    "@AddAttribute('name', \"TextLengthCheck\")\n",
    "@AddAttribute('description', \"\"\"\n",
    "TextLengthCheck:\n",
    "    Checks if the token count of the given `text_document` is smaller or lesser than the `threshold`.\n",
    "\n",
    "    Input params/args: \n",
    "        text_document (str): document to verify if its length or word count or token count is less than threshold.\n",
    "        threshold (int): Token count, text_document token count is below this then returns True\n",
    "\n",
    "    Returns: \n",
    "        bool: whether length or token count is less than given threshold.\n",
    "\n",
    "    Usage:\n",
    "        `length_valid = TextLengthCheck(text_document=\"document to check length\") # This tool needs no initialization`\n",
    "        `less_than_ten = TextLengthCheck(text_document=\"document to check length\", threshold=10)`\n",
    "\n",
    "    \"\"\")\n",
    "def TextLengthCheck(text_document: str, threshold: int=3400):\n",
    "    return len(enc.encode(text_document)) < threshold\n",
    "\n",
    "@AddAttribute('name', \"Search\")\n",
    "@AddAttribute('description', \"\"\"\n",
    "Search:\n",
    "    This tool takes a search phrase, performs search over a web search engine and returns a list of urls for the search.\n",
    "\n",
    "    Input params/args: \n",
    "        search_phrase (str): phrase or keywords to search over the web/internet.\n",
    "        top_n (int): Number of webpages or results to return from search. Default is 5.\n",
    "\n",
    "    Returns: \n",
    "        List[str]: List of webpage urls for given search_phrase, List length same as top_n input parameter.\n",
    "\n",
    "    Usage:\n",
    "        `web_url_list = Search(search_phrase=\"phrase to search\") # This tool needs no initialization`\n",
    "        \n",
    "    Alternative Usage:\n",
    "        `web_url_list = Search(search_phrase=\"phrase to search\", top_n=20) # Get a custom number of results\n",
    "\n",
    "    \"\"\")\n",
    "def Search(search_phrase: str, top_n: int=5):\n",
    "    return [r[\"link\"] for r in  BingSearchAPIWrapper().results(search_phrase, top_n)]\n",
    "\n",
    "@AddAttribute('name', \"ChunkText\")\n",
    "@AddAttribute('description', \"\"\"\n",
    "ChunkText:\n",
    "    This tool takes a text document and chunks it into given chunk size lengths, then returns a list of strings as chunked sub-documents.\n",
    "\n",
    "    Input params/args: \n",
    "        text_document (str): document to create chunks from.\n",
    "        chunk_size (int): Size of each chunk. Default is 3400, smaller chunk sizes are needed if downstream systems throw length error or token limit exceeded errors.\n",
    "\n",
    "    Returns: \n",
    "        List[str]: text_chunks\n",
    "\n",
    "    Usage:\n",
    "        `text_chunks = ChunkText(text_document=\"document to chunk\") # This tool needs no initialization`\n",
    "        \n",
    "    Alternative Usage:\n",
    "        `text_chunks = ChunkText(text_document=\"document to chunk\", chunk_size=1800) # Smaller chunk size, more chunks, but avoid token limit exceeded or length errors.\n",
    "\n",
    "    \"\"\")\n",
    "def ChunkText(text_document: str, chunk_size: int=3400, chunk_overlap:int=100):\n",
    "    text_splitter = TokenTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    return text_splitter.split_text(text_document)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a47e38c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T18:16:27.505282Z",
     "start_time": "2023-05-21T18:16:27.489005Z"
    }
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "tlc_gpt3 = partial(TextLengthCheck, threshold=1700)\n",
    "tlc_gpt4 = partial(TextLengthCheck, threshold=3400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f371f137",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T18:16:27.567346Z",
     "start_time": "2023-05-21T18:16:27.507025Z"
    },
    "code_folding": [
     131
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "class Summarizer:\n",
    "    def __init__(self):\n",
    "        self.name = \"Summariser\"\n",
    "        self.description = \"\"\"\n",
    "Summarizer:\n",
    "    This tool takes a text document and summarizes it into a shorter version while preserving the main points and context. Useful when the document is too long and needs to be shortened before further processing.\n",
    "\n",
    "    Input params/args: \n",
    "        text_document (str): document to summarize.\n",
    "\n",
    "    Returns: \n",
    "        str: summarized_document.\n",
    "\n",
    "    Usage:\n",
    "        `summary = Summarizer()(text_document=\"document to summarize\") # Note: this tool needs to be initialized first.`\n",
    "    \"\"\"\n",
    "        self.prompt = PromptTemplate(\n",
    "            input_variables=[\"document\"],\n",
    "            template=\"\"\" \n",
    "Summarize the document below into a shorter version (by eliminating repeatation, by paraphrasing etc.) while preserving the main points and context, do not miss any important details.\n",
    "Document is given below:\n",
    "{document}\n",
    "\"\"\",\n",
    "        )\n",
    "    def __call__(self, text_document):\n",
    "        prompt = self.prompt.format(document=text_document)\n",
    "        return callGpt.get_easy_call()(prompt, temperature=0.7)\n",
    "    \n",
    "class ReduceRepeatTool:\n",
    "    def __init__(self):\n",
    "        self.name = \"ReduceRepeatTool\"\n",
    "        self.description = \"\"\"       \n",
    "ReduceRepeatTool:\n",
    "    This tool takes a text document reduces repeated content in the document. Useful when document has a lot of repeated content or ideas which can be mentioned in a shorter version.\n",
    "\n",
    "    Input params/args: \n",
    "        text_document (str): document to summarize.\n",
    "\n",
    "    Returns: \n",
    "        str: non_repeat_document.\n",
    "\n",
    "    Usage:\n",
    "        `non_repeat_document = ReduceRepeatTool()(text_document=\"document to to reduce repeats\") # Note: this tool needs to be initialized first.`\n",
    "        \n",
    "    \"\"\"\n",
    "        self.prompt = PromptTemplate(\n",
    "            input_variables=[\"document\"],\n",
    "            template=\"\"\" \n",
    "Reduce repeated content in the document given. Some ideas or phrases or points are repeated with minor variation, please remove them by paraphrasing the document into a shorter version, while preserving the main points and context, do not miss any important details.\n",
    "Document is given below:\n",
    "{document}\n",
    "\"\"\",\n",
    "        )\n",
    "    def __call__(self, text_document):\n",
    "        prompt = self.prompt.format(document=text_document)\n",
    "        return callGpt.get_turbo_call()(prompt, temperature=0.4)\n",
    "\n",
    "def combine_array_two_at_a_time(array):\n",
    "    result = []\n",
    "    if len(array) % 2 == 1:\n",
    "        array.append('')\n",
    "    for i in range(0, len(array), 2):\n",
    "        result.append(array[i] + ' ' + array[i+1])\n",
    "    return result\n",
    "\n",
    "\n",
    "def process_text(text, chunk_size, my_function):\n",
    "    # Split the text into chunks\n",
    "    chunks = list(chunk_text_langchain(text, chunk_size))\n",
    "#     print([len(c.split()) for c in chunks])\n",
    "#     # Create a ThreadPoolExecutor\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        # Use the executor to apply my_function to each chunk\n",
    "        futures = [executor.submit(my_function, chunk) for chunk in chunks]\n",
    "\n",
    "    # Get the results from the futures\n",
    "    results = [future.result() for future in futures]\n",
    "    tlc = partial(TextLengthCheck, threshold=1800)\n",
    "    summariser = Summarizer()\n",
    "    while len(results) > 1:\n",
    "        results = [r if tlc(r) else summariser(r) for r in results]\n",
    "        results = combine_array_two_at_a_time(results)\n",
    "    results = [r if tlc(r) else summariser(r) for r in results]\n",
    "    # Concatenate the results\n",
    "    # TODO: what if after concatenation the returned result is too long.\n",
    "    results = ReduceRepeatTool()(' '.join(results))\n",
    "    \n",
    "    return results\n",
    "\n",
    "async def get_url_content(url):\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch()\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(url)\n",
    "#         await page.on(\"load\", lambda: None)\n",
    "        title = await page.title()\n",
    "        page_content = await page.content()\n",
    "        # TODO: get rendered body\n",
    "        page_content = await page.evaluate(\"\"\"\n",
    "        (() => document.body.innerText)()\n",
    "        \"\"\")\n",
    "        await browser.close()\n",
    "        return {\"title\": title, \"page_content\": page_content}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Cleaner:\n",
    "    def __init__(self, model=\"gpt-3.5-turbo\", prompt=None, context=None):\n",
    "        self.instruction = \"\"\"\n",
    "You will be given unclean text fragments from web scraping a url.\n",
    "Your goal is to return cleaned text without html tags and other irrelevant content (including code exception stack traces). \n",
    "If you are given a user request, instruction or query, then use that as well in filtering the information and return information relevant to the user query or instruction.\n",
    "just extract relevant information if user query is given (Try to answer mostly in bullet points in this case.) else return cleaned text..\n",
    "No creativity needed here.\n",
    "Some context about the source document and user query is provided next, use the user query if provided and give very concise succint response.\n",
    "        \"\"\" if prompt is None else prompt\n",
    "        self.clean_now_follows = \"\\nActual text to be cleaned follows: \\n\"\n",
    "        self.prompt = (self.instruction + \" \" + (context if context is not None else \"\") + \" \" + self.clean_now_follows) if prompt is None else prompt\n",
    "        \n",
    "    def clean_one(self, string, model=None):\n",
    "        return callGpt.get_easy_call()(self.prompt + string, temperature=0.1)\n",
    "    \n",
    "    def clean_one_with_exception(self, string):\n",
    "        try:\n",
    "            cleaned_text = self.clean_one(string)\n",
    "            return cleaned_text\n",
    "        except Exception as e:\n",
    "            exp_str = str(e)\n",
    "            too_long = \"maximum context length\" in exp_str and \"your messages resulted in\" in exp_str\n",
    "            if too_long:\n",
    "                return \" \".join([self.clean_one_with_exception(st) for st in split_text(string)])\n",
    "            raise e\n",
    "                \n",
    "    def __call__(self, string, chunk_size=3400):\n",
    "        from functools import partial\n",
    "#         cleaners = [self.clean_one_with_exception for m in self.models]\n",
    "#         cleaners = round_robin(cleaners)\n",
    "        # TODO: Cleaner's next chunks could use short summary from the previous chunks to model understand page context.\n",
    "#         return process_text_round_robin(string, chunk_size, cleaners)\n",
    "        return process_text(string, chunk_size, self.clean_one_with_exception)\n",
    "\n",
    "class GetWebPage:\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        self.name = \"GetWebPage\"\n",
    "        self.description = \"\"\"\n",
    "GetWebPage:\n",
    "    This tool takes a url link to a webpage and returns cleaned text content of that Page. Useful if you want to visit a page and get it's content. Optionally it can also take a user context or instruction and give only relevant parts of the page for the provided context.\n",
    "\n",
    "    Input params/args: \n",
    "        url (str): url of page to visit\n",
    "        context (str): user query/instructions/context about what to look for in this webpage\n",
    "\n",
    "    Returns: \n",
    "        str: page_content\n",
    "\n",
    "    Usage:\n",
    "        `page_content = GetWebPage()(url=\"url to visit\", context=\"user query or page reading instructions\") # Note: this tool needs to be initialized first.`\n",
    "\n",
    "    \"\"\"\n",
    "    def __call__(self, url, context=None):\n",
    "        page_items = run_async(get_url_content, url)\n",
    "#         clean_title = Cleaner(context=f\"Some context about the source, url: {url}, title: {page_items['title']}\")(page_items[\"title\"])\n",
    "        if not isinstance(page_items, dict):\n",
    "            print(f\"url: {url}, title: None, content: None\")\n",
    "            return f\"url: {url}, title: None, content: None\"\n",
    "        page_content = page_items[\"page_content\"]\n",
    "        if not isinstance(page_content, str):\n",
    "            print(f\"url: {url}, title: {page_items['title']}, content: None\")\n",
    "            return f\"url: {url}, title: {page_items['title']}, content: None\"\n",
    "        page_content = Cleaner(context=f\"\\n\\n url: {url}, title: {page_items['title']}\" + (f\"user query or context: {context}\" if context is not None else \"\"))(page_content, \n",
    "        chunk_size=768)\n",
    "        return f\"url: {url}, title: {page_items['title']}, content: {page_content}\"\n",
    "    def _run(self, url: str, run_manager: Optional[CallbackManagerForToolRun] = None) -> str:\n",
    "        \"\"\"Use the tool.\"\"\"\n",
    "        return self.__call__(url)\n",
    "    async def _arun(self, query: str, run_manager: Optional[AsyncCallbackManagerForToolRun] = None) -> str:\n",
    "        \"\"\"Use the tool asynchronously.\"\"\"\n",
    "        raise NotImplementedError(\"custom_search does not support async\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16f23aaa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T18:07:45.066235Z",
     "start_time": "2023-05-21T18:07:37.126301Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Large language model - Wikipedia',\n",
       " 'page_content': 'Jump to content\\nMain menu\\nCreate account\\nLog in\\nPersonal tools\\nContents hide\\n(Top)\\nProperties\\nToggle Properties subsection\\nPretraining datasets\\nScaling laws\\nEmergent abilities\\nHallucination\\nArchitecture\\nToggle Architecture subsection\\nTokenization\\nOutput\\nTraining\\nToggle Training subsection\\nTraining dataset size\\nTraining cost\\nApplication to downstream tasks\\nToggle Application to downstream tasks subsection\\nFine-tuning\\nPrompting\\nInstruction tuning\\nReinforcement learning\\nEvaluation\\nToggle Evaluation subsection\\nPerplexity\\nTask-specific datasets and benchmarks\\nAdversarially constructed evaluations\\nList of large language models\\nSee also\\nNotes\\nReferences\\nLarge language model\\n12 languages\\nArticle\\nTalk\\nRead\\nEdit\\nView history\\nTools\\nFrom Wikipedia, the free encyclopedia\\n\\nA large language model (LLM) is a language model consisting of a neural network with many parameters (typically billions of weights or more), trained on large quantities of unlabeled text using self-supervised learning or semi-supervised learning.[1] LLMs emerged around 2018 and perform well at a wide variety of tasks. This has shifted the focus of natural language processing research away from the previous paradigm of training specialized supervised models for specific tasks.[2]\\n\\nThough the term large language model has no formal definition, it often refers to deep learning models having a parameter count on the order of billions or more.[3] LLMs are general purpose models which excel at a wide range of tasks, as opposed to being trained for one specific task (such as sentiment analysis, named entity recognition, or mathematical reasoning).[2][4] The skill with which they accomplish tasks, and the range of tasks at which they are capable, seems to be a function of the amount of resources (data, parameter-size, computing power) devoted to them, in a way that is not dependent on additional breakthroughs in design.[5]\\n\\nThough trained on simple tasks along the lines of predicting the next word in a sentence, neural language models with sufficient training and parameter counts are found to capture much of the syntax and semantics of human language. In addition, large language models demonstrate considerable general knowledge about the world, and are able to \"memorize\" a great quantity of facts during training.[2]\\n\\nProperties[edit]\\nPretraining datasets[edit]\\nSee also: list of datasets for machine-learning research §\\xa0Internet\\n\\nLLMs are pre-trained on large textual datasets. Some commonly used textual datasets are Common Crawl, The Pile, MassiveText,[6] Wikipedia, and GitHub. The datasets run up to 10 trillion words in size.\\n\\nThe stock of high-quality language data is within 4.6-17 trillion words, which is within an order of magnitude for the largest textual datasets.[7]\\n\\nScaling laws[edit]\\nMain article: Neural scaling law\\n\\nIn general, a LLM can be characterized by 4 parameters: size of the model, size of the training dataset, cost of training, performance after training. Each of these four variables can be precisely defined into a real number, and they are empirically found to be related by simple statistical laws, called \"scaling laws\".\\n\\nOne particular scaling law (\"Chinchilla scaling\") for LLM autoregressively trained for one epoch, with a log-log learning rate schedule, states that:[8]\\n\\n{\\n𝐶\\n=\\n𝐶\\n0\\n𝑁\\n𝐷\\n\\n\\n𝐿\\n=\\n𝐴\\n𝑁\\n𝛼\\n+\\n𝐵\\n𝐷\\n𝛽\\n+\\n𝐿\\n0\\nwhere the variables are\\n\\n𝐶\\n is the cost of training the model, in FLOPs.\\n𝑁\\n is the number of parameters in the model.\\n𝐷\\n is the number of tokens in the training set.\\n𝐿\\n is the average negative log-likelihood loss per token (nats/token), achieved by the trained LLM on the test dataset.\\n\\nand the statistical parameters are\\n\\n𝐶\\n0\\n=\\n6\\n, meaning that it costs 6 FLOPs per parameter to train on one token.[9] Note that training cost is much higher than inference cost, where it costs 1 to 2 FLOPs per parameter to infer on one token.\\n𝛼\\n=\\n0.34\\n,\\n𝛽\\n=\\n0.28\\n,\\n𝐴\\n=\\n406.4\\n,\\n𝐵\\n=\\n410.7\\n,\\n𝐿\\n0\\n=\\n1.69\\n.\\nEmergent abilities[edit]\\nOn a number of natural language benchmarks involving tasks such as question answering, models perform no better than random chance until they reach a certain scale (in this case, measured by training computation), at which point their performance sharply increases. These are examples of emergent abilities.\\n\\nWhile it is generally the case that performance of large models on various tasks can be extrapolated based on the performance of similar smaller models, sometimes \"breaks\"[10] in downstream scaling laws occur such that larger models suddenly acquire substantial abilities at a different rate than in smaller models. These are often referred to as \"emergent abilities\", and have been the subject of substantial study. Researchers note that such abilities often \"cannot be predicted simply by extrapolating the performance of smaller models\".[4] These abilities are discovered rather than programmed-in or designed, in some cases only after the LLM has been publicly deployed.[5] Hundreds of emergent abilities have been described. Examples include multi-step arithmetic, taking college-level exams, identifying the intended meaning of a word,[4] chain-of-thought prompting,[4] decoding the International Phonetic Alphabet, unscrambling a word’s letters, identifying offensive content in paragraphs of Hinglish (a combination of Hindi and English), and generating a similar English equivalent of Kiswahili proverbs.[11]\\n\\nHallucination[edit]\\n\\nGenerative LLMs have been observed to confidently assert claims of fact which do not seem to be justified by their training data, a phenomenon which has been termed \"hallucination\".[12]\\n\\nArchitecture[edit]\\n\\nLarge language models have most commonly used the transformer architecture, which, since 2018, has become the standard deep learning technique for sequential data (previously, recurrent architectures such as the LSTM were most common).[2]\\n\\nTokenization[edit]\\n\\nLLMs are mathematical functions whose input and output are lists of numbers. Consequently, words must be converted to numbers.\\n\\nIn general, a LLM uses a separate tokenizer. A tokenizer maps between texts and lists of integers. The tokenizer is generally adapted to the entire training dataset first, then frozen, before the LLM is trained. A common choice is byte pair encoding.\\n\\nAnother function of tokenizers is text compression, which saves compute. Common words or phrases like \"where is\" can be encoded into one token, instead of 7 characters. The OpenAI GPT series uses a tokenizer where 1 token maps to around 4 characters, or around 0.75 words, in common English text.[13] Uncommon English text is less predictable, thus less compressible, thus requiring more tokens to encode.\\n\\nA tokenizer cannot output arbitrary integers. They generally outputs only integers in the range \\n{\\n0\\n,\\n1\\n,\\n2\\n,\\n.\\n.\\n.\\n,\\n𝑉\\n−\\n1\\n}\\n, where \\n𝑉\\n is called its vocabulary size.\\n\\nSome tokenizers are capable of handling arbitrary text (generally by operating directly on Unicode), but some do not. When encountering un-encodable text, a tokenizer would output a special token (often 0) that represents \"unknown text\". This is often written as [UNK], such as in the BERT paper.\\n\\nAnother special token commonly used is [PAD] (often 1), for \"padding\". This is used because LLMs are generally used on batches of text at one time, and these texts do not encode to the same length. Since LLMs generally require input to be an array that is not jagged, the shorter encoded texts must be padded until they match the length of the longest one.\\n\\nOutput[edit]\\n\\nThe output of a LLM is a probability distribution over its vocabulary. This is usually implemented as follows:\\n\\nUpon receiving a text, the bulk of the LLM outputs a vector \\n𝑦\\n∈\\n𝑅\\n𝑉\\n where \\n𝑉\\n is its vocabulary size (defined above).\\nThe vector \\n𝑦\\n is passed through a softmax function to obtain \\nsoftmax\\n(\\n𝑦\\n)\\n.\\n\\nIn the process, the vector \\n𝑦\\n is usually called the unnormalized logit vector, and the vector \\nsoftmax\\n(\\n𝑦\\n)\\n is called the probability vector. Since the vector \\nsoftmax\\n(\\n𝑦\\n)\\n has \\n𝑉\\n entries, all non-negative, and they sum to 1, we can interpret it as a probability distribution over \\n{\\n0\\n,\\n1\\n,\\n2\\n,\\n.\\n.\\n.\\n,\\n𝑉\\n−\\n1\\n}\\n—that is, it is a probability distribution over the LLM\\'s vocabulary.\\n\\nNote that the softmax function is defined mathematically with no parameters to vary. Consequently it is not trained.\\n\\nTraining[edit]\\n\\nMost LLM are pre-trained such that given a training dataset of text tokens, the model predicts the tokens in the dataset. There are two general styles of such pretraining:[14]\\n\\nautoregressive (GPT-style, \"predict the next word\"): Given a segment of text like \"I like to eat\" the model predicts the next tokens, like \"ice cream\".\\nmasked (\"BERT-style\",[15] \"cloze test\"): Given a segment of text like \"I like to [MASK] [MASK] cream\" the model predicts the masked tokens, like \"eat ice\".\\n\\nLLMs may be trained on auxiliary tasks which test their understanding of the data distribution, such as Next Sentence Prediction (NSP), in which pairs of sentences are presented and the model must predict whether they appear consecutively in the training corpus.[15]\\n\\nUsually, LLMs are trained to minimize a specific loss function: the average negative log likelihood per token (also called cross-entropy loss).[citation needed] For example, if an autoregressive model, given \"I like to eat\", predicts a probability distribution \\n𝑃\\n𝑟\\n(\\n⋅\\n|\\nI like to eat\\n)\\n then the negative log likelihood loss on this token is \\n−\\nlog\\n\\u2061\\n𝑃\\n𝑟\\n(\\nice\\n|\\nI like to eat\\n)\\n.\\n\\nDuring training, regularization loss is also used to stabilize training. However regularization loss is usually not used during testing and evaluation. There are also many more evaluation criteria than just negative log likelihood. See the section below for details.\\n\\nTraining dataset size[edit]\\n\\nThe earliest LLMs were trained on corpora having on the order of billions of words.\\n\\nGPT-1, the first model in OpenAI\\'s numbered series of generative pre-trained transformer models, was trained in 2018 on BookCorpus, consisting of 985 million words.[16] In the same year, BERT was trained on a combination of BookCorpus and English Wikipedia, totalling 3.3 billion words.[15] Since then, training corpora for LLMs have increased by orders of magnitude, reaching up to trillions of tokens.[15]\\n\\nTraining cost[edit]\\n\\nLLMs are computationally expensive to train. A 2020 study estimated the cost of training a 1.5 billion parameter model (2 orders of magnitude smaller than the state of the art at the time) at $1.6 million.[17] Advances in software and hardware have brought the cost substantially down, with a 2023 paper reporting a cost of 72,300 A100-GPU-hours to train a 12 billion parameter model.[18]\\n\\nFor Transformer-based LLM, it costs 6 FLOPs per parameter to train on one token.[9] Note that training cost is much higher than inference cost, where it costs 1 to 2 FLOPs per parameter to infer on one token.\\n\\nApplication to downstream tasks[edit]\\n\\nBetween 2018 and 2020, the standard method for harnessing an LLM for a specific natural language processing (NLP) task was to fine tune the model with additional task-specific training. It has subsequently been found that more powerful LLMs such as GPT-3 can solve tasks without additional training via \"prompting\" techniques, in which the problem to be solved is presented to the model as a text prompt, possibly with some textual examples of similar problems and their solutions.[2]\\n\\nFine-tuning[edit]\\nMain article: Fine-tuning (machine learning)\\n\\nFine-tuning is the practice of modifying an existing pretrained language model by training it (in a supervised fashion) on a specific task (e.g. sentiment analysis, named-entity recognition, or part-of-speech tagging). It is a form of transfer learning. It generally involves the introduction of a new set of weights connecting the final layer of the language model to the output of the downstream task. The original weights of the language model may be \"frozen\", such that only the new layer of weights connecting them to the output are learned during training. Alternatively, the original weights may receive small updates (possibly with earlier layers frozen).[15]\\n\\nPrompting[edit]\\nSee also: Prompt engineering and Few-shot learning (natural language processing)\\n\\nIn the prompting paradigm, popularized by GPT-3,[4] the problem to be solved is formulated via a text prompt, which the model must solve by providing a completion (via inference). In \"few-shot prompting\", the prompt includes a small number of examples of similar (problem, solution) pairs.[2] For example, a sentiment analysis task of labelling the sentiment of a movie review could be prompted as follows:[4]\\n\\nReview: This movie stinks.\\nSentiment: negative\\n\\nReview: This movie is fantastic!\\nSentiment:\\n\\nIf the model outputs \"positive\", then it has correctly solved the task. In zero-shot prompting, no solved examples are provided.[17][19] An example of a zero-shot prompt for the same sentiment analysis task would be \"The sentiment associated with the movie review \\'This movie is fantastic!\\' is\".[20]\\n\\nFew-shot performance of LLMs has been shown to achieve competitive results on NLP tasks, sometimes surpassing prior state-of-the-art fine-tuning approaches. Examples of such NLP tasks are translation, question answering, cloze tasks, unscrambling words, and using a novel word in a sentence.[19] The creation and optimisation of such prompts is called prompt engineering.\\n\\nInstruction tuning[edit]\\n\\nInstruction tuning is a form of fine-tuning designed to facilitate more natural and accurate zero-shot prompting interactions. Given a text input, a pretrained language model will generate a completion which matches the distribution of text on which it was trained. A naive language model given the prompt \"Write an essay about the main themes of Hamlet.\" might provide a completion such as \"A late penalty of 10% per day will be applied to submissions received after March 17.\" In instruction tuning, the language model is trained on many examples of tasks formulated as natural language instructions, along with appropriate responses.\\n\\nVarious techniques for instruction tuning have been applied in practice. One example, \"self-instruct\", fine-tunes the language model on a training set of examples which are themselves generated by an LLM (bootstrapped from a small initial set of human-generated examples).[21]\\n\\nReinforcement learning[edit]\\n\\nOpenAI\\'s InstructGPT protocol involves supervised fine-tuning on a dataset of human-generated (prompt, response) pairs, followed by reinforcement learning from human feedback (RLHF), in which a reward model was supervised-learned on a dataset of human preferences, then this reward model was used to train the LLM itself by proximal policy optimization.[22]\\n\\nEvaluation[edit]\\nPerplexity[edit]\\n\\nThe most commonly used measure of a language model\\'s performance is its perplexity on a given text corpus. Perplexity is a measure of how well a model is able to predict the contents of a dataset; the higher the likelihood the model assigns to the dataset, the lower the perplexity. Mathematically, perplexity is defined as the exponential of the average negative log likelihood per token:\\n\\nlog\\n\\u2061\\n(\\nPerplexity\\n)\\n=\\n−\\n1\\n𝑁\\n∑\\n𝑖\\n=\\n1\\n𝑁\\nlog\\n\\u2061\\n(\\n𝑃\\n𝑟\\n(\\ntoken\\n𝑖\\n|\\ncontext for token\\n𝑖\\n)\\n)\\nhere \\n𝑁\\n is the number of tokens in the text corpus, and \"context for token i\" depends on the specific type of LLM used. If the LLM is autoregressive, then \"context for token i\" is the segment of text appearing before token i. If the LLM is masked, then \"context for token i\" is the segment of text surrounding token i.\\n\\nBecause language models may overfit to their training data, models are usually evaluated by their perplexity on a test set of unseen data.[15] This presents particular challenges for the evaluation of large language models. As they are trained on increasingly large corpora of text largely scraped from the web, it becomes increasingly likely that models\\' training data inadvertently includes portions of any given test set.[19]\\n\\nTask-specific datasets and benchmarks[edit]\\n\\nA large number of testing datasets and benchmarks have also been developed to evaluate the capabilities of language models on more specific downstream tasks. Tests may be designed to evaluate a variety of capabilities, including general knowledge, commonsense reasoning, and mathematical problem-solving.\\n\\nOne broad category of evaluation dataset is question answering datasets, consisting of pairs of questions and correct answers, for example, (\"Have the San Jose Sharks won the Stanley Cup?\", \"No\").[23] A question answering task is considered \"open book\" if the model\\'s prompt includes text from which the expected answer can be derived (for example, the previous question could be adjoined with some text which includes the sentence \"The Sharks have advanced to the Stanley Cup finals once, losing to the Pittsburgh Penguins in 2016.\"[23]). Otherwise, the task is considered \"closed book\", and the model must draw on knowledge retained during training.[24] Some examples of commonly used question answering datasets include TruthfulQA, Web Questions, TriviaQA, and SQuAD.[24]\\n\\nEvaluation datasets may also take the form of text completion, having the model select the most likely word or sentence to complete a prompt, for example: \"Alice was friends with Bob. Alice went to visit her friend, ____\".[19]\\n\\nSome composite benchmarks have also been developed which combine a diversity of different evaluation datasets and tasks. Examples include GLUE, SuperGLUE, MMLU, BIG-bench, and HELM.[25][24]\\n\\nIt was previously standard to report results on a heldout portion of an evaluation dataset after doing supervised fine-tuning on the remainder. It is now more common to evaluate a pre-trained model directly through prompting techniques, though researchers vary in the details of how they formulate prompts for particular tasks, particularly with respect to how many examples of solved tasks are adjoined to the prompt (i.e. the value of n in n-shot prompting).\\n\\nAdversarially constructed evaluations[edit]\\n\\nBecause of the rapid pace of improvement of large language models, evaluation benchmarks have suffered from short lifespans, with state of the art models quickly \"saturating\" existing benchmarks, exceeding the performance of human annotators, leading to efforts to replace or augment the benchmark with more challenging tasks.[26]\\n\\nSome datasets have been constructed adversarially, focusing on particular problems on which extant language models seem to have unusually poor performance compared to humans. One example is the TruthfulQA dataset, a question answering dataset consisting of 817 questions which language models are susceptible to answering incorrectly by mimicking falsehoods to which they were repeatedly exposed during training. For example, an LLM may answer \"No\" to the question \"Can you teach an old dog new tricks?\" because of its exposure to the English idiom you can\\'t teach an old dog new tricks, even though this is not literally true.[27]\\n\\nAnother example of an adversarial evaluation dataset is Swag and its successor, HellaSwag, collections of problems in which one of multiple options must be selected to complete a text passage. The incorrect completions were generated by sampling from a language model and filtering with a set of classifiers. The resulting problems are trivial for humans but at the time the datasets were created state of the art language models had poor accuracy on them. For example:\\n\\nWe see a fitness center sign. We then see a man talking to the camera and sitting and laying on a exercise ball. The man...\\na) demonstrates how to increase efficient exercise work by running up and down balls.\\nb) moves all his arms and legs and builds up a lot of muscle.\\nc) then plays the ball and we see a graphics and hedge trimming demonstration.\\n\\n\\nd) performs sits ups while on the ball and talking.[28]\\n\\nBERT selects b) as the most likely completion, though the correct answer is d).[28]\\n\\nList of large language models[edit]\\nList of large language models\\nName\\tRelease date[a]\\tDeveloper\\tNumber of parameters[b]\\tCorpus size\\tLicense[c]\\tNotes\\nBERT\\t2018\\tGoogle\\t340 million[29]\\t3.3 billion words[29]\\tApache 2.0[30]\\tAn early and influential language model,[2] but encoder-only and thus not built to be prompted or generative[31]\\nXLNet\\t2019\\tGoogle\\t~340 million[32]\\t33 billion words\\t\\tAn alternative to BERT; designed as encoder-only[33][34]\\nGPT-2\\t2019\\tOpenAI\\t1.5 billion[35]\\t40GB[36] (~10 billion tokens)[37]\\tMIT[38]\\tgeneral-purpose model based on transformer architecture\\nGPT-3\\t2020\\tOpenAI\\t175 billion[17]\\t300 billion tokens[37]\\tpublic web API\\tA fine-tuned variant of GPT-3, termed GPT-3.5, was made available to the public through a web interface called ChatGPT in 2022.[39]\\nGPT-Neo\\tMarch 2021\\tEleutherAI\\t2.7 billion[40]\\t825 GiB[41]\\tMIT[42]\\tThe first of a series of free GPT-3 alternatives released by EleutherAI. GPT-Neo outperformed an equivalent-size GPT-3 model on some benchmarks, but was significantly worse than the largest GPT-3.[42]\\nGPT-J\\tJune 2021\\tEleutherAI\\t6 billion[43]\\t825 GiB[41]\\tApache 2.0\\tGPT-3-style language model\\nMegatron-Turing NLG\\tOctober 2021[44]\\tMicrosoft and Nvidia\\t530 billion[45]\\t338.6 billion tokens[45]\\tRestricted web access\\tStandard architecture but trained on a supercomputing cluster.\\nErnie 3.0 Titan\\tDecember 2021\\tBaidu\\t260 billion[46]\\t4 Tb\\tProprietary\\tChinese-language LLM. Ernie Bot is based on this model.\\nClaude[47]\\tDecember 2021\\tAnthropic\\t52 billion[48]\\t400 billion tokens[48]\\tClosed beta\\tFine-tuned for desirable behavior in conversations.[49]\\nGLaM (Generalist Language Model)\\tDecember 2021\\tGoogle\\t1.2 trillion[50]\\t1.6 trillion tokens[50]\\tProprietary\\tSparse mixture-of-experts model, making it more expensive to train but cheaper to run inference compared to GPT-3.\\nGopher\\tDecember 2021\\tDeepMind\\t280 billion[51]\\t300 billion tokens[52]\\tProprietary\\t\\nLaMDA (Language Models for Dialog Applications)\\tJanuary 2022\\tGoogle\\t137 billion[53]\\t1.56T words,[53] 168 billion tokens[52]\\tProprietary\\tSpecialized for response generation in conversations.\\nGPT-NeoX\\tFebruary 2022\\tEleutherAI\\t20 billion[54]\\t825 GiB[41]\\tApache 2.0\\tbased on the Megatron architecture\\nChinchilla\\tMarch 2022\\tDeepMind\\t70 billion[55]\\t1.4 trillion tokens[55][52]\\tProprietary\\tReduced-parameter model trained on more data. Used in the Sparrow bot.\\nPaLM (Pathways Language Model)\\tApril 2022\\tGoogle\\t540 billion[56]\\t768 billion tokens[55]\\tProprietary\\taimed to reach the practical limits of model scale\\nOPT (Open Pretrained Transformer)\\tMay 2022\\tMeta\\t175 billion[57]\\t180 billion tokens[58]\\tNon-commercial research[d]\\tGPT-3 architecture with some adaptations from Megatron\\nYaLM 100B\\tJune 2022\\tYandex\\t100 billion[59]\\t1.7TB[59]\\tApache 2.0\\tEnglish-Russian model based on Microsoft\\'s Megatron-LM.\\nMinerva\\tJune 2022\\tGoogle\\t540 billion[60]\\t38.5B tokens from webpages filtered for mathematical content and from papers submitted to the arXiv preprint server[60]\\tProprietary\\tLLM trained for solving \"mathematical and scientific questions using step-by-step reasoning\".[61] Minerva is based on PaLM model, further trained on mathematical and scientific data.\\nBLOOM\\tJuly 2022\\tLarge collaboration led by Hugging Face\\t175 billion[62]\\t350 billion tokens (1.6TB)[63]\\tResponsible AI\\tEssentially GPT-3 but trained on a multi-lingual corpus (30% English excluding programming languages)\\nGalactica\\tNovember 2022\\tMeta\\t120 billion\\t106 billion tokens[64]\\tCC-BY-NC-4.0\\tTrained on scientific text and modalities.\\nAlexaTM (Teacher Models)\\tNovember 2022\\tAmazon\\t20 billion[65]\\t1.3 trillion[66]\\tpublic web API[67]\\tbidirectional sequence-to-sequence architecture\\nLLaMA (Large Language Model Meta AI)\\tFebruary 2023\\tMeta\\t65 billion[68]\\t1.4 trillion[68]\\tNon-commercial research[e]\\tTrained on a large 20-language corpus to aim for better performance with fewer parameters.[68] Researchers from Stanford University trained a fine-tuned model based on LLaMA weights, called Alpaca.[69]\\nGPT-4\\tMarch 2023\\tOpenAI\\tExact number unknown, approximately 1 trillion [f]\\tUnknown\\tpublic web API\\tAvailable for ChatGPT Plus users and used in several products.\\nCerebras-GPT\\tMarch 2023\\tCerebras\\t13 billion[71]\\t\\tApache 2.0\\tTrained with Chinchilla formula.\\nFalcon\\tMarch 2023\\tTechnology Innovation Institute\\t40 billion[72]\\t1 Trillion tokens (1TB)[72]\\tProprietary\\tThe model is claimed to use only 75% of GPT-3\\'s training compute, 40% of Chinchilla\\'s, and 80% of PaLM-62B\\'s.\\nBloombergGPT\\tMarch 2023\\tBloomberg L.P.\\t50 billion\\t363 billion token dataset based on Bloomberg\\'s data sources, plus 345 billion tokens from general purpose datasets[73]\\tProprietary\\tLLM trained on financial data from proprietary sources, that \"outperforms existing models on financial tasks by significant margins without sacrificing performance on general LLM benchmarks\"\\nPanGu-Σ\\tMarch 2023\\tHuawei\\t1.085 trillion\\t329 billion tokens[74]\\tProprietary\\t\\nOpenAssistant[75]\\tMarch 2023\\tLAION\\t17 billion\\t1.5 trillion tokens\\tApache 2.0\\tTrained on crowdsourced open data\\nPaLM 2 (Pathways Language Model 2)\\tMay 2023\\tGoogle\\t340 billion[76]\\t3.6 trillion tokens[76]\\tProprietary\\tUsed in Bard chatbot.[77]\\nSee also[edit]\\nFoundation models\\nNotes[edit]\\n^ This is the date that documentation describing the model\\'s architecture was first released.\\n^ In many cases, researchers release or report on multiple versions of a model having different sizes. In these cases, the size of the largest model is listed here.\\n^ This is the license of the pre-trained model weights. In almost all cases the training code itself is open-source or can be easily replicated.\\n^ The smaller models including 66B are publicly available, while the 175B model is available on request.\\n^ Facebook\\'s license and distribution scheme restricted access to approved researchers, but the model weights were leaked and became widely available.\\n^ As stated in Technical report: \"Given both the competitive landscape and the safety implications of large-scale models like GPT-4, this report contains no further details about the architecture (including model size), hardware, training compute, dataset construction, training method ...\"[70] Approximate number in the comparison chart that compares the relative storage, from the same report.\\nReferences[edit]\\n^ Goled, Shraddha (May 7, 2021). \"Self-Supervised Learning Vs Semi-Supervised Learning: How They Differ\". Analytics India Magazine.\\n^ \\nJump up to:\\na b c d e f g Manning, Christopher D. (2022). \"Human Language Understanding & Reasoning\". Daedalus. 151 (2): 127–138. doi:10.1162/daed_a_01905. S2CID\\xa0248377870.\\n^ Carlini, Nicholas; Tramer, Florian; Wallace, Eric; Jagielski, Matthew; Herbert-Voss, Ariel; Lee, Katherine; Roberts, Adam; Brown, Tom B; Song, Dawn; Erlingsson, Ulfar (2021). Extracting Training Data from Large Language Models (PDF). USENIX Security Symposium. Vol.\\xa06.\\n^ \\nJump up to:\\na b c d e f Wei, Jason; Tay, Yi; Bommasani, Rishi; Raffel, Colin; Zoph, Barret; Borgeaud, Sebastian; Yogatama, Dani; Bosma, Maarten; Zhou, Denny; Metzler, Donald; Chi, Ed H.; Hashimoto, Tatsunori; Vinyals, Oriol; Liang, Percy; Dean, Jeff; Fedus, William (31 August 2022). \"Emergent Abilities of Large Language Models\". Transactions on Machine Learning Research. ISSN\\xa02835-8856.\\n^ \\nJump up to:\\na b Bowman, Samuel R. \"Eight Things to Know about Large Language Models\" (PDF).\\n^ \"Papers with Code - MassiveText Dataset\". paperswithcode.com. Retrieved 2023-04-26.\\n^ Villalobos, Pablo; Sevilla, Jaime; Heim, Lennart; Besiroglu, Tamay; Hobbhahn, Marius; Ho, Anson (2022-10-25). \"Will we run out of data? An analysis of the limits of scaling datasets in Machine Learning\". arXiv:2211.04325 [cs].\\n^ Hoffmann, Jordan; Borgeaud, Sebastian; Mensch, Arthur; Buchatskaya, Elena; Cai, Trevor; Rutherford, Eliza; Casas, Diego de Las; Hendricks, Lisa Anne; Welbl, Johannes; Clark, Aidan; Hennigan, Tom; Noland, Eric; Millican, Katie; Driessche, George van den; Damoc, Bogdan (2022-03-29). \"Training Compute-Optimal Large Language Models\". arXiv:2203.15556 [cs].\\n^ \\nJump up to:\\na b Kaplan, Jared; McCandlish, Sam; Henighan, Tom; Brown, Tom B.; Chess, Benjamin; Child, Rewon; Gray, Scott; Radford, Alec; Wu, Jeffrey; Amodei, Dario (2020). \"Scaling Laws for Neural Language Models\". CoRR. abs/2001.08361. arXiv:2001.08361.\\n^ Caballero, Ethan; Gupta, Kshitij; Rish, Irina; Krueger, David (2022). Broken Neural Scaling Laws. International Conference on Learning Representations (ICLR), 2023.\\n^ Ornes, Stephen (March 16, 2023). \"The Unpredictable Abilities Emerging From Large AI Models\". Quanta Magazine.\\n^ Ji, Ziwei; Lee, Nayeon; Frieske, Rita; Yu, Tiezheng; Su, Dan; Xu, Yan; Ishii, Etsuko; Bang, Yejin; Dai, Wenliang; Madotto, Andrea; Fung, Pascale (November 2022). \"Survey of Hallucination in Natural Language Generation\" (pdf). ACM Computing Surveys. Association for Computing Machinery. 55 (12): 1–38. arXiv:2202.03629. doi:10.1145/3571730. S2CID\\xa0246652372. Retrieved 15 January 2023.\\n^ \"OpenAI API\". platform.openai.com. Archived from the original on April 23, 2023. Retrieved 2023-04-30.\\n^ Zaib, Munazza; Sheng, Quan Z.; Emma Zhang, Wei (4 February 2020). \"A Short Survey of Pre-trained Language Models for Conversational AI-A New Age in NLP\". Proceedings of the Australasian Computer Science Week Multiconference: 1–4. arXiv:2104.10810. doi:10.1145/3373017.3373028. ISBN\\xa09781450376976. S2CID\\xa0211040895.\\n^ \\nJump up to:\\na b c d e f Jurafsky, Dan; Martin, James H. (7 January 2023). Speech and Language Processing (PDF) (3rd edition draft\\xa0ed.). Retrieved 24 May 2022.\\n^ Zhu, Yukun; Kiros, Ryan; Zemel, Rich; Salakhutdinov, Ruslan; Urtasun, Raquel; Torralba, Antonio; Fidler, Sanja (December 2015). \"Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books\" (PDF). 2015 IEEE International Conference on Computer Vision (ICCV): 19–27. arXiv:1506.06724. doi:10.1109/ICCV.2015.11. ISBN\\xa0978-1-4673-8391-2. S2CID\\xa06866988. Retrieved 11 April 2023.\\n^ \\nJump up to:\\na b c Wiggers, Kyle (28 April 2022). \"The emerging types of language models and why they matter\". TechCrunch.\\n^ Biderman, Stella; Schoelkopf, Hailey; Anthony, Quentin; Bradley, Herbie; Khan, Mohammad Aflah; Purohit, Shivanshu; Prashanth, USVSN Sai (April 2023). \"Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling\". arXiv:2304.01373 [cs.CL].\\n^ \\nJump up to:\\na b c d Brown, Tom B.; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Gretchen; Henighan, Tom; Child, Rewon; Ramesh, Aditya; Ziegler, Daniel M.; Wu, Jeffrey; Winter, Clemens; Hesse, Christopher; Chen, Mark; Sigler, Eric; Litwin, Mateusz; Gray, Scott; Chess, Benjamin; Clark, Jack; Berner, Christopher; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario (Dec 2020). Larochelle, H.; Ranzato, M.; Hadsell, R.; Balcan, M.F.; Lin, H. (eds.). \"Language Models are Few-Shot Learners\" (PDF). Advances in Neural Information Processing Systems. Curran Associates, Inc. 33: 1877–1901.\\n^ Bosma, Maarten; Wei, Jason (6 October 2021). \"Introducing FLAN: More generalizable Language Models with Instruction Fine-Tuning\". Google Research.\\n^ Wang, Yizhong; Kordi, Yeganeh; Mishra, Swaroop; Liu, Alisa; Smith, Noah A.; Khashabi, Daniel; Hajishirzi, Hannaneh (2022). \"Self-Instruct: Aligning Language Model with Self Generated Instructions\". arXiv:2212.10560 [cs.CL].\\n^ Ouyang, Long; Wu, Jeff; Jiang, Xu; Almeida, Diogo; Wainwright, Carroll L.; Mishkin, Pamela; Zhang, Chong; Agarwal, Sandhini; Slama, Katarina; Ray, Alex; Schulman, John; Hilton, Jacob; Kelton, Fraser; Miller, Luke; Simens, Maddie; Askell, Amanda; Welinder, Peter; Christiano, Paul; Leike, Jan; Lowe, Ryan (2022). \"Training language models to follow instructions with human feedback\". arXiv:2203.02155 [cs.CL].\\n^ \\nJump up to:\\na b Clark, Christopher; Lee, Kenton; Chang, Ming-Wei; Kwiatkowski, Tom; Collins, Michael; Toutanova, Kristina (2019). \"BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions\". arXiv:1905.10044 [cs.CL].\\n^ \\nJump up to:\\na b c Wayne Xin Zhao; Zhou, Kun; Li, Junyi; Tang, Tianyi; Wang, Xiaolei; Hou, Yupeng; Min, Yingqian; Zhang, Beichen; Zhang, Junjie; Dong, Zican; Du, Yifan; Yang, Chen; Chen, Yushuo; Chen, Zhipeng; Jiang, Jinhao; Ren, Ruiyang; Li, Yifan; Tang, Xinyu; Liu, Zikang; Liu, Peiyu; Nie, Jian-Yun; Wen, Ji-Rong (2023). \"A Survey of Large Language Models\". arXiv:2303.18223 [cs.CL].\\n^ Huyen, Chip (18 October 2019). \"Evaluation Metrics for Language Modeling\". The Gradient.\\n^ Srivastava, Aarohi; et\\xa0al. (2022). \"Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models\". arXiv:2206.04615 [cs.CL].\\n^ Lin, Stephanie; Hilton, Jacob; Evans, Owain (2021). \"TruthfulQA: Measuring How Models Mimic Human Falsehoods\". arXiv:2109.07958 [cs.CL].\\n^ \\nJump up to:\\na b Zellers, Rowan; Holtzman, Ari; Bisk, Yonatan; Farhadi, Ali; Choi, Yejin (2019). \"HellaSwag: Can a Machine Really Finish Your Sentence?\". arXiv:1905.07830 [cs.CL].\\n^ \\nJump up to:\\na b Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina (11 October 2018). \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\". arXiv:1810.04805v2 [cs.CL].\\n^ \"BERT\". March 13, 2023 – via GitHub.\\n^ Patel, Ajay; Li, Bryan; Rasooli, Mohammad Sadegh; Constant, Noah; Raffel, Colin; Callison-Burch, Chris (2022). \"Bidirectional Language Models Are Also Few-shot Learners\". ArXiv. S2CID\\xa0252595927.\\n^ \"BERT, RoBERTa, DistilBERT, XLNet: Which one to use?\".\\n^ Naik, Amit Raja (September 23, 2021). \"Google Introduces New Architecture To Reduce Cost Of Transformers\". Analytics India Magazine.\\n^ Yang, Zhilin; Dai, Zihang; Yang, Yiming; Carbonell, Jaime; Salakhutdinov, Ruslan; Le, Quoc V. (2 January 2020). \"XLNet: Generalized Autoregressive Pretraining for Language Understanding\". arXiv:1906.08237 [cs]. Retrieved 5 May 2023.\\n^ \"GPT-2: 1.5B Release\". OpenAI. 2019-11-05. Archived from the original on 2019-11-14. Retrieved 2019-11-14.\\n^ \"Better language models and their implications\". openai.com.\\n^ \\nJump up to:\\na b \"OpenAI\\'s GPT-3 Language Model: A Technical Overview\". lambdalabs.com.\\n^ \"gpt-2\". GitHub. Retrieved 13 March 2023.\\n^ \"ChatGPT: Optimizing Language Models for Dialogue\". OpenAI. 2022-11-30. Retrieved 2023-01-13.\\n^ \"GPT Neo\". March 15, 2023 – via GitHub.\\n^ \\nJump up to:\\na b c Gao, Leo; Biderman, Stella; Black, Sid; Golding, Laurence; Hoppe, Travis; Foster, Charles; Phang, Jason; He, Horace; Thite, Anish; Nabeshima, Noa; Presser, Shawn; Leahy, Connor (31 December 2020). \"The Pile: An 800GB Dataset of Diverse Text for Language Modeling\". arXiv:2101.00027 [cs.CL].\\n^ \\nJump up to:\\na b Iyer, Abhishek (15 May 2021). \"GPT-3\\'s free alternative GPT-Neo is something to be excited about\". VentureBeat.\\n^ \"GPT-J-6B: An Introduction to the Largest Open Source GPT Model | Forefront\". www.forefront.ai. Retrieved 2023-02-28.\\n^ Alvi, Ali; Kharya, Paresh (11 October 2021). \"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, the World\\'s Largest and Most Powerful Generative Language Model\". Microsoft Research.\\n^ \\nJump up to:\\na b Smith, Shaden; Patwary, Mostofa; Norick, Brandon; LeGresley, Patrick; Rajbhandari, Samyam; Casper, Jared; Liu, Zhun; Prabhumoye, Shrimai; Zerveas, George; Korthikanti, Vijay; Zhang, Elton; Child, Rewon; Aminabadi, Reza Yazdani; Bernauer, Julie; Song, Xia (2022-02-04). \"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model\". arXiv:2201.11990.\\n^ Wang, Shuohuan; Sun, Yu; Xiang, Yang; Wu, Zhihua; Ding, Siyu; Gong, Weibao; Feng, Shikun; Shang, Junyuan; Zhao, Yanbin; Pang, Chao; Liu, Jiaxiang; Chen, Xuyi; Lu, Yuxiang; Liu, Weixin; Wang, Xi; Bai, Yangfan; Chen, Qiuliang; Zhao, Li; Li, Shiyong; Sun, Peng; Yu, Dianhai; Ma, Yanjun; Tian, Hao; Wu, Hua; Wu, Tian; Zeng, Wei; Li, Ge; Gao, Wen; Wang, Haifeng (December 23, 2021). \"ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation\". arXiv:2112.12731.\\n^ \"Product\". Anthropic. Retrieved 14 March 2023.\\n^ \\nJump up to:\\na b Askell, Amanda; Bai, Yuntao; Chen, Anna; et\\xa0al. (9 December 2021). \"A General Language Assistant as a Laboratory for Alignment\". arXiv:2112.00861 [cs.CL].\\n^ Bai, Yuntao; Kadavath, Saurav; Kundu, Sandipan; et\\xa0al. (15 December 2022). \"Constitutional AI: Harmlessness from AI Feedback\". arXiv:2212.08073 [cs.CL].\\n^ \\nJump up to:\\na b Dai, Andrew M; Du, Nan (December 9, 2021). \"More Efficient In-Context Learning with GLaM\". ai.googleblog.com. Retrieved 2023-03-09.\\n^ \"Language modelling at scale: Gopher, ethical considerations, and retrieval\". www.deepmind.com. Retrieved 20 March 2023.\\n^ \\nJump up to:\\na b c Hoffmann, Jordan; Borgeaud, Sebastian; Mensch, Arthur; et\\xa0al. (29 March 2022). \"Training Compute-Optimal Large Language Models\". arXiv:2203.15556 [cs.CL].\\n^ \\nJump up to:\\na b Cheng, Heng-Tze; Thoppilan, Romal (January 21, 2022). \"LaMDA: Towards Safe, Grounded, and High-Quality Dialog Models for Everything\". ai.googleblog.com. Retrieved 2023-03-09.\\n^ Black, Sidney; Biderman, Stella; Hallahan, Eric; et\\xa0al. (2022-05-01). GPT-NeoX-20B: An Open-Source Autoregressive Language Model. Proceedings of BigScience Episode #5 -- Workshop on Challenges & Perspectives in Creating Large Language Models. Vol.\\xa0Proceedings of BigScience Episode #5 -- Workshop on Challenges & Perspectives in Creating Large Language Models. pp.\\xa095–136. Retrieved 2022-12-19.\\n^ \\nJump up to:\\na b c Hoffmann, Jordan; Borgeaud, Sebastian; Mensch, Arthur; Sifre, Laurent (12 April 2022). \"An empirical analysis of compute-optimal large language model training\". Deepmind Blog.\\n^ Narang, Sharan; Chowdhery, Aakanksha (April 4, 2022). \"Pathways Language Model (PaLM): Scaling to 540 Billion Parameters for Breakthrough Performance\". ai.googleblog.com. Retrieved 2023-03-09.\\n^ \"Democratizing access to large-scale language models with OPT-175B\". ai.facebook.com.\\n^ Zhang, Susan; Roller, Stephen; Goyal, Naman; Artetxe, Mikel; Chen, Moya; Chen, Shuohui; Dewan, Christopher; Diab, Mona; Li, Xian; Lin, Xi Victoria; Mihaylov, Todor; Ott, Myle; Shleifer, Sam; Shuster, Kurt; Simig, Daniel; Koura, Punit Singh; Sridhar, Anjali; Wang, Tianlu; Zettlemoyer, Luke (21 June 2022). \"OPT: Open Pre-trained Transformer Language Models\". arXiv:2205.01068 [cs.CL].\\n^ \\nJump up to:\\na b Khrushchev, Mikhail; Vasilev, Ruslan; Petrov, Alexey; Zinov, Nikolay (2022-06-22), YaLM 100B, retrieved 2023-03-18\\n^ \\nJump up to:\\na b Lewkowycz, Aitor; Andreassen, Anders; Dohan, David; Dyer, Ethan; Michalewski, Henryk; Ramasesh, Vinay; Slone, Ambrose; Anil, Cem; Schlag, Imanol; Gutman-Solo, Theo; Wu, Yuhuai; Neyshabur, Behnam; Gur-Ari, Guy; Misra, Vedant (30 June 2022). \"Solving Quantitative Reasoning Problems with Language Models\". arXiv:2206.14858 [cs.CL].\\n^ \"Minerva: Solving Quantitative Reasoning Problems with Language Models\". ai.googleblog.com. Retrieved 20 March 2023.\\n^ Ananthaswamy, Anil (8 March 2023). \"In AI, is bigger always better?\". Nature.\\n^ \"bigscience/bloom · Hugging Face\". huggingface.co.\\n^ Taylor, Ross; Kardas, Marcin; Cucurull, Guillem; Scialom, Thomas; Hartshorn, Anthony; Saravia, Elvis; Poulton, Andrew; Kerkez, Viktor; Stojnic, Robert (16 November 2022). \"Galactica: A Large Language Model for Science\". arXiv:2211.09085 [cs.CL].\\n^ \"20B-parameter Alexa model sets new marks in few-shot learning\". Amazon Science. 2 August 2022.\\n^ Soltan, Saleh; Ananthakrishnan, Shankar; FitzGerald, Jack; et\\xa0al. (3 August 2022). \"AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model\". arXiv:2208.01448 [cs.CL].\\n^ \"AlexaTM 20B is now available in Amazon SageMaker JumpStart | AWS Machine Learning Blog\". aws.amazon.com. 17 November 2022. Retrieved 13 March 2023.\\n^ \\nJump up to:\\na b c \"Introducing LLaMA: A foundational, 65-billion-parameter large language model\". Meta AI. 24 February 2023.\\n^ \"Stanford CRFM\". crfm.stanford.edu.\\n^ \"GPT-4 Technical Report\" (PDF). OpenAI. 2023. Archived (PDF) from the original on March 14, 2023. Retrieved March 14, 2023.\\n^ Dey, Nolan (March 28, 2023). \"Cerebras-GPT: A Family of Open, Compute-efficient, Large Language Models\". Cerebras.\\n^ \\nJump up to:\\na b \"Abu Dhabi-based TII launches its own version of ChatGPT\". tii.ae.\\n^ Wu, Shijie; Irsoy, Ozan; Lu, Steven; Dabravolski, Vadim; Dredze, Mark; Gehrmann, Sebastian; Kambadur, Prabhanjan; Rosenberg, David; Mann, Gideon (March 30, 2023). \"BloombergGPT: A Large Language Model for Finance\". arXiv:2303.17564.\\n^ Ren, Xiaozhe; Zhou, Pingyi; Meng, Xinfan; Huang, Xinjing; Wang, Yadao; Wang, Weichao; Li, Pengfei; Zhang, Xiaoda; Podolskiy, Alexander; Arshinov, Grigory; Bout, Andrey; Piontkovskaya, Irina; Wei, Jiansheng; Jiang, Xin; Su, Teng; Liu, Qun; Yao, Jun (March 19, 2023). \"PanGu-Σ: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing\". arXiv:2303.10845.\\n^ Köpf, Andreas; Kilcher, Yannic; von Rütte, Dimitri; Anagnostidis, Sotiris; Tam, Zhi-Rui; Stevens, Keith; Barhoum, Abdullah; Duc, Nguyen Minh; Stanley, Oliver; Nagyfi, Richárd; ES, Shahul; Suri, Sameer; Glushkov, David; Dantuluri, Arnav; Maguire, Andrew (2023-04-14). \"OpenAssistant Conversations -- Democratizing Large Language Model Alignment\". arXiv:2304.07327 [cs].\\n^ \\nJump up to:\\na b Elias, Jennifer (16 May 2023). \"Google\\'s newest A.I. model uses nearly five times more text data for training than its predecessor\". CNBC. Retrieved 18 May 2023.\\n^ \"Introducing PaLM 2\". Google. May 10, 2023.\\nshow\\nvte\\nNatural language processing\\nCategories: Large language modelsDeep learningNatural language processing\\nThis page was last edited on 18 May 2023, at 17:45\\xa0(UTC).\\nText is available under the Creative Commons Attribution-ShareAlike License 3.0; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.\\nPrivacy policy\\nAbout Wikipedia\\nDisclaimers\\nContact Wikipedia\\nMobile view\\nDevelopers\\nStatistics\\nCookie statement'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_async(get_url_content, \"https://en.wikipedia.org/wiki/Large_language_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "1049be35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T08:14:13.471671Z",
     "start_time": "2023-05-24T08:11:35.492899Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url: https://en.wikipedia.org/wiki/Large_language_model, title: Large language model - Wikipedia, content: Large language models (LLMs) are versatile tools that can be used for a variety of natural language processing tasks, such as sentiment analysis and text summarization. They are trained on large textual datasets and evaluated using the perplexity measure. LLMs can be fine-tuned for specific tasks or prompted with a text prompt and examples. They are useful in scenarios where understanding complex language structures, long-term dependencies, context, multiple languages, domains, or topics is required. LLMs can be used to generate natural language text, improve accuracy in NLP tasks, and generate text for creative applications.\n",
      "\n",
      "LLMs are used when a large amount of data is available for training, a high degree of accuracy is required, or a complex task needs to be solved. They can be used in various applications, including chatbots and virtual assistants, speech recognition, and improving search engine results. LLMs have been shown to outperform smaller models on a variety of tasks. There are a number of large language models available, including the GPT-3 model from OpenAI, the BloombergGPT model from Bloomberg, and the PanGu-Σ model from Alibaba.\n",
      "\n",
      "When to use large language models? Large language models should be used when the task requires a large amount of data and/or a large amount of parameters, compute resources, open-domain question answering, alignment of large language models, or deep learning for natural language processing.\n"
     ]
    }
   ],
   "source": [
    "print(GetWebPage()(\"https://en.wikipedia.org/wiki/Large_language_model\", context=\"When to use large language models?\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a4759b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T18:20:49.793886Z",
     "start_time": "2023-05-21T18:20:30.512057Z"
    }
   },
   "outputs": [],
   "source": [
    "class PDFReaderTool:\n",
    "    def __init__(self,):\n",
    "        self.reader = download_loader(\"PyMuPDFReader\")\n",
    "        \n",
    "    def __call__(self, url):\n",
    "        with tempfile.NamedTemporaryFile(dir=os.getcwd(), delete=False) as temp_file:\n",
    "            response = requests.get(url)\n",
    "            temp_file.write(response.content)\n",
    "\n",
    "\n",
    "            documents = self.reader().load(file_path=temp_file.name, metadata=True)\n",
    "\n",
    "        # ensure document texts are not bytes objects\n",
    "        for doc in documents:\n",
    "            doc.text = doc.text.decode()\n",
    "        doc_text = \" \".join([doc.text for doc in documents])\n",
    "#         doc_text = \" \".join([t for t in doc_text.split(\"\\n\") if len(t.strip())>0])\n",
    "        return doc_text\n",
    "    \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a110956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_array_two_at_a_time(array):\n",
    "    result = []\n",
    "    if len(array) % 2 == 1:\n",
    "        array.append('')\n",
    "    for i in range(0, len(array), 2):\n",
    "        result.append([array[i],array[i+1]])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "1f364594",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T18:06:33.325962Z",
     "start_time": "2023-05-22T18:06:33.302314Z"
    }
   },
   "outputs": [],
   "source": [
    "class LongSummarizer:\n",
    "    def __init__(self, chunk_size=1000):\n",
    "        self.chunk_size=1000\n",
    "        self.name = \"LongSummarizer\"\n",
    "        self.description = \"\"\"\n",
    "LongSummarizer:\n",
    "    This tool takes a text document breaks it into chunks, then summarizes the chunks and creates chunk level and document level summary.\n",
    "\n",
    "    Input params/args: \n",
    "        long_document (str): document to summarize.\n",
    "\n",
    "    Returns: \n",
    "        str: summarized_document.\n",
    "\n",
    "    Usage:\n",
    "        `summarized_document = LongSummarizer()(text_document=\"Long document to summarize\") # Note: this tool needs to be initialized first.`\n",
    "    \"\"\"\n",
    "        self.out_dict = \"\"\"\n",
    "{\"current_summary\": <summary for the current fragment>, \"title\": <title or short description of this fragment>, \"summary_till_now\": <running summary or continual summary>, \"questions_and_answers\": <python list of question-answer tuples>}\n",
    "        \"\"\"\n",
    "        self.prompt = PromptTemplate(\n",
    "            input_variables=[\"summary_till_now\", \"document\", \"out_dict\"],\n",
    "            template=\"\"\" \n",
    "The below text is small part/fragment text of a larger document that we are reading sequentially.\n",
    "\n",
    "{document}\n",
    "\n",
    "You are also given a summary of what we have read till now (summary_till_now) from the larger document.\n",
    "\n",
    "{summary_till_now}\n",
    "\n",
    "Based on the fragment of document and previous summary provided you need to:\n",
    "- Write a summary for the current fragment in `current_summary` using the fragment and `summary till now` as guide. Ensure that `current_summary` captures all details of this fragment.\n",
    "- Write a one line title describing the section in `title`.\n",
    "- write a short and concise running summary (updated `summary_till_now`) which uses the provided `summary_till_now` and the current fragment of document to write a continual summary which summarizes details we have read till now including this part.\n",
    "- Generate multiple relevant questions which can be answered from this fragment of document in `questions_and_answers` as a python list of tuples within the output dictionary.\n",
    "- This is a raw unformatted document which may contain irrelevant text/syntax or other noise. Ignore what you consider noise or irrelevant.\n",
    "\n",
    "Your output should be a python dictionary only, with keys as `current_summary`, `title`, `summary_till_now` (which included this fragment details) and `questions_and_answers`.\n",
    "Expected Output python dictionary structure is given below:\n",
    "\n",
    "{out_dict}\n",
    "\n",
    "output python dictionary:\n",
    "\n",
    "\"\"\",\n",
    "        )\n",
    "    def __call__(self, long_document):\n",
    "        \n",
    "        chunks = ChunkText(long_document, self.chunk_size)\n",
    "        running_summary = \"No summary yet, this is the first fragment.\"\n",
    "        chunk_questions = []\n",
    "        chunked_summary = []\n",
    "        title = []\n",
    "        for cnk in tqdm(chunks):\n",
    "            prompt = self.prompt.format(document=cnk, summary_till_now=running_summary, out_dict=self.out_dict)\n",
    "            try:\n",
    "                resp = callGpt.get_turbo_call()(prompt, temperature=0.7)\n",
    "                resp = eval(resp)\n",
    "                print(resp)\n",
    "                chunked_summary.append(resp[\"current_summary\"])\n",
    "                chunk_questions.append(resp[\"questions_and_answers\"])\n",
    "                running_summary = resp[\"summary_till_now\"]\n",
    "                title.append(resp[\"title\"])\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                try:\n",
    "                    resp = callGpt.get_turbo_call()(prompt, temperature=0.7)\n",
    "                    resp = eval(resp)\n",
    "                    print(resp)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    resp = callGpt.get_hard_call()(prompt, temperature=0.2)\n",
    "                    resp = eval(resp)\n",
    "                    print(resp)\n",
    "                chunked_summary.append(resp[\"current_summary\"])\n",
    "                chunk_questions.append(resp[\"questions_and_answers\"])\n",
    "                running_summary = resp[\"summary_till_now\"]\n",
    "                title.append(resp[\"title\"])\n",
    "            \n",
    "        full_length_summary = \" \".join(chunked_summary)        \n",
    "        return dict(full_length_summary=full_length_summary, title=title, chunked_summary=chunked_summary, chunks=chunks, running_summary=running_summary, chunk_questions=chunk_questions)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "344044c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T09:27:46.160541Z",
     "start_time": "2023-05-24T09:27:46.123795Z"
    }
   },
   "outputs": [],
   "source": [
    "def call_api_parallel_multi_fn(api_calls, fns):\n",
    "    assert len(api_calls) == len(fns)\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        # Submit tasks and collect Future objects\n",
    "        futures = [executor.submit(fn, **api_call) for fn, api_call in zip(fns, api_calls)]\n",
    "\n",
    "        # Collect results in order of input tasks\n",
    "        results = [future.result() for future in futures]\n",
    "    return results\n",
    "\n",
    "class LongSummarizer:\n",
    "    def __init__(self, chunk_size=1000):\n",
    "        self.chunk_size=1000\n",
    "        self.name = \"LongSummarizer\"\n",
    "        self.description = \"\"\"\n",
    "LongSummarizer:\n",
    "    This tool takes a text document breaks it into chunks, then summarizes the chunks and creates chunk level and document level summary.\n",
    "\n",
    "    Input params/args: \n",
    "        long_document (str): document to summarize.\n",
    "\n",
    "    Returns: \n",
    "        str: summarized_document.\n",
    "\n",
    "    Usage:\n",
    "        `summarized_document = LongSummarizer()(text_document=\"Long document to summarize\") # Note: this tool needs to be initialized first.`\n",
    "    \"\"\"\n",
    "        self.info_dict = dict(title={\"information_request\": \"Write a one line title describing the section.\", \n",
    "                                     \"information\":\"title\"}, \n",
    "                             summary={\"information_request\": \"Write a thorough and detailed summary for the current document fragment using the fragment text and `summary_till_now` as guide. Ensure that you capture all details of this fragment.\", \n",
    "                                      \"information\":\"summary\"},\n",
    "                             summary_till_now={\"information_request\": \"Write a coherent running summary using the provided summary till now and the current fragment of document. Ensure that you capture all details of what we have read till now including this part.\", \n",
    "                                               \"information\":\"summary till now\"},\n",
    "                             questions_and_answers={\"information_request\": \"Generate multiple relevant questions and their answers which can be answered from this fragment with each question and answer in a different line. Don't do numbering of questions and answers\", \n",
    "                                                    \"information\":\"Generated Questions and Answers\"})\n",
    "        self.prompt = PromptTemplate(\n",
    "            input_variables=[\"summary_till_now\", \"document\", \"information_request\", \"information\"],\n",
    "            template=\"\"\" \n",
    "Given below text is small part/fragment text of a larger document that we are reading sequentially.\n",
    "\"{document}\"\n",
    "\n",
    "Note: This is a raw unformatted document which may contain irrelevant text/syntax or other noise. Ignore what you consider noise or irrelevant.\n",
    "\n",
    "\n",
    "You are also given a summary of what we have read till now (summary till now excluding this fragment text) from the larger document below:\n",
    "{summary_till_now}\n",
    "\n",
    "\n",
    "Based on the fragment of document and previous summary above provided you need to:\n",
    "{information_request}\n",
    "\n",
    "\n",
    "{information}:\n",
    "\n",
    "\"\"\",\n",
    "        )\n",
    "    def __call__(self, long_document):\n",
    "        \n",
    "        chunks = ChunkText(long_document, self.chunk_size)\n",
    "        running_summary = \"No summary yet, this is the first fragment.\"\n",
    "        chunk_questions = []\n",
    "        chunked_summary = []\n",
    "        title = []\n",
    "        \n",
    "        for cnk in tqdm(chunks):\n",
    "            rdict = dict()\n",
    "            keys, prompts = [], []\n",
    "            for k, v in self.info_dict.items():\n",
    "                prompt = self.prompt.format(document=cnk, summary_till_now=running_summary, \n",
    "                                            information_request=v[\"information_request\"], information=v[\"information\"])\n",
    "#                 print(prompt, \"\\n\", \"=\"*80)\n",
    "                \n",
    "                prompts.append(prompt)\n",
    "                keys.append(k)\n",
    "            calls = [{\"text\": p, \"temperature\": 0.7, \"num_tokens\": 512} for p in prompts]\n",
    "            fns = [call_ai21 if k==\"summary_till_now\" else call_ai21 for k in keys]\n",
    "            \n",
    "#             resp = call_api_parallel(calls, call_ai21)\n",
    "            resp = call_api_parallel_multi_fn(calls, fns)\n",
    "            resp = dict(zip(keys, resp))\n",
    "            pprint(resp)\n",
    "            chunked_summary.append(resp[\"summary\"])\n",
    "            qna_success = False\n",
    "            qna_resp = resp[\"questions_and_answers\"]\n",
    "            while not qna_success:\n",
    "                qna = [qa.strip() for qa in qna_resp.split(\"\\n\") if len(qa.strip()) > 0]\n",
    "                # any line with less than 3 words.\n",
    "                qna = [qa for qa in qna if len(qa.split())>2]\n",
    "                print(qna)\n",
    "                qna_success = len(qna) % 2 == 0\n",
    "                if not qna_success:\n",
    "                    qna_call = calls[keys.index(\"questions_and_answers\")]\n",
    "                    qna_resp = call_ai21(**qna_call)\n",
    "                    \n",
    "            \n",
    "            qna = concat_array_two_at_a_time(qna)\n",
    "            chunk_questions.append(qna)\n",
    "            running_summary = resp[\"summary_till_now\"]\n",
    "            title.append(resp[\"title\"])\n",
    "        full_length_summary = \" \".join(chunked_summary)        \n",
    "        return dict(full_length_summary=full_length_summary, title=title, chunked_summary=chunked_summary, chunks=chunks, running_summary=running_summary, chunk_questions=chunk_questions)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f036c81f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T18:25:28.706968Z",
     "start_time": "2023-05-21T18:25:28.595737Z"
    },
    "code_folding": [
     1,
     16,
     32
    ]
   },
   "outputs": [],
   "source": [
    "full_summary = {'full_length_summary': 'The paper proposes a novel criterion for CNN pruning based on interpretability. The criterion involves finding the most relevant units using relevance scores obtained from concepts of explainable AI. The method is evaluated on computer vision datasets and can efficiently prune CNN models in transfer-learning setups while maintaining or even improving accuracy. The paper proposes a novel pruning framework based on Layer-wise Relevance Propagation (LRP) to reduce redundancy while maintaining performance. LRP computes relevance scores for every unit of the deep model and can be used as a pruning criterion. The LRP criterion is scalable, easy to apply, and has linear computational cost similar to backpropagation. The LRP criterion outperforms previous criteria on all datasets if retraining is prohibited after pruning. The proposed pruning framework is not limited to LRP and image data, but can be used with other explanation techniques and data types. The paper proposes a novel criterion for pruning neural network units based on relevance quantity computed with LRP. The LRP-based pruning procedure involves a standard forward pass, backward propagation using LRP propagation rules, and pruning by eliminating irrelevant units w.r.t. the relevance quantity R obtained via LRP. The method is efficient and easily scalable to generic network structures, and can significantly reduce the computational complexity of deep neural networks. The fragment discusses the Layer-wise Relevance Propagation (LRP) pruning criterion and its effectiveness in the pruning of deep learning models. LRP rule works well in practice with feedforward-DNNs and ReLU activations. It performs two separate relevance propagation steps per layer, exclusively considering activatory parts of forward propagated quantities and inhibitory parts, respectively. The rule is locally conservative and provides technical advantages over gradient-based or activation-based pruning techniques. The LRP-based assessment of neuron and filter importance is performed as a single LRP backward pass through the model, with an aggregation of relevance per filter channel as described above, for convolutional layers, and does not require additional normalization or regularization.  The fragment discusses the pruning of deep learning models using the Layer-wise Relevance Propagation (LRP) criterion. The LRP-based measure consistently outperforms all other criteria in all reference set sizes and datasets. The results show that LRP is most capable of preserving the relevant core of the learned network function and dismissing unimportant parts of the model during pruning. The fragment discusses the similarity analysis of neuron selection between LRP and other criteria, computed over different random seeds. The consistency of LRP in selecting important neural network units for pruning and preservation is high compared to other methods, while remaining adaptive to the selection of reference samples for criterion computation. The self-similarity in neuron selection in the extremes of the ranking across random seeds for all criteria and toy settings is reported in Table 2 and elucidates that LRP constitutes an orthogonal pruning criterion. The fragment discusses the complexity and architecture of various pre-trained deep CNN models such as VGG-16, AlexNet, ResNet-18, and ResNet-50. It further explains the process of canonization which fuses a sequence of a convolution and a BN layer into a convolution layer with updated weights to make LRP scores implementation-invariant. The experiments were performed on six public datasets and the pruning criteria were evaluated on five different image benchmark datasets. The results were reported in Table 3 and Figure 5. The document explains the results of applying the proposed LRP-based pruning criteria to VGG-16 and ResNet-50 models. The LRP-based criterion outperforms other criteria in most cases, achieving higher test accuracies, and is stable and independent of the chosen dataset. The number of FLOPs depends on the location where pruning is applied within the network. The LRP and weight criteria focus the pruning on upper layers closer to the model output, whereas the Taylor and Gradient criteria focus more on the lower layers. The fragment describes experiments on pruning convolutional and fully connected layers in VGG-16 and ResNets for a domain adaptation task from ImageNet to the Cats & Dogs dataset. Layer-wise Relevance Propagation (LRP) is used as a pruning criterion and is compared to weight, Taylor, and gradient with l2-norm criteria. LRP is capable of pruning any layer type efficiently, and as the pruning ratio increases, LRP-based pruning criterion can keep the test accuracy stable, even without fine-tuning. The fragment describes experiments on pruning convolutional and fully connected layers in VGG-16 and ResNets for a domain adaptation task from ImageNet to the Cats & Dogs dataset. Layer-wise Relevance Propagation (LRP) is used as a pruning criterion and is compared to weight, Taylor, and gradient with l2-norm criteria. LRP is capable of pruning any layer type efficiently, and as the pruning ratio increases, LRP-based pruning criterion can keep the test accuracy stable, even without fine-tuning. The paper proposes a new criterion for the iterative pruning of CNNs based on the explanation method LRP. Removing units according to low LRP scores means discarding all aspects in the model that do not contribute relevance to its decision making. LRP is scalable and computationally cheap. In experiments, LRP criterion has shown favorable compression performance on a variety of datasets both with and without retraining after pruning. The same method can be used to visually interpret the model and explain individual decisions as intuitive relevance heatmaps. The fragment cites multiple methods for compressing deep neural networks, including pruning, distillation, and low-rank and sparse decomposition. It also mentions several papers that propose novel pruning techniques, such as Layer-wise Relevance Propagation (LRP), which consistently outperforms other criteria in selecting important neural network units for pruning and preservation. The experiments show that LRP-based pruning criterion can keep the test accuracy stable even without fine-tuning, as the pruning ratio increases. The fragment discusses the consistency of LRP-based neuron selection across reference sample sizes for a low number of reference samples. The results show that the selected set of neurons remains consistent for larger sets of reference samples and that low numbers of reference samples are sufficient for consistently pruning toy models. The fragment discusses the consistency of LRP-based neuron selection across reference sample sizes for a low number of reference samples. The results show that the selected set of neurons remains consistent for larger sets of reference samples and that low numbers of reference samples are sufficient for consistently pruning toy models. The fragment discusses the comparison of pruning performance without a subsequent finetuning step for a ResNet-50 network when pruned by criteria using weights, gradient, Taylor-expansion and LRP. The results show that LRP-based pruning consistently prunes the largest fraction of filters before starting to lose prediction accuracy compared to other methods. The paper suggests measuring dataset complexities with respect to a neural network by the area under the accuracy graph with respect to the amount of pruned filters.',\n",
    " 'chunked_summary': ['The paper proposes a novel criterion for CNN pruning based on interpretability. The criterion involves finding the most relevant units using relevance scores obtained from concepts of explainable AI. The method is evaluated on computer vision datasets and can efficiently prune CNN models in transfer-learning setups while maintaining or even improving accuracy.',\n",
    "  'The paper proposes a novel pruning framework based on Layer-wise Relevance Propagation (LRP) to reduce redundancy while maintaining performance. LRP computes relevance scores for every unit of the deep model and can be used as a pruning criterion. The LRP criterion is scalable, easy to apply, and has linear computational cost similar to backpropagation. The LRP criterion outperforms previous criteria on all datasets if retraining is prohibited after pruning. The proposed pruning framework is not limited to LRP and image data, but can be used with other explanation techniques and data types.',\n",
    "  'The paper proposes a novel criterion for pruning neural network units based on relevance quantity computed with LRP. The LRP-based pruning procedure involves a standard forward pass, backward propagation using LRP propagation rules, and pruning by eliminating irrelevant units w.r.t. the relevance quantity R obtained via LRP. The method is efficient and easily scalable to generic network structures, and can significantly reduce the computational complexity of deep neural networks.',\n",
    "  'The fragment discusses the Layer-wise Relevance Propagation (LRP) pruning criterion and its effectiveness in the pruning of deep learning models. LRP rule works well in practice with feedforward-DNNs and ReLU activations. It performs two separate relevance propagation steps per layer, exclusively considering activatory parts of forward propagated quantities and inhibitory parts, respectively. The rule is locally conservative and provides technical advantages over gradient-based or activation-based pruning techniques. The LRP-based assessment of neuron and filter importance is performed as a single LRP backward pass through the model, with an aggregation of relevance per filter channel as described above, for convolutional layers, and does not require additional normalization or regularization. ',\n",
    "  'The fragment discusses the pruning of deep learning models using the Layer-wise Relevance Propagation (LRP) criterion. The LRP-based measure consistently outperforms all other criteria in all reference set sizes and datasets. The results show that LRP is most capable of preserving the relevant core of the learned network function and dismissing unimportant parts of the model during pruning.',\n",
    "  'The fragment discusses the similarity analysis of neuron selection between LRP and other criteria, computed over different random seeds. The consistency of LRP in selecting important neural network units for pruning and preservation is high compared to other methods, while remaining adaptive to the selection of reference samples for criterion computation. The self-similarity in neuron selection in the extremes of the ranking across random seeds for all criteria and toy settings is reported in Table 2 and elucidates that LRP constitutes an orthogonal pruning criterion.',\n",
    "  'The fragment discusses the complexity and architecture of various pre-trained deep CNN models such as VGG-16, AlexNet, ResNet-18, and ResNet-50. It further explains the process of canonization which fuses a sequence of a convolution and a BN layer into a convolution layer with updated weights to make LRP scores implementation-invariant. The experiments were performed on six public datasets and the pruning criteria were evaluated on five different image benchmark datasets. The results were reported in Table 3 and Figure 5.',\n",
    "  'The document explains the results of applying the proposed LRP-based pruning criteria to VGG-16 and ResNet-50 models. The LRP-based criterion outperforms other criteria in most cases, achieving higher test accuracies, and is stable and independent of the chosen dataset. The number of FLOPs depends on the location where pruning is applied within the network. The LRP and weight criteria focus the pruning on upper layers closer to the model output, whereas the Taylor and Gradient criteria focus more on the lower layers.',\n",
    "  'The fragment describes experiments on pruning convolutional and fully connected layers in VGG-16 and ResNets for a domain adaptation task from ImageNet to the Cats & Dogs dataset. Layer-wise Relevance Propagation (LRP) is used as a pruning criterion and is compared to weight, Taylor, and gradient with l2-norm criteria. LRP is capable of pruning any layer type efficiently, and as the pruning ratio increases, LRP-based pruning criterion can keep the test accuracy stable, even without fine-tuning.',\n",
    "  'The fragment describes experiments on pruning convolutional and fully connected layers in VGG-16 and ResNets for a domain adaptation task from ImageNet to the Cats & Dogs dataset. Layer-wise Relevance Propagation (LRP) is used as a pruning criterion and is compared to weight, Taylor, and gradient with l2-norm criteria. LRP is capable of pruning any layer type efficiently, and as the pruning ratio increases, LRP-based pruning criterion can keep the test accuracy stable, even without fine-tuning.',\n",
    "  'The paper proposes a new criterion for the iterative pruning of CNNs based on the explanation method LRP. Removing units according to low LRP scores means discarding all aspects in the model that do not contribute relevance to its decision making. LRP is scalable and computationally cheap. In experiments, LRP criterion has shown favorable compression performance on a variety of datasets both with and without retraining after pruning. The same method can be used to visually interpret the model and explain individual decisions as intuitive relevance heatmaps.',\n",
    "  'The fragment cites multiple methods for compressing deep neural networks, including pruning, distillation, and low-rank and sparse decomposition. It also mentions several papers that propose novel pruning techniques, such as Layer-wise Relevance Propagation (LRP), which consistently outperforms other criteria in selecting important neural network units for pruning and preservation. The experiments show that LRP-based pruning criterion can keep the test accuracy stable even without fine-tuning, as the pruning ratio increases.',\n",
    "  'The fragment discusses the consistency of LRP-based neuron selection across reference sample sizes for a low number of reference samples. The results show that the selected set of neurons remains consistent for larger sets of reference samples and that low numbers of reference samples are sufficient for consistently pruning toy models.',\n",
    "  'The fragment discusses the consistency of LRP-based neuron selection across reference sample sizes for a low number of reference samples. The results show that the selected set of neurons remains consistent for larger sets of reference samples and that low numbers of reference samples are sufficient for consistently pruning toy models.',\n",
    "  'The fragment discusses the comparison of pruning performance without a subsequent finetuning step for a ResNet-50 network when pruned by criteria using weights, gradient, Taylor-expansion and LRP. The results show that LRP-based pruning consistently prunes the largest fraction of filters before starting to lose prediction accuracy compared to other methods. The paper suggests measuring dataset complexities with respect to a neural network by the area under the accuracy graph with respect to the amount of pruned filters.'],\n",
    " 'chunks': ['Pruning by Explaining: A Novel Criterion for\\nDeep Neural Network Pruning\\nSeul-Ki Yeoma,i, Philipp Seegerera,h, Sebastian Lapuschkinc, Alexander Binderd,e,\\nSimon Wiedemannc, Klaus-Robert M¨ullera,f,g,b,∗, Wojciech Samekc,b,∗\\naMachine Learning Group, Technische Universit¨at Berlin, 10587 Berlin, Germany\\nbBIFOLD – Berlin Institute for the Foundations of Learning and Data, Berlin, Germany\\ncDepartment of Artiﬁcial Intelligence, Fraunhofer Heinrich Hertz Institute, 10587 Berlin, Germany\\ndISTD Pillar, Singapore University of Technology and Design, Singapore 487372, Singapore\\neDepartment of Informatics, University of Oslo, 0373 Oslo, Norway\\nfDepartment of Artiﬁcial Intelligence, Korea University, Seoul 136-713, Korea\\ngMax Planck Institut f¨ur Informatik, 66123 Saarbr¨ucken, Germany\\nhAignostics GmbH, 10557 Berlin, Germany\\niNota AI GmbH, 10117 Berlin, Germany\\nAbstract\\nThe success of convolutional neural networks (CNNs) in various applications is accompanied by a\\nsigniﬁcant increase in computation and parameter storage costs. Recent eﬀorts to reduce these over-\\nheads involve pruning and compressing the weights of various layers while at the same time aiming\\nto not sacriﬁce performance. In this paper, we propose a novel criterion for CNN pruning inspired\\nby neural network interpretability: The most relevant units, i.e. weights or ﬁlters, are automatically\\nfound using their relevance scores obtained from concepts of explainable AI (XAI). By exploring\\nthis idea, we connect the lines of interpretability and model compression research. We show that our\\nproposed method can eﬃciently prune CNN models in transfer-learning setups in which networks\\npre-trained on large corpora are adapted to specialized tasks. The method is evaluated on a broad\\nrange of computer vision datasets. Notably, our novel criterion is not only competitive or better\\ncompared to state-of-the-art pruning criteria when successive retraining is performed, but clearly\\noutperforms these previous criteria in the resource-constrained application scenario in which the\\ndata of the task to be transferred to is very scarce and one chooses to refrain from ﬁne-tuning. Our\\nmethod is able to compress the model iteratively while maintaining or even improving accuracy. At\\nthe same time, it has a computational cost in the order of gradient computation and is comparatively\\nsimple to apply without the need for tuning hyperparameters for pruning.\\nKeywords: Pruning, Layer-wise Relevance Propagation (LRP), Convolutional Neural Network\\n(CNN), Interpretation of Models, Explainable AI (XAI)\\n∗Corresponding Authors\\nEmail addresses: yeom@tu-berlin.de (Seul-Ki Yeom), philipp.seegerer@tu-berlin.de (Philipp\\nSeegerer), sebastian.lapuschkin@hhi.fraunhofer.de (Sebastian Lapuschkin), alexabin@uio.no (Alexander\\nBinder), simon.wiedemann@hhi.fraunhofer.de (Simon Wiedemann),\\nklaus-robert.mueller@tu-berlin.de (Klaus-Robert M¨uller), wojciech.samek@hhi.fraunhofer.de\\n(Wojciech Samek)\\nArticle published in Pattern Recognition 115. doi:10.1016/j.patcog.2021.107899\\nMarch 15, 2021\\narXiv:1912.08881v3  [cs.LG]  12 Mar 2021\\n 1. Introduction\\nDeep CNNs have become an indispensable tool for a wide range of applications [1], such as\\nimage classiﬁcation, speech recognition, natural language processing, chemistry, neuroscience,\\nmedicine and even are applied for playing games such as Go, poker or Super Smash Bros. They\\nhave achieved high predictive performance, at times even outperforming humans. Furthermore, in\\nspecialized domains where limited training data is available, e.g., due to the cost and diﬃculty of\\ndata generation (medical imaging from fMRI, EEG, PET etc.), transfer learning can improve the\\nCNN performance by extracting the knowledge from the source tasks and applying it to a target\\ntask which has limited training data.\\nHowever, the high predictive performance of CNNs often comes at the expense of high storage\\nand computational costs, which are related to the energy expenditure of the ﬁne-tuned network.\\nThese deep architectures are composed of millions of parameters to be trained, leading to overpa-\\nrameterization (i.e. having more parameters than training samples) of the model [2]. The run-times\\nare typically dominated by the evaluation of convolutional layers, while dense layers are cheap but\\nmemory-heavy [3]. For instance, the VGG-16 model has approximately 138 million parameters,\\ntaking up more than 500MB in storage space, and needs 15.5 billion ﬂoating-point operations\\n(FLOPs) to classify a single image. ResNet50 has approx. 23 million parameters and needs 4.1\\nbillion FLOPs. Note that overparametrization is helpful for an eﬃcient and successful training of\\nneural networks, however, once the trained and well generalizing network structure is established,\\npruning can help to reduce redundancy while still maintaining good performance [4].\\nReducing a model’s storage requirements and computational cost becomes critical for a broader\\napplicability, e.g., in embedded systems, autonomous agents, mobile devices, or edge devices [5].\\nNeural network pruning has a decades long history with interest from both academia and industry [6]\\naiming to eliminate the subset of network units (i.e. weights or ﬁlters) which is the least important\\nw.r.t. the network’s intended task. For network pruning, it is crucial to decide how to identify the\\n“irrelevant” subset of the parameters meant for deletion. To address this issue, previous researches\\nhave proposed speciﬁc criteria based on Taylor expansion, weight, gradient, and others, to reduce\\ncomplexity and computation costs in the network. Related works are introduced in Section 2.\\nFrom a practical point of view, the full capacity (in terms of weights and ﬁlters) of an overpa-\\nrameterized model may not be required, e.g., when (1) parts of the model lie dormant after training\\n(i.e., are permanently ”switched oﬀ”), (2) a user is not interested in the model’s full array of possible\\noutputs, which is a common scenario in transfer learning (e.g. the user only has use for 2 out of 10\\navailable network outputs), or (3) a user lacks data and resources for ﬁne-tuning and running the\\noverparameterized model.\\nIn these scenarios the redundant parts of the model will still occupy space in memory, and\\ninformation will be propagated through those parts, consuming energy and increasing runtime.\\nThus, criteria able to stably and signiﬁcantly reduce the computational complexity of deep neural\\nnetworks across applications are relevant for practitioners.\\nIn this paper, we propose a novel pruning framework based on Layer-wise Relevance Propagation\\n(LRP) [7]. LRP was originally developed as an explanation method to assign importance scores, so\\ncalled relevance, to the diﬀerent input dimensions of a neural network that reﬂect the contribution\\nof an input dimension to the models decision, and has been',\n",
    "  'this paper, we propose a novel pruning framework based on Layer-wise Relevance Propagation\\n(LRP) [7]. LRP was originally developed as an explanation method to assign importance scores, so\\ncalled relevance, to the diﬀerent input dimensions of a neural network that reﬂect the contribution\\nof an input dimension to the models decision, and has been applied to diﬀerent ﬁelds of computer\\nvision (e.g., [8, 9, 10]). The relevance is backpropagated from the output to the input and hereby\\nassigned to each unit of the deep model. Since relevance scores are computed for every layer\\n2\\n and neuron from the model output to the input, these relevance scores essentially reﬂect the\\nimportance of every single unit of a model and its contribution to the information ﬂow through the\\nnetwork — a natural candidate to be used as pruning criterion. The LRP criterion can be motivated\\ntheoretically through the concept of Deep Taylor Decomposition (DTD) (c.f. [11, 12, 13]). Moreover,\\nLRP is scalable and easy to apply, and has been implemented in software frameworks such as\\niNNvestigate [14]. Furthermore, it has linear computational cost in terms of network inference cost,\\nsimilar to backpropagation.\\nWe systematically evaluate the compression eﬃcacy of the LRP criterion compared to common\\npruning criteria for two diﬀerent scenarios.\\nScenario 1: We prune pre-trained CNNs followed by subsequent ﬁne-tuning. This is the usual\\nsetting in CNN pruning and requires a suﬃcient amount of data and computational power.\\nScenario 2: In this scenario a pretrained model needs to be transferred to a related problem as\\nwell, but the data available for the new task is too scarce for a proper ﬁne-tuning and/or the time\\nconsumption, computational power or energy consumption is constrained. Such transfer learning\\nwith restrictions is common in mobile or embedded applications.\\nOur experimental results on various benchmark datasets and four diﬀerent popular CNN architec-\\ntures show that the LRP criterion for pruning is more scalable and eﬃcient, and leads to better\\nperformance than existing criteria regardless of data types and model architectures if retraining\\nis performed (Scenario 1). Especially, if retraining is prohibited due to external constraints after\\npruning, the LRP criterion clearly outperforms previous criteria on all datasets (Scenario 2). Finally,\\nwe would like to note that our proposed pruning framework is not limited to LRP and image data,\\nbut can be also used with other explanation techniques and data types.\\nThe rest of this paper is organized as follows: Section 2 summarizes related works for network\\ncompression and introduces the typical criteria for network pruning. Section 3 describes the\\nframework and details of our approach. The experimental results are illustrated and discussed in\\nSection 4, while our approach is discussed in relation to previous studies in Section 5. Section 6\\ngives conclusions and an outlook to future work.\\n2. Related Work\\nWe start the discussion of related research in the ﬁeld of network compression with network\\nquantization methods which have been proposed for storage space compression by decreasing the\\nnumber of possible and unique values for the parameters [15, 16]. Tensor decomposition approaches\\ndecompose network matrices into several smaller ones to estimate the informative parameters of the\\ndeep CNNs with low-rank approximation/factorization [17].\\nMore recently, [18] also propose a framework of architecture distillation based on layer-wise\\nreplacement, called LightweightNet for memory and time saving. Algorithms for designing eﬃcient\\nmodels focus more on acceleration instead of compression by optimizing convolution operations or\\narchitectures directly (e.g. [19]).\\nNetwork pruning approaches remove redundant or irrelevant units — i.e. nodes, ﬁlters, or layers\\n— from the model which are not critical for performance [6, 20]. Network pruning is robust to\\nvarious settings and gives reasonable compression rates while not (or minimally) hurting the model\\naccuracy. Also it can support both training from scratch and transfer learning from pre-trained\\nmodels. Early works have shown that network pruning is eﬀective in reducing network complexity\\n3\\n and simultaneously addressing over-ﬁtting problems. Current network pruning techniques make\\nweights or channels sparse by removing non-informative connections and require an appropriate\\ncriterion for identifying which units of the model are not relevant for solving a problem. Thus, it\\nis crucial to decide how to quantify the relevance of the parameters (i.e. weights or channels) in\\nthe current state of the learning process for deletion without sacriﬁcing predictive performance. In\\nprevious studies, pruning criteria have been proposed based on the magnitude of their 1) weights, 2)\\ngradients, 3) Taylor expansion/derivative, and 4) other criteria, as described in the following section.\\nTaylor expansion: Early approaches towards neural network pruning — optimal brain dam-\\nage [4] and optimal brain surgeon [21] — leveraged a second-order Taylor expansion based on the\\nHessian matrix of the loss function to select parameters for deletion. However, computing the inverse\\nof Hessian is computationally expensive. The work of [22, 23] used a ﬁrst-order Taylor expansion as\\na criterion to approximate the change of loss in the objective function as an eﬀect of pruning away\\nnetwork units. We contrast our novel criterion to the computationally more comparable ﬁrst-order\\nTaylor expansion from [22].\\nGradient: Liu and Wu [24] proposed a hierarchical global pruning strategy by calculating the\\nmean gradient of feature maps in each layer. They adopt a hierarchical global pruning strategy\\nbetween the layers with similar sensitivity. Sun et al. [25] proposes a sparsiﬁed back-propagation\\napproach for neural network training using the magnitude of the gradient to ﬁnd essential and\\nnon-essential features in Multi-Layer Perceptron (MLP) and Long Short-Term Memory Network\\n(LSTM) models, which can be used for pruning. We implement the gradient-based pruning criterion\\nafter [25].\\nWeight: A recent trend is to prune redundant, non-informative weights in pre-trained CNN mod-\\nels, based on the magnitude of the weights themselves. Han et al. [26] and Han et al. [27] proposed\\nthe pruning of weights for which the magnitude is below a certain threshold, and to subsequently\\nﬁne-tune with a lp-norm regularization. This pruning strategy has been used on fully-connected\\nlayers and introduced sparse connections with BLAS libraries, supporting specialized hardware\\nto achieve its acceleration. In the same context, Structured Sparsity Learning (SSL) added group\\nsparsity regularization to penalize unimportant parameters by removing some weights [28]. Li et al.\\n[29], against which we compare in our experiments, proposed a one-shot channel pruning method\\nusing the lp norm of weights for ﬁlter selection, provided that those channels with smaller weights\\nalways produce weaker activations.\\nOther criteria: [30] proposed the Neuron Importance Score Propagation (NISP) algorithm to\\npropagate the importance scores of ﬁnal responses before the softmax, classiﬁcation layer in the\\nnetwork. The method is based on — in contrast to our proposed metric — a per-layer pruning\\nprocess which does not consider global importance in the network. Luo et al. [31] proposed ThiNet,\\na data-driven statistical channel pruning technique based on the statistics computed from the next\\nlayer. Further hybrid approaches can be found in, e.g.',\n",
    "  'classiﬁcation layer in the\\nnetwork. The method is based on — in contrast to our proposed metric — a per-layer pruning\\nprocess which does not consider global importance in the network. Luo et al. [31] proposed ThiNet,\\na data-driven statistical channel pruning technique based on the statistics computed from the next\\nlayer. Further hybrid approaches can be found in, e.g. [32], which suggests a fusion approach to\\ncombine with weight-based channel pruning and network quantization. More recently, Dai et al.\\n[33] proposed an evolutionary paradigm for weight-based pruning and gradient-based growing to\\nreduce the network heuristically.\\n3. LRP-Based Network Pruning\\nA feedforward CNN consists of neurons established in a sequence of multiple layers, where\\neach neuron receives the input data from one or more previous layers and propagates its output to\\n4\\n every neuron in the succeeding layers, using a potentially non-linear mapping. Network pruning\\naims to sparsify these units by eliminating weights or ﬁlters that are non-informative (according to a\\ncertain criterion). We speciﬁcally focus our experiments on transfer learning, where the parameters\\nof a network pre-trained on a source domain is subsequently ﬁne-tuned on a target domain, i.e., the\\nﬁnal data or prediction task. Here, the general pruning procedure is outlined in Algorithm 1.\\nAlgorithm 1 Neural Network Pruning\\n2:1: Input: pre-trained model net, reference data xr, training data xt\\npruning threshold t, pruning criterion c, pruning ratio r\\n4:3: while t not reached do\\n// Step 1: assess network substructure importance\\n5:\\nfor all layer in net do\\n6:\\nfor all units in layer do\\n7:\\n▷ compute importance of unit w.r.t. c (and xr)\\n8:\\nend for\\n9:\\nif required for c then\\n10:\\n▷ globally regularize importance per unit\\n11:\\nend if\\n12:\\nend for\\n13:\\n// Step 2: identify and remove least important units in groups of r\\n14:\\n▷ remove r units from net where importance is minimal\\n15:\\n▷ remove orphaned connections of each removed unit\\n16:\\nif desired then\\n17:\\n// Step 2.1: optional ﬁne-tuning to recover performance\\n18:\\n▷ ﬁne-tune net on xt\\n19:\\nend if\\n20: end while\\n21: // return the pruned network upon hitting threshold t (e.g. model performance or size)\\n22: return net\\nEven though most approaches use an identical process, choosing a suitable pruning criterion to\\nquantify the importance of model parameters for deletion while minimizing performance drop (Step\\n1) is of critical importance, governing the success of the approach.\\n3.1. Layer-wise Relevance Propagation\\nIn this paper, we propose a novel criterion for pruning neural network units: the relevance\\nquantity computed with LRP [7]. LRP decomposes a classiﬁcation decision into proportionate\\ncontributions of each network unit to the overall classiﬁcation score, called “relevances”. When\\ncomputed for the input dimensions of a CNN and visualized as a heatmap, these relevances highlight\\nparts of the input that are important for the classiﬁcation decision. LRP thus originally served as a\\ntool for interpreting non-linear learning machines and has been applied as such in various ﬁelds,\\namongst others for general image recognition, medical imaging and natural language processing,\\ncf. [34]. The direct linkage of the relevances to the classiﬁer output, as well as the conservativity\\nconstraint imposed on the propagation of relevance between layers, makes LRP not only attractive\\nfor model explaining, but can also naturally serve as pruning criterion (see Section 4.1).\\n5\\n relevance\\nlow \\nrelevance\\nhigh \\nactivation\\nstrong\\nactivation\\nweak\\nFigure 1: Illustration of LRP-based sequential process for pruning. A. Forward propagation of a given image (i.e.\\ncat) through a pre-trained model. B. Evaluation on relevance for weights/ﬁlters using LRP, C. Iterative pruning by\\neliminating the least relevant units (depicted by circles) and ﬁne-tuning if necessary. The units can be individual neurons,\\nﬁlters, or other arbitrary grouping of parameters, depending on the model architecture.\\nThe main characteristic of LRP is a backward pass through the network during which the\\nnetwork output is redistributed to all units of the network in a layer-by-layer fashion. This backward\\npass is structurally similar to gradient backpropagation and has therefore a similar runtime. The\\nredistribution is based on a conservation principle such that the relevances can immediately be\\ninterpreted as the contribution that a unit makes to the network output, hence establishing a direct\\nconnection to the network output and thus its predictive performance. Therefore, as a pruning\\ncriterion, the method is eﬃcient and easily scalable to generic network structures. Independent of\\nthe type of neural network layer — that is pooling, fully-connected, convolutional layers — LRP\\nallows to quantify the importance of units throughout the network, given a global prediction context.\\n3.2. LRP-based Pruning\\nThe procedure of LRP-based pruning is summarized in Figure 1. In the ﬁrst phase, a standard\\nforward pass is performed by the network and the activations at each layer are collected. In the\\nsecond phase, the score f(x) obtained at the output of the network is propagated backwards through\\nthe network according to LRP propagation rules [7]. In the third phase, the current model is pruned\\nby eliminating the irrelevant (w.r.t. the “relevance” quantity R obtained via LRP) units and is\\n(optionally) further ﬁne-tuned.\\nLRP is based on a layer-wise conservation principle that allows the propagated quantity (e.g.\\nrelevance for a predicted class) to be preserved between neurons of two adjacent layers. Let R(l)\\ni\\nbe the relevance of neuron i at layer l and R(l+1)\\nj\\nbe the relevance of neuron j at the next layer l + 1.\\nStricter deﬁnitions of conservation that involve only subsets of neurons can further impose that\\n6\\n relevance is locally redistributed in the lower layers and we deﬁne R(l)\\ni← j as the share of R(l+1)\\nj\\nthat is\\nredistributed to neuron i in the lower layer. The conservation property always satisﬁes\\n�\\ni\\nR(l)\\ni←j = R(l+1)\\nj\\n,\\n(1)\\nwhere the sum runs over all neurons i of the (during inference) preceeding layer l. When using\\nrelevance as a pruning criterion, this property helps to preserve its quantity layer-by-layer, regardless\\nof hidden layer size and the number of iteratively pruned neurons for each layer. At each layer l, we\\ncan extract node i’s global importance as its attributed relevance R(l)\\ni .\\nIn this paper, we speciﬁcally adopt relevance quantities computed with the LRP-α1β0-rule as\\npruning criterion. The LRP-αβ-rule was developed with feedforward-DNNs with ReLU activations\\nin mind and assumes positive (pre-softmax) logit activations flogit(x) > 0 for decomposition. The\\nrule has been shown to work well in practice in such a setting [35]. This particular variant of\\nLRP is tightly rooted in DTD',\n",
    "  'as\\npruning criterion. The LRP-αβ-rule was developed with feedforward-DNNs with ReLU activations\\nin mind and assumes positive (pre-softmax) logit activations flogit(x) > 0 for decomposition. The\\nrule has been shown to work well in practice in such a setting [35]. This particular variant of\\nLRP is tightly rooted in DTD [11], and other than the criteria based on network derivatives we\\ncompare against [25, 22], always produces continuous explanations, even if backpropagation is\\nperformed through the discontinuous (and commonly used) ReLU nonlinearity [12]. When used as\\na criterion for pruning, its assessment of network unit importance will change less abruptly with\\n(small) changes in the choice of reference samples, compared to gradient-based criteria.\\nThe propagation rule performs two separate relevance propagation steps per layer: one exclu-\\nsively considering activatory parts of the forward propagated quantities (i.e. all a(l)\\ni wi j > 0) and\\nanother only processing the inhibitory parts (a(l)\\ni wi j < 0) which are subsequently merged in a sum\\nwith components weighted by α and β (s.t. α + β = 1) respectively.\\nBy selecting α = 1, the propagation rule simpliﬁes to\\nR(l)\\ni =\\n�\\nj\\n�\\na(l)\\ni wi j\\n�+\\n�\\ni′\\n�\\nai′(l)wi′ j\\n�+R(l+1)\\nj\\n,\\n(2)\\nwhere R(l)\\ni denotes relevance attributed to the ith neuron at layer l, as an aggregation of downward-\\npropagated relevance messages R(l,l+1)\\ni←j . The terms (·)+ indicate the positive part of the forward\\npropagated pre-activation from layer l, to layer (l + 1). The i′ is a running index over all input\\nactivations a. Note that a choice of α = 1 only decomposes w.r.t. the parts of the inference signal\\nsupporting the model decision for the class of interest.\\nEquation (2) is locally conservative, i.e. no quantity of relevance gets lost or injected during\\nthe distribution of Rj where each term of the sum corresponds to a relevance message Rj←k. For\\nthis reason, LRP has the following technical advantages over other pruning techniques such as\\ngradient-based or activation-based methods: (1) Localized relevance conservation implicitly ensures\\nlayer-wise regularized global redistribution of importances from each network unit. (2) By summing\\nrelevance within each (convolutional) ﬁlter channel, the LRP-based criterion is directly applicable as\\na measure of total relevance per node/ﬁlter, without requiring a post-hoc layer-wise renormalization,\\ne.g., via lp norm. (3) The use of relevance scores is not restricted to a global application of pruning\\nbut can be easily applied to locally and (neuron- or ﬁlter-)group-wise constrained pruning without\\nregularization. Diﬀerent strategies for selecting (sub-)parts of the model might still be considered,\\ne.g., applying diﬀerent weightings/priorities for pruning diﬀerent parts of the model: Should the\\naim of pruning be the reduction of FLOPs required during inference, one would prefer to focus on\\n7\\n primarily pruning units of the convolutional layers. In case the aim is a reduction of the memory\\nrequirement, pruning should focus on the fully-connected layers instead.\\nIn the context of Algorithm 1, Step 1 of the LRP-based assessment of neuron and ﬁlter im-\\nportance is performed as a single LRP backward pass through the model, with an aggregation of\\nrelevance per ﬁlter channel as described above, for convolutional layers, and does not require addi-\\ntional normalization or regularization. We would like to point out that instead of backpropagating\\nthe model output fc(x) for the true class c of any given sample x (as it is commonly done when\\nLRP is used for explaining a prediction [7, 8]), we initialize the algorithm with R(L)\\nc\\n= 1 at the\\noutput layer L. We thus gain robustness against the model’s (in)conﬁdence in its predictions on\\nthe previously unseen reference samples x and ensure an equal weighting of the inﬂuence of all\\nreference samples in the identiﬁcation of relevant neural pathways.\\n4. Experiments\\nWe start by an attempt to intuitively illuminate the properties of diﬀerent pruning criteria,\\nnamely, weight magnitude, Taylor, gradient and LRP, via a series of toy datasets. We then show the\\neﬀectiveness of the LRP criterion for pruning on widely-used image recognition benchmark datasets\\n— i.e. the Scene 15 [36], Event 8 [37], Cats & Dogs [38], Oxford Flower 102 [39], CIFAR-101, and\\nILSVRC 2012 [40] datasets — and four pre-trained feed-forward deep neural network architectures,\\nAlexNet and VGG-16 with only a single sequence of layers, and ResNet-18 and ResNet-50 [41],\\nwhich both contain multiple parallel branches of layers and skip connections.\\nThe ﬁrst scenario focuses speciﬁcally on pruning of pre-trained CNNs with subsequent ﬁne-\\ntuning, as it is common in pruning research [22]. We compare our method with several state-of-the-\\nart criteria to demonstrate the eﬀectiveness of LRP as a pruning criterion in CNNs. In the second\\nscenario, we tested whether the proposed pruning criterion also works well if only a very limited\\nnumber of samples is available for pruning the model. This is relevant in case of devices with limited\\ncomputational power, energy and storage such as mobile devices or embedded applications.\\n4.1. Pruning Toy Models\\nFirst, we systematically compare the properties and eﬀectiveness of the diﬀerent pruning criteria\\non several toy datasets in order to foster an intuition about the properties of all approaches, in\\na controllable and computationally inexpensive setting. To this end we evaluate all four criteria\\non diﬀerent toy data distributions qualitatively and quantitatively. We generated three k-class toy\\ndatasets (“moon” (k = 2), “circle” (k = 2) and “multi” (k = 4)), using respective generator\\nfunctions2,3.\\nEach generated 2D dataset consists of 1000 training samples per class. We constructed and\\ntrained the models as a sequence of three consecutive ReLU-activated dense layers with 1000\\nhidden neurons each. After the ﬁrst linear layer, we have added a DropOut layer with a dropout\\nprobability of 50%. The model receives inputs from R2 and has — depending on the toy problem\\nset — k ∈ {2, 4} output neurons:\\n1https://www.cs.toronto.edu/~kriz/cifar.html\\n2https://scikit-learn.org/stable/datasets\\n3https://github.com/seulkiyeom/LRP_Pruning_toy_example\\n8\\n Dense(1000) -> ReLU -> DropOut(0.5) -> Dense(1000) ->\\n-> ReLU -> Dense(1000) -> ReLU -> Dense(k)\\nWe then sample a number of new datapoints (unseen during training) for the computation of the\\npruning criteria. During pruning, we removed a ﬁxed number of 1000 of the 3000 hidden neurons\\nthat have the least relevance for prediction according to each criterion. This is equivalent to removing\\n1000 learned',\n",
    "  'ReLU -> Dense(1000) -> ReLU -> Dense(k)\\nWe then sample a number of new datapoints (unseen during training) for the computation of the\\npruning criteria. During pruning, we removed a ﬁxed number of 1000 of the 3000 hidden neurons\\nthat have the least relevance for prediction according to each criterion. This is equivalent to removing\\n1000 learned (yet insigniﬁcant, according to the criterion) ﬁlters from the model. After pruning,\\nwe observed the changes in the decision boundaries and re-evaluated for classiﬁcation accuracy\\nusing the original training samples and re-sampled datapoints across criteria. This experiment is\\nperformed with n ∈ [1, 2,5, 10, 20, 50, 100, 200] reference samples for testing and the computation\\nof pruning criteria. Each setting is repeated 50 times, using the same set of random seeds (depending\\non the repetition index) for each n across all pruning criteria to uphold comparability.\\nFigure 2 shows the data distributions of the generated toy datasets, an exemplary set of n = 5\\nsamples generated for criteria computation, as well as the qualitative impact to the models’ decision\\nboundary when removing a ﬁxed set of 1000 neurons as selected via the compared criteria. Figure 3\\ninvestigates how the pruning criteria preserve the models’ problem solving capabilities as a function\\nof the number of samples selected for computing the criteria. Figure 4 then quantitatively summarizes\\nthe results for speciﬁc numbers of unseen samples (n ∈ [1, 5, 20, 100]) for computing the criteria.\\nHere we report the model accuracy on the training set in order to relate the preservation of the\\ndecision function as learned from data between unpruned (2nd column) to pruned models and\\npruning criteria (remaining columns).\\nFigure 2: Qualitative comparison of the impact of the pruning criteria on the decision function on three toy datasets. 1st\\ncolumn: scatter plot of the training data and decision boundary of the trained model, 2nd column: data samples randomly\\nselected for computing the pruning criteria, 3rd to 6th columns: changed decision boundaries after the application of\\npruning w.r.t. diﬀerent criteria.\\n9\\n performance after pruning in %\\ncircle dataset\\nmoon dataset\\nmulti dataset\\non training set\\non noisy test set\\nFigure 3: Pruning performance (accuracy) comparison of criteria depending on the number of reference samples per\\nclass used for criterion computation. 1st row: Model evaluation on the training data. 2nd row: Model evaluation on an\\nunseen test dataset with added Gaussian noise (N(0, 0.3)), which have not been used for the computation of pruning\\ncriteria. Columns: Results over diﬀerent datasets. Solid lines show the average post-pruning performance of the models\\npruned w.r.t. to the evaluated criteria weight (black), Taylor (blue), grad(ient) (green) and LRP (red) over 50 repetitions\\nof the experiment. The dashed black line indicates the model’s evaluation performance without pruning. Shaded areas\\naround the lines show the standard deviation over the repetition of experiments. Further results for noise levels N(0, 0.1)\\nand N(0, 0.01) are available on github3.\\nThe results in Figure 4 show that, among all criteria based on reference sample for the computa-\\ntion of relevance, the LRP-based measure consistently outperforms all other criteria in all reference\\nset sizes and datasets. Only in the case of n = 1 reference sample per class, the weight criterion\\npreserves the model the best. Note that using the weight magnitude as a measure of network unit\\nimportance is a static approach, independent from the choice of reference samples. Given n = 5\\npoints of reference per class, the LRP-based criterion already outperforms also the weight magnitude\\nas a criterion for pruning unimportant neural network structures, while successfully preserving the\\nfunctional core of the predictor. Figure 2 demonstrates how the toy models’ decision boundaries\\nchange under inﬂuence of pruning with all four criteria. We can observe that the weight criterion and\\nLRP preserve the models’ learned decision boundary well. Both the Taylor and gradient measures\\ndegrade the model signiﬁcantly. Compared to weight- and LRP-based criteria, models pruned by\\ngradient-based criteria misclassify a large part of samples.\\nThe ﬁrst row of Figure 3 shows that all (data dependent) measures beneﬁt from increasing the\\nnumber of reference points. LRP is able to ﬁnd and preserve the functionally important network\\ncomponents with only very little data, while at the same time being considerably less sensitive to\\n10\\n Figure 4: Comparison of training accuracy after one-shot pruning one third of all ﬁlters w.r.t one of the four metrics on\\ntoy datasets, with n ∈ [1, 5, 20, 100] reference samples used for criteria computation for Weight, Gradient, Taylor and\\nLRP. The experiment is repeated 50 times. Note that the Weight criterion is not inﬂuenced by the number of reference\\nsamples n. Compare to Supplementary Table 1.\\nthe choice of reference points than other metrics, visible in the measures’ standard deviations. Both\\nthe gradient and Taylor-based measures do not reach the performance of LRP-based pruning, even\\nwith 200 reference samples for each class. The performance of pruning with the weight magnitude\\nbased measure is constant, as it does only depend on the learned weights itself. The bottom row of\\nFigure 3 shows the test performance of the pruned models as a function of the number of samples\\nused for criteria computation. Here, we tested on 500 samples per class, drawn from the datasets’\\nrespective distributions, and perturbed with additional gaussian noise (N(0, 0.3)) added after data\\ngeneration. Due to the large amounts of noise added to the data, we see the prediction performance\\nof the pruned and unpruned models to decrease in all settings. Here we can observe that two out\\nof three times the LRP-pruned models outperforming all other criteria. Only once, on the “moon”\\ndataset, pruning based on the weight criterion yields a higher performance than the LRP-pruned\\nmodel. Most remarkably though, only the models pruned with the LRP-based criterion exhibit\\nprediction performance and behavior — measured in mean and standard deviation of accuracies\\nmeasured over all 50 random seeds per n reference samples on the deliberatly heavily noisy data —\\nhighly similar to the original and unpruned model, from only n = 5 reference samples per class on,\\non all datasets. This yields another strong indicator that LRP is, among the compared criteria, most\\ncapable at preserving the relevant core of the learned network function, and to dismiss unimportant\\nparts of the model during pruning.\\nThe strong results of LRP, and the partial similarity between the results on the training datasets\\nbetween LRP and weight raises the question where and how both metrics (and Taylor and gradient)\\ndeviate, as it can be expected that both metrics at least select highly overlapping sets of network units\\nfor pruning and preservation. We therefore investigate in all three toy settings — across the diﬀerent\\nnumber of reference samples and random seeds — the (dis)similarities and (in)consistencies in\\nneuron selection and ranking by measuring the set similarities (S 1 ∩ S 2)/ min(|S 1|, |S 2|) of the k\\nneurons selected for pruning (ranked ﬁrst) and preservation (ranked last) between and within criteria.\\nSince the weight criterion is not inﬂuenced by the choice of reference samples for computation,\\n11\\n it is expected that the resulting neuron',\n",
    "  'selection and ranking by measuring the set similarities (S 1 ∩ S 2)/ min(|S 1|, |S 2|) of the k\\nneurons selected for pruning (ranked ﬁrst) and preservation (ranked last) between and within criteria.\\nSince the weight criterion is not inﬂuenced by the choice of reference samples for computation,\\n11\\n it is expected that the resulting neuron order is perfectly consistent with itself in all settings (cf.\\nTable 2). What is unexpected however, given the results in Figure 3 and Figure 4 indicating similar\\nmodel behavior after pruning to be expected between LRP- and weight-based criteria, at least\\non the training data, is the minimal set overlap between LRP and weight, given the higher set\\nsimilarities between LRP and the gradient and Taylor criteria, as shown in Table 1. Overall, the\\nset overlap between the neurons ranked in the extremes of the orderings show that LRP-derived\\npruning strategies have very little in common with the ones originating from the other criteria.\\nThis observation can also be made on more complex networks at hand of Figure 7, as shown and\\ndiscussed later in this Section.\\nTable 1: Similarity analysis of neuron selection between LRP and the other criteria, computed over 50 diﬀerent random\\nseeds. Higher values indicate higher similarity in neuron selection of the ﬁrst/last k neurons for pruning compared to\\nLRP. Note that below table reports results only for n = 10 reference samples for criteria computation (Weight, Taylor,\\nGradient and LRP) and k = 250 and k = 1000. Similar observations have been made for n ∈ [1, 2,5, 20, 50, 100, 200]\\nand k ∈ [125, 500] and can be found on github3.\\nDataset\\nﬁrst-250\\nlast-250\\nﬁrst-1000\\nlast-1000\\nW\\nT\\nG\\nL\\nW\\nT\\nG\\nL\\nW\\nT\\nG\\nL\\nW\\nT\\nG\\nL\\nmoon\\n0.002\\n0.006\\n0.006\\n1.000\\n0.083\\n0.361\\n0.369\\n1.000\\n0.381\\n0.639\\n0.626\\n1.000\\n0.409\\n0.648\\n0.530\\n1.000\\ncircle\\n0.033\\n0.096\\n0.096\\n1.000\\n0.086\\n0.389\\n0.405\\n1.000\\n0.424\\n0.670\\n0.627\\n1.000\\n0.409\\n0.623\\n0.580\\n1.000\\nmult\\n0.098\\n0.220\\n0.215\\n1.000\\n0.232\\n0.312\\n0.299\\n1.000\\n0.246\\n0.217\\n0.243\\n1.000\\n0.367\\n0.528\\n0.545\\n1.000\\nTable 2 reports the self-similarity in neuron selection in the extremes of the ranking across\\nrandom seeds (and thus sets of reference samples), for all criteria and toy settings. While LRP yields\\na high consistency in neuron selection for both the pruning (ﬁrst-k) and the preservation (last-k) of\\nneural network units, both gradient and moreso Taylor exhibit lower self-similarities. The lower\\nconsistency of both latter criteria in the model components ranked last (i.e. preserved in the model\\nthe longest during pruning) yields an explanation for the large variation in results observed earlier:\\nalthough gradient and Taylor are highly consistent in the removal of neurons rated as irrelevant,\\ntheir volatility in the preservation of neurons which constitute the functional core of the network\\nafter pruning yields dissimilarities in the resulting predictor function. The high consistency reported\\nfor LRP in terms of neuron sets selected for pruning and preservation, given the relatively low\\nSpearman correlation coeﬃcient points out only minor local perturbations of the pruning order due\\nto the selection of reference samples. We ﬁnd a direct correspondence between the here reported\\n(in)consistency of pruning behavior for the three data-dependent criteria, and the in [12] observed\\n“explanation continuity” observed for LRP (and discontinuity for gradient and Taylor) in neural\\nnetworks containing the commonly used ReLU activation function, which provides an explanation\\nfor the high pruning consistency obtained with LRP, and the extreme volatility for gradient and\\nTaylor. A supplementary analysis of the neuron selection consistency of LRP over diﬀerent counts\\nof reference samples n, demonstrating the requirement of only very few reference samples per class\\nin order to obtain stable pruning results, can be found in Supplementary Results 1.\\nTaken together, the results of Tables 1 to 2 and Supplementary Tables 1 and 2 elucidate that\\nLRP constitutes — compared to the other methods — an orthogonal pruning criterion which is very\\nconsistent in its selection of (un)important neural network units, while remaining adaptive to the\\nselection of reference samples for criterion computation. Especially the similarity in post-pruning\\nmodel performance to the static weight criterion indicates that both metrics are able to ﬁnd valid,\\nyet completely diﬀerent pruning solutions. However, since LRP can still beneﬁt from the inﬂuence\\n12\\n Table 2: A consistency comparison of neuron selection and ranking for network pruning with criteria (Weight, Taylor,\\nGradient and LRP), averaged over all 1225 unique random seed combinations. Higher values indicate higher consistency\\nin selecting the same sets of neurons and generating neuron rankings for diﬀerent sets of reference samples. We report\\nresults for n = 10 reference samples and k = 250. Observations for n ∈ [1, 2,5, 20, 50, 100, 200] and k ∈ [125, 500, 1000]\\nare available on github3.\\nDataset\\nﬁrst-250\\nlast-250\\nSpearman Correlation\\nW\\nT\\nG\\nL\\nW\\nT\\nG\\nL\\nW\\nT\\nG\\nL\\nmoon\\n1.000\\n0.920\\n0.918\\n0.946\\n1.000\\n0.508\\n0.685\\n0.926\\n1.000\\n0.072\\n0.146\\n0.152\\ncircle\\n1.000\\n0.861\\n0.861\\n0.840\\n1.000\\n0.483\\n0.635\\n0.936\\n1.000\\n0.074\\n0.098\\n0.137\\nmult\\n1.000\\n0.827\\n0.829\\n0.786\\n1.000\\n0.463\\n0.755\\n0.941\\n1.000\\n0.080\\n0.131\\n0.155\\nof reference samples, we will show in Section 4.2.2 that our proposed criterion is able to outperform\\nnot only weight, but all other criteria in Scenario 2, where pruning is is used instead of ﬁne-tuning\\nas a means of domain adaptation. This will be discussed in the following sections.\\n4.2. Pruning Deep Image Classiﬁers for Large-scale Benchmark Data\\nWe now evaluate the performance of all pruning criteria on the CNNs, VGG-16, AlexNet\\nas well as ResNet-18 and ResNet-50, — popular models in compression research [42] — all of\\nwhich are pre-trained on ILSVRC 2012 (ImageNet). VGG-16 consists of 13 convolutional layers\\nwith 4224 ﬁlters and 3 fully-connected layers and AlexNet contains 5 convolutional layers with\\n1552 ﬁlters and 3 fully-connected layers. In dense layers, there exist 4,096+4,096+k neurons (i.e.\\nﬁlters), respectively, where k is the number of output classes. In terms of complexity of the model,\\nthe pre-trained VGG-16 and AlexNet on ImageNet originally consist of 138.36/60.97 million of\\nparameters and 154.7/7.27 Giga',\n",
    "  'layers. In dense layers, there exist 4,096+4,096+k neurons (i.e.\\nﬁlters), respectively, where k is the number of output classes. In terms of complexity of the model,\\nthe pre-trained VGG-16 and AlexNet on ImageNet originally consist of 138.36/60.97 million of\\nparameters and 154.7/7.27 Giga Multiply-Accumulate Operations per Second (GMACS) (as a\\nmeasure of FLOPs), respectively. ResNet-18 and ResNet-50 consist of 20/53 convolutional layers\\nwith 4,800/26,560 ﬁlters. In terms of complexity of the model, the pre-trained ResNet-18 and\\nResNet-50 on ImageNet originally consist of 11.18/23.51 million of parameters and 1.82/4.12\\nGMACS (as a measure of FLOPs), respectively.\\nFurthermore, since the LRP scores are not implementation-invariant and depend on the LRP\\nrules used for the batch normalization (BN) layers, we convert a trained ResNet into a canonized\\nversion, which yields the same predictions up to numerical errors. The canonization fuses a sequence\\nof a convolution and a BN layer into a convolution layer with updated weights4 and resets the BN\\nlayer to be the identity function. This removes the BN layer eﬀectively by rewriting a sequence of\\ntwo aﬃne mappings into one updated aﬃne mapping [43]. The second change replaced calls to\\ntorch.nn.functional methods and the summation in the residual connection by classes derived\\nfrom torch.nn.Module which then were wrapped by calls to torch.autograd.function to\\nenable custom backward computations suitable for LRP rule computations.\\nExperiments are performed within the PyTorch and torchvision frameworks under Intel(R)\\nXeon(R) CPU E5-2660 2.20GHz and NVIDIA Tesla P100 with 12GB for GPU processing. We\\nevaluated the criteria on six public datasets (Scene 15 [36], Event 8, Cats and Dogs [38], Oxford\\nFlower 102 [39], CIFAR-10, and ILSVRC 2012 [40]). For more detail on the datasets and the\\n4See bnafterconv overwrite intoconv(conv,bn) in the ﬁle lrp general6.py in https://github.com/\\nAlexBinder/LRP_Pytorch_Resnets_Densenet\\n13\\n preprocessing, see Supplementary Methods 1. Our complete experimental setup covering these\\ndatasets is publicly available at https://github.com/seulkiyeom/LRP pruning.\\nResNet-50\\nTest Accuracy\\nTest Accuracy\\nVGG-16\\nScene 15\\nCifar 10\\nCats and Dogs\\nEvent 8\\nOxford Flower 102\\nFigure 5: Comparison of test accuracy in diﬀerent criteria as pruning rate increases on VGG-16 (top) and ResNet-50\\n(bottom) with ﬁve datasets. Pruning with ﬁne-tuning. Prematurely terminated lines in above row of panels indicate that\\nduring pruning, the respective criterion removed ﬁlters vital to the network structure by disconnecting the model input\\nfrom the output.\\nTable 3: A performance comparison between criteria (Weight, Taylor, Gradient with ℓ2-norm each and LRP) and the\\nUnpruned model for VGG-16 (top) and ResNet-50 (bottom) on ﬁve diﬀerent image benchmark datasets. Criteria are\\nevaluated at ﬁxed pruning rates per model and dataset, identiﬁed as ⟨dataset⟩@⟨percent pruned filters⟩%. We\\nreport test accuracy (in %), (training) loss (×10−2), number of remaining parameters (×107) and FLOPs (in GMAC) per\\nforward pass. For all measures except accuracy, lower outcomes are better.\\nVGG-16\\nScene 15 @ 55%\\nEvent 8 @ 55%\\nCats & Dogs @ 60%\\nU\\nW\\nT\\nG\\nL\\nU\\nW\\nT\\nG\\nL\\nU\\nW\\nT\\nG\\nL\\nLoss\\n2.09\\n2.27\\n1.76\\n1.90\\n1.62\\n0.85\\n1.35\\n1.01\\n1.18\\n0.83\\n0.19\\n0.50\\n0.51\\n0.57\\n0.44\\nAccuracy\\n88.59\\n82.07\\n83.00\\n82.72\\n83.99\\n95.95\\n90.19\\n91.79\\n90.55\\n93.29\\n99.36\\n97.90\\n97.54\\n97.19\\n98.24\\nParams\\n119.61\\n56.17\\n53.10\\n53.01\\n49.67\\n119.58\\n56.78\\n48.48\\n50.25\\n47.35\\n119.55\\n47.47\\n51.19\\n57.27\\n43.75\\nFLOPs\\n15.50\\n8.03\\n4.66\\n4.81\\n6.94\\n15.50\\n8.10\\n5.21\\n5.05\\n7.57\\n15.50\\n7.02\\n3.86\\n3.68\\n6.49\\nOxford Flower 102 @ 70%\\nCIFAR-10 @ 30%\\nU\\nW\\nT\\nG\\nL\\nU\\nW\\nT\\nG\\nL\\nLoss\\n3.69\\n3.83\\n3.27\\n3.54\\n2.96\\n1.57\\n1.83\\n1.76\\n1.80\\n1.71\\nAccuracy\\n82.26\\n71.84\\n72.11\\n70.53\\n74.59\\n91.04\\n93.36\\n93.29\\n93.05\\n93.42\\nParams\\n119.96\\n39.34\\n41.37\\n42.68\\n37.54\\n119.59\\n74.55\\n97.30\\n97.33\\n89.20\\nFLOPs\\n15.50\\n5.48\\n2.38\\n2.45\\n4.50\\n15.50\\n11.70\\n8.14\\n8.24\\n9.93\\nResNet-50\\nScene 15 @ 55%\\nEvent 8 @ 55%\\nCats & Dogs @ 60%\\nU\\nW\\nT\\nG\\nL\\nU\\nW\\nT\\nG\\nL\\nU\\nW\\nT\\nG\\nL\\nLoss\\n0.81\\n1.32\\n1.08\\n1.32\\n0.50\\n0.33\\n1.07\\n0.63\\n0.85\\n0.28\\n0.01\\n0.05\\n0.06\\n0.21\\n0.02\\nAccuracy\\n88.28\\n80.17\\n80.26\\n78.71\\n85.38\\n96.17\\n88.27\\n87.55\\n86.38\\n94.22\\n98.42\\n97.02\\n96.33\\n93.13\\n98.03\\nParams\\n23.54\\n14.65\\n12.12\\n11.84\\n13.73\\n23.52\\n13.53\\n11.85\\n11.93\\n14.05\\n23.51\\n12.11\\n10.40\\n10.52\\n12.48\\nFLOPs\\n4.12\\n3.22\\n2.45\\n2.42\\n3.01\\n4.12\\n3.16\\n2.48\\n2.47\\n3.10\\n4.12\\n3.04\\n2.40\\n2.27\\n2.89\\nOxford Flower 102 @ 70%\\nCIFAR-10 @',\n",
    "  'Flower 102 @ 70%\\nCIFAR-10 @ 30%\\nU\\nW\\nT\\nG\\nL\\nU\\nW\\nT\\nG\\nL\\nLoss\\n0.82\\n3.04\\n2.18\\n2.69\\n0.83\\n0.003\\n0.002\\n0.004\\n0.009\\n0.003\\nAccuracy\\n77.82\\n51.88\\n58.62\\n53.96\\n76.83\\n93.55\\n93.37\\n93.15\\n92.76\\n93.23\\nParams\\n23.72\\n9.24\\n8.82\\n8.48\\n9.32\\n23.52\\n19.29\\n18.10\\n17.96\\n18.11\\nFLOPs\\n4.12\\n2.55\\n1.78\\n1.81\\n2.38\\n1.30\\n1.14\\n1.06\\n1.05\\n1.16\\nIn order to prepare the models for evaluation, we ﬁrst ﬁne-tuned the models for 200 epochs with\\n14\\n constant learning rate 0.001 and batch size of 20. We used the Stochastic Gradient Descent (SGD)\\noptimizer with momentum of 0.9. In addition, we also apply dropout to the fully-connected layers\\nwith probability of 0.5. Fine-tuning and pruning are performed on the training set, while results are\\nevaluated on each test dataset. Throughout the experiments, we iteratively prune 5% of all the ﬁlters\\nin the network by eliminating units including their input and output connections. In Scenario 1, we\\nsubsequently ﬁne-tune and re-evaluate the model to account for dependency across parameters and\\nregain performance, as it is common.\\nFigure 6: Performance comparison of the proposed method (i.e. LRP) and other criteria on VGG-16 and ResNet-50\\nwith ﬁve datasets. Each point in the scatter plot corresponds to the performance at a speciﬁc pruning rate of two criteria,\\nwhere the vertical axis shows the performance of our LRP criterion and the horizontal axis the performance of a single\\nother criterion (compare to Figure 5 that displays the same data for more than two criteria). The black dashed line shows\\nthe set of points where models pruned by one of the compared criteria would exhibit identical performance to LRP. For\\naccuracy, higher values are better. For loss, lower values are better.\\n4.2.1. Scenario 1: Pruning with Fine-tuning\\nOn the ﬁrst scenario, we retrain the model after each iteration of pruning in order to regain\\nlost performance. We then evaluate the performance of the diﬀerent pruning criteria after each\\npruning-retraining-step. That is, we quantify the importance of each ﬁlter by the magnitude of the\\nrespective criterion and iteratively prune 5% of all ﬁlters (w.r.t. the original number of ﬁlters in\\nthe model) rated least important in each pruning step. Then, we compute and record the training\\nloss, test accuracy, number of remaining parameters and total estimated FLOPs. We assume that the\\nleast important ﬁlters should have only little inﬂuence on the prediction and thus incur the lowest\\nperformance drop if they are removed from the network.\\nFigure 5 (and Supplementary Figure 2) depict test accuracies with increasing pruning rate in\\nVGG-16 and ResNet-50 (and AlexNet and ResNet-18, respectively) after ﬁne-tuning for each dataset\\nand each criterion. It is observed that LRP achieves higher test accuracies compared to other criteria\\nin a large majority of cases (see Figure 6 and Supplementary Figure 1). These results demonstrate\\nthat the performance of LRP-based pruning is stable and independent of the chosen dataset. Apart\\nfrom performance, regularization by layer is a critical constraint which obstructs the expansion\\nof some of the criteria toward several pruning strategies such as local pruning, global pruning,\\netc. Except for the LRP criterion, all criteria perform substantially worse without lp regularization\\n15\\n LRP\\nWeight\\nGradient\\nTaylor\\nRemaining\\n filters (%)\\n100\\n0\\nIndex of Convolutional Layer\\n100\\n0\\nA. Cats and Dogs\\nB. Oxford Flower 102\\nRemaining\\n filters (%)\\nFigure 7: An observation of per-layer pruning performed w.r.t the diﬀerent evaluated criteria on VGG-16 and two\\ndatasets. Each colored line corresponds to a speciﬁc (global) ratio of ﬁlters pruned from the network (black (top) : 0%,\\nred : 15%, green: 30%, blue: 45%, violet: 75% and black (bottom) 90%). The dots on each line identify the ratio\\nof pruning applied to speciﬁc convolutional layers, given a global ratio of pruning, depending on the pruning criterion.\\ncompared to those with lp regularization and result in unexpected interruptions during the pruning\\nprocess due to the biased redistribution of importance in the network (cf. top rows of Figure 5 and\\nSupplementary Figure 2).\\nTable 3 shows the predictive performance of the diﬀerent criteria in terms of training loss, test\\naccuracy, number of remaining parameters and FLOPs, for the VGG-16 and ResNet-50 models.\\nSimilar results for AlexNet and ResNet-18 can be found in Supplementary Table 2. Except for\\nCIFAR-10, the highest compression rate (i.e. lowest number of parameters) could be achieved by\\nthe proposed LRP-based criterion (row “Params”) for VGG-16, but not for ResNet-50. However, in\\nterms of FLOPs, the proposed criterion only outperformed the weight criterion, but not the Taylor\\nand Gradient criteria (row“FLOPs”). This is due to the fact that a reduction in number of FLOPs\\ndepends on the location where pruning is applied within the network: Figure 7 shows that the LRP\\nand weight criteria focus the pruning on upper layers closer to the model output, whereas the Taylor\\nand Gradient criteria focus more on the lower layers.\\nThroughout the pruning process usually a gradual decrease in performance can be observed.\\nHowever, with the Event 8, Oxford Flower 102 and CIFAR-10 datasets, pruning leads to an initial\\nperformance increase, until a pruning rate of approx. 30% is reached. This behavior has been\\nreported before in the literature and might stem from improvements of the model structure through\\nelimination of ﬁlters related to classes in the source dataset (i.e., ILSVRC 2012) that are not present\\nin the target dataset anymore [44]. Supplementary Table 3 and Supplementary Figure 2 similarly\\nshow that LRP achieves the highest test accuracy in AlexNet and ResNet-18 for nearly all pruning\\nratios with almost every dataset.\\nFigure 7 shows the number of the remaining convolutional ﬁlters for each iteration. We observe\\nthat, on the one hand, as pruning rate increases, the convolutional ﬁlters in earlier layers that are\\nassociated with very generic features, such as edge and blob detectors, tend to generally be preserved\\nas opposed to those in latter layers which are associated with abstract, task-speciﬁc features. On\\nthe other hand, the LRP- and weight-criterion ﬁrst keep the ﬁlters in early layers in the beginning,\\nbut later aggressively prune ﬁlters near the input which now have lost functionality as input to\\n16\\n later layers, compared to the gradient-based criteria such as gradient and Taylor-based approaches.\\nAlthough gradient-based criteria also adopt the greedy layer-by-layer approach, we can see that\\ngradient-based criteria pruned the less important ﬁlters almost uniformly across all the layers due\\nto re-normalization of the criterion in each iteration. However, this result contrasts with',\n",
    "  'later layers, compared to the gradient-based criteria such as gradient and Taylor-based approaches.\\nAlthough gradient-based criteria also adopt the greedy layer-by-layer approach, we can see that\\ngradient-based criteria pruned the less important ﬁlters almost uniformly across all the layers due\\nto re-normalization of the criterion in each iteration. However, this result contrasts with previous\\ngradient-based works [22, 25] that have shown that units deemed unimportant in earlier layers,\\ncontribute signiﬁcantly compared to units deemed important in latter layers. In contrast to this, LRP\\ncan eﬃciently preserve units in the early layers — as long as they serve a purpose — despite of\\niterative global pruning.\\n4.2.2. Scenario 2: Pruning without Fine-tuning\\nIn this section, we evaluate whether pruning works well if only a (very) limited number of\\nsamples is available for quantifying the pruning criteria. To the best of our knowledge, there are no\\nprevious studies that show the performance of pruning approaches when acting w.r.t. very small\\namounts of data. With large amounts of data available (and even though we can expect reasonable\\nperformance after pruning), an iterative pruning and ﬁne-tuning procedure of the network can\\namount to a very time consuming and computationally heavy process. From a practical point of\\nview, this issue becomes a signiﬁcant problem, e.g. with limited computational resources (mobile\\ndevices or in general; consumer-level hardware) and reference data (e.g., private photo collections),\\nwhere capable and eﬀective one-shot pruning approaches are desired and only little leeway (or none\\nat all) for ﬁne-tuning strategies after pruning is available.\\nTo investigate whether pruning is possible also in these scenarios, we performed experiments\\nwith a relatively small number of data on the 1) Cats & Dogs and 2) subsets from the ILSVRC 2012\\nclasses. On the Cats & Dogs dataset, we only used 10 samples each from the “cat” and “dog” classes\\nto prune the (on ImageNet) pre-trained AlexNet, VGG-16, ResNet-18 and ResNet-50 networks\\nwith the goal of domain/dataset adaption. The binary classiﬁcation (i.e. “cat” vs. “dog”) is a subtask\\nwithin the ImageNet taxonomy and corresponding output neurons can be identiﬁed by its WordNet5\\nassociations. This experiment implements the task of domain adaptation.\\nIn a second experiment on the ILSVRC 2012 dataset, we randomly chose k = 3 classes for the\\ntask of model specialization, selected only n = 10 images per class from the training set and used\\nthem to compare the diﬀerent pruning criteria. For each criterion, we used the same selection of\\nclasses and samples. In both experimental settings, we do not ﬁne-tune the models after each pruning\\niteration, in contrast to Scenario 1 in Section 4.2.1. The obtained post-pruning model performance\\nis averaged over 20 random selections of classes (ImageNet) and samples (Cats & Dogs) to account\\nfor randomness. Please note that before pruning, we ﬁrst restructured the models’ fully connected\\noutput layers to only preserve the task-relevant k network outputs by eliminating the 1000 − k\\nredundant output neurons.\\nFurthermore, as our target datasets are relatively small and only have an extremely reduced set of\\ntarget classes, the pruned models could still be very heavy w.r.t. memory requirements if the pruning\\nprocess would be limited to the convolutional layers, as in Section 4.2.1. More speciﬁcally, while\\nconvolutional layers dominantly constitute the source of computation cost (FLOPs), fully connected\\nlayers are proven to be more redundant [29]. In this respect, we applied pruning procedures in both\\nfully connected layers and convolutional layers in combination for VGG-16.\\n5http://www.image-net.org/archive/wordnet.is_a.txt\\n17\\n Figure 8: Test accuracy after pruning of n% of convolutional (rows) and m% of fully connected (columns) ﬁlters on\\nVGG-16 without ﬁne-tuning for a random subset of the classes from ILSVRC 2012 (k = 3) based on diﬀerent criteria\\n(averaged over 20 repetitions). Each color represents a range of 5% in test accuracy. The brighter the color the better the\\nperformance after a given degree of pruning .\\nFor pruning, we iterate a sequence of ﬁrst pruning ﬁlters from the convolutional layers, followed\\nby a step of pruning neurons from the model’s fully connected layers. Note that both evaluated\\nResNet architectures mainly consist of convolutional- and pooling layers, and conclude in a single\\ndense layer, of which the set of input neurons are only aﬀected via their inputs by pruning the below\\nconvolutional stack. We therefore restrict the iterative pruning ﬁlters from the sequence of dense\\nlayers of the feed-forward architecture of the VGG-16.\\nThe model performance after the application of each criterion for classifying a small number of\\nclasses (k = 3) from the ILSVRC 2012 dataset is indicated in Figure 8 for VGG 16 and Figure 9 for\\nResNets (please note again that ResNets do not have fully-connected layers). During pruning at fully-\\nconnected layers, no signiﬁcant diﬀerence across diﬀerent pruning ratios can be observed. Without\\nfurther ﬁne-tuning, pruning weights/ﬁlters at the fully connected layers can retain performance\\neﬃciently. However, there is a certain diﬀerence between LRP and other criteria with increasing\\npruning ratio of convolutional layers for VGG-16/ResNet-18/ResNet-50, respectively: (LRP vs.\\nTaylor with l2-norm; up to of 9.6/61.8/51.8%, LRP vs. gradient with l2-norm; up to 28.0/63.6/54.5 %,\\nLRP vs. weight with l2-norm; up to 27.1/48.3/30.2 %). Moreover, pruning convolutional layers\\nneeds to be carefully managed compared to pruning fully connected layers. We can observe that\\n18\\n Figure 9: Test accuracy after pruning of n% of convolutional ﬁlters on ResNet18 and ResNet50 without ﬁne-tuning for\\na random subset of the classes from ILSVRC 2012 (k = 3) based on the criteria Weight, Taylor, Gradient with ℓ2-norm\\nand LRP (averaged over 20 repetitions). Compare to Figure 8 .\\nLRP is applicable for pruning any layer type (i.e. fully connected, convolutional, pooling, etc.)\\neﬃciently. Additionally, as mentioned in Section 3.1, our method can be applied to general network\\narchitectures because it can automatically measure the importance of weights or ﬁlters in a global\\n(network-wise) context without further normalization.\\nFigure 10 shows the test accuracy as a function of the pruning ratio, in context a domain adaption\\ntask from ImageNet towards the Cats & Dogs dataset for all models. As the pruning ratio increases,\\nwe can see that even without ﬁne-tuning, using LRP as pruning criterion can keep the test accuracy\\nnot only stable, but close to 100%, given the extreme scarcity of data in this experiment. In contrast,\\nthe performance decreases signiﬁcantly when using the other criteria requiring an application of\\nthe l2-norm. Initially, the performance is even slightly increasing when pruning with LRP. During\\niterative pruning, unexpected changes in accuracy',\n",
    "  'using LRP as pruning criterion can keep the test accuracy\\nnot only stable, but close to 100%, given the extreme scarcity of data in this experiment. In contrast,\\nthe performance decreases signiﬁcantly when using the other criteria requiring an application of\\nthe l2-norm. Initially, the performance is even slightly increasing when pruning with LRP. During\\niterative pruning, unexpected changes in accuracy with LRP (for 2 out of 20 repetitions of the\\nexperiment) have been shown around 50 - 55% pruning ratio, but accuracy is regained quickly\\nagain. However, only the VGG-16 model seems to be aﬀected, and none other for this task. For\\nboth ResNet models, this phenomenon occurs for the other criteria instead. A series of in-depth\\ninvestigations of this momentary decrease in performance did not lead to any insights and will be\\nsubject of future work6.\\nBy pruning over 99% of convolutional ﬁlters in the networks using our proposed method, we\\ncan have 1) greatly reduced computational cost, 2) faster forward and backward processing (e.g. for\\nthe purpose of further training, inference or the computation of attribution maps), and 3) a lighter\\nmodel even in the small sample case, all while adapting oﬀ-the-shelf pre-trained ImageNet models\\ntowards a dog-vs.-cat classiﬁcation task.\\n5. Discussion\\nOur experiments demonstrate that the novel LRP criterion consistently performed well com-\\npared to other criteria across various datasets, model architectures and experimental settings, and\\noftentimes outperformed the competing criteria. This is especially pronounced in our Scenario\\n2 (cf. Section 4.2.2), where only little resources are available for criterion computation, and no\\nﬁne-tuning after pruning is allowed. Here, LRP considerably outperformed the other metrics on\\ntoy data (cf. Section 4.1) and image processing benchmark data (cf. Section 4.2.2). The strongly\\n6We consequently have to assume that this phenomenon marks the downloaded pre-trained VGG-16 model as an\\noutlier in this respect. A future line of research will dedicate inquiries about the circumstances leading to intermediate\\nloss and later recovery of model performance during pruning.\\n19\\n Figure 10: Performance comparison of pruning without ﬁne-tuning for AlexNet, VGG-16, ResNet-18 and ResNet-50\\nbased on only few (10) samples per class from the Cats & Dogs dataset, as a means for domain adaption. Additional\\nresults on further target domains can be found in the Supplement with Supplementary Figure 3.\\nsimilar results between criteria observed in Scenario 1 (cf. Section 4.2.2) are also not surprising, as\\nan additional ﬁle-tuning step after pruning may allow the pruned neural network model to recover\\nits original performance, as long as the model has the capacity to do so [22].\\nFrom the results of Table 3 and Supplementary Table 3 we can observe that with a ﬁxed pruning\\ntarget of n% ﬁlters removed, LRP might not always result in the cheapest sub-network after pruning\\nin terms of parameter count and FLOPs per inference, however it consistently is able to identify\\nthe network components for removal and preservation leading to the best performing model after\\npruning. Latter results resonate also strongly in our experiments of Scenario 2 on both image and toy\\ndata, where, without the additional ﬁne-tuning step, the LRP-pruned models vastly outperform their\\ncompetitors. The results obtained in multiple toy settings verify that only the LRP-based pruning\\ncriterion is able to preserve the original structure of the prediction function (cf. Figures 2 and 3).\\nUnlike the weight criterion, which is a static quantity once the network is not in training anymore,\\nthe criteria Taylor, gradient and LRP require reference samples for computation, which in turn\\nmay aﬀect the estimation of neuron importance. From the latter three criteria, however, only LRP\\nprovides a continuous measure of network structure importance (cf. Sec 7.2 in [12]) which does not\\nsuﬀer from abrupt changes in the estimated importance measures with only marginal steps between\\nreference samples. This quality of continuity is reﬂected in the stability and quality of LRP results\\nreported in Section 4.1, compared to the high volatility in neuron selection for pruning and model\\nperformance after pruning observable for the gradient and Taylor criteria. From this observation it\\ncan also be deduced that LRP requires relatively few data points to converge to a pruning solution\\nthat possesses a similar prediction behavior as the original model. Hence, we conclude that LRP is a\\nrobust pruning criterion that is broadly applicable in practice. Especially in a scenario where no\\nﬁnetuning is applied after pruning (see Sec. 4.2.2), the LRP criterion allows for pruning of a large\\npart of the model without signiﬁcant accuracy drops.\\nIn terms of computational cost, LRP is comparable to the Taylor and Gradient criteria because\\nthese criteria require both a forward and a backward pass for all reference samples. The weight\\ncriterion is substantially cheaper to compute since it does not require to evaluate any reference\\nsamples; however, its performance falls short in most of our experiments. Additionally, our exper-\\niments demonstrate that LRP requires less reference samples than the other criteria (cf. Figure 3\\nand Figure 4), thus the required computational cost is lower in practical scenarios, and better\\n20\\n performance can be expected if only low numbers of reference samples are available (cf. Figure 10).\\nUnlike all other criteria, LRP does not require explicit regularization via ℓp-normalization,\\nas it is naturally normalized via its enforced relevance conservation principle during relevance\\nbackpropagation, which leads to the preservation of important network substructures and bottlenecks\\nin a global model context. In line with the ﬁndings by [22], our results in Figure 5 and Supplementary\\nFigure 2 show that additional normalization after criterion computation for weight, gradient and\\nTaylor is not only vital to obtain good performance, but also to avoid disconnected model segments\\n— something which is prevented out-of-the-box with LRP.\\nHowever, our proposed criterion still provides several open questions that deserve a deeper\\ninvestigation in future work. First of all, LRP is not implementation invariant, i.e., the structure\\nand composition of the analyzed network might aﬀect the computation of the LRP-criterion and\\n“network canonization” — a functionally equivalent restructuring of the model — might be required\\nfor optimal results, as discussed early in Section 4 and [43]. Furthermore, while our LRP-criterion\\ndoes not require additional hyperparameters, e.g., for normalization, the pruning result might still\\ndepend on the chosen LRP variant. In this paper, we chose the α1β0-rule in all layers, because this\\nparticular parameterization identiﬁes the network’s neural pathways positively contributing to the\\nselected output neurons for which reference samples are provided, is robust to the detrimental eﬀects\\nof shattered gradients aﬀecting especially very deep CNNs [11] (i.e., other than gradient-based\\nmethods, it does not suﬀer from potential discontinuities in the backpropagated quantities), and has\\na mathematical well-motivated foundation in DTD [11, 12]. However, other work from literature\\nprovide [14] or suggest [9, 8] alternative parameterizations to optimize the method for explanatory\\npurposes. It is an interesting direction for future work to examine whether these ﬁndings also apply\\nto LRP as a pruning criterion.\\n6. Conclusion\\nModern CNNs typically have a high capacity with',\n",
    "  'mathematical well-motivated foundation in DTD [11, 12]. However, other work from literature\\nprovide [14] or suggest [9, 8] alternative parameterizations to optimize the method for explanatory\\npurposes. It is an interesting direction for future work to examine whether these ﬁndings also apply\\nto LRP as a pruning criterion.\\n6. Conclusion\\nModern CNNs typically have a high capacity with millions of parameters as this allows to\\nobtain good optimization results in the training process. After training, however, high inference\\ncosts remain, despite the fact that the number of eﬀective parameters in the deep model is actually\\nsigniﬁcantly lower (see e.g. [45]). To alleviate this, pruning aims at compressing and accelerating\\nthe given models without sacriﬁcing much predictive performance. In this paper, we have proposed\\na novel criterion for the iterative pruning of CNNs based on the explanation method LRP, linking\\nfor the ﬁrst time two so far disconnected lines of research. LRP has a clearly deﬁned meaning,\\nnamely the contribution of an individual network unit, i.e. weight or ﬁlter, to the network output.\\nRemoving units according to low LRP scores thus means discarding all aspects in the model that\\ndo not contribute relevance to its decision making. Hence, as a criterion, the computed relevance\\nscores can easily and cheaply give eﬃcient compression rates without further postprocessing, such\\nas per-layer normalization. Besides, technically LRP is scalable to general network structures and\\nits computational cost is similar to the one of a gradient backward pass.\\nIn our experiments, the LRP criterion has shown favorable compression performance on a\\nvariety of datasets both with and without retraining after pruning. Especially when pruning without\\nretraining, our results for small datasets suggest that the LRP criterion outperforms the state of the\\nart and therefore, its application is especially recommended in transfer learning settings where only\\na small target dataset is available.\\nIn addition to pruning, the same method can be used to visually interpret the model and explain\\n21\\n individual decisions as intuitive relevance heatmaps. Therefore, in future work, we propose to use\\nthese heatmaps to elucidate and explain which image features are most strongly aﬀected by pruning\\nto additionally avoid that the pruning process leads to undesired Clever Hans phenomena [8].\\nAcknowledgements\\nThis work was supported by the German Ministry for Education and Research (BMBF)\\nthrough BIFOLD (refs. 01IS18025A and 01IS18037A), MALT III (ref. 01IS17058), Patho234 (ref.\\n031L0207D) and TraMeExCo (ref. 01IS18056A), as well as the Grants 01GQ1115 and 01GQ0850;\\nand by Deutsche Forschungsgesellschaft (DFG) under Grant Math+, EXC 2046/1, Project ID\\n390685689; by the Institute of Information & Communications Technology Planning & Evaluation\\n(IITP) grant funded by the Korea Government (No. 2019-0-00079, Artiﬁcial Intelligence Graduate\\nSchool Program, Korea University); and by STE-SUTD Cyber Security Corporate Laboratory; the\\nAcRF Tier2 grant MOE2016-T2-2-154; the TL project Intent Inference; and the SUTD internal\\ngrant Fundamentals and Theory of AI Systems. The authors would like to express their thanks to\\nChristopher J Anders for insightful discussions.\\nReferences\\n[1] J. Gu, Z. Wang, J. Kuen, L. Ma, A. Shahroudy, B. Shuai, T. Liu, X. Wang, G. Wang, J. Cai,\\nT. Chen, Recent advances in convolutional neural networks, Pattern Recognition 77 (2018)\\n354–377.\\n[2] M. Denil, B. Shakibi, L. Dinh, M. Ranzato, N. de Freitas, Predicting parameters in deep\\nlearning, in: Advances in Neural Information Processing Systems (NIPS), 2013, pp. 2148–\\n2156.\\n[3] V. Sze, Y. Chen, T. Yang, J. S. Emer, Eﬃcient processing of deep neural networks: A tutorial\\nand survey, Proceedings of the IEEE 105 (2017) 2295–2329.\\n[4] Y. LeCun, J. S. Denker, S. A. Solla, Optimal brain damage, in: Advances in Neural Information\\nProcessing Systems (NIPS), 1989, pp. 598–605.\\n[5] Y. Tu, Y. Lin, Deep neural network compression technique towards eﬃcient digital signal\\nmodulation recognition in edge device, IEEE Access 7 (2019) 58113–58119.\\n[6] Y. Cheng, D. Wang, P. Zhou, T. Zhang, Model compression and acceleration for deep neural\\nnetworks: The principles, progress, and challenges, IEEE Signal Processing Magazine 35\\n(2018) 126–136.\\n[7] S. Bach, A. Binder, G. Montavon, F. Klauschen, K.-R. M¨uller, W. Samek, On pixel-wise\\nexplanations for non-linear classiﬁer decisions by layer-wise relevance propagation, PLoS\\nONE 10 (2015) e0130140.\\n[8] S. Lapuschkin, S. W¨aldchen, A. Binder, G. Montavon, W. Samek, K.-R. M¨uller, Unmasking\\nClever Hans predictors and assessing what machines really learn, Nature Communications 10\\n(2019) 1096.\\n22\\n [9] M. H¨agele, P. Seegerer, S. Lapuschkin, M. Bockmayr, W. Samek, F. Klauschen, K.-R. M¨uller,\\nA. Binder, Resolving challenges in deep learning-based analyses of histopathological images\\nusing explanation methods, Scientiﬁc Reports 10 (2020) 6423.\\n[10] P. Seegerer, A. Binder, R. Saitenmacher, M. Bockmayr, M. Alber, P. Jurmeister, F. Klauschen,\\nK.-R. M¨uller, Interpretable deep neural network to predict estrogen receptor status from\\nhaematoxylin-eosin images, in: Artiﬁcial Intelligence and Machine Learning for Digital\\nPathology: State-of-the-Art and Future Challenges, Springer International Publishing, Cham,\\n2020, pp. 16–37.\\n[11] G. Montavon, S. Lapuschkin, A. Binder, W. Samek, K.-R. M¨uller, Explaining nonlinear\\nclassiﬁcation decisions with deep taylor decomposition, Pattern Recognition 65 (2017) 211–\\n222.\\n[12] G. Montavon, W. Samek, K.-R. M¨uller, Methods for interpreting and understanding deep\\nneural networks, Digital Signal Processing 73 (2018) 1–15.\\n[13] W. Samek, G. Montavon, S. Lapuschkin, C. J. Anders, K.-R. M¨uller, Toward interpretable ma-\\nchine learning: Transparent deep neural networks and beyond, arXiv preprint arXiv:2003.07631\\n(2020).\\n[14] M. Alber, S. Lapuschkin, P. Seegerer, M. H¨agele, K. T. Sch¨utt, G. Montavon, W. Samek, K.-R.\\nM¨uller, S. D¨ahne, P.-J. Kindermans, iNNvestigate neural networks!, Journal of Machine\\nLearning Research 20 (2019) 93:1–93:8.\\n[15] S. Wiedemann, K.-R.',\n",
    "  'P. Seegerer, M. H¨agele, K. T. Sch¨utt, G. Montavon, W. Samek, K.-R.\\nM¨uller, S. D¨ahne, P.-J. Kindermans, iNNvestigate neural networks!, Journal of Machine\\nLearning Research 20 (2019) 93:1–93:8.\\n[15] S. Wiedemann, K.-R. M¨uller, W. Samek, Compact and computationally eﬃcient representation\\nof deep neural networks, IEEE Transactions on Neural Networks and Learning Systems 31\\n(2020) 772–785.\\n[16] F. Tung, G. Mori, Deep neural network compression by in-parallel pruning-quantization, IEEE\\nTransactions on Pattern Analysis and Machine Intelligence 42 (2020) 568–579.\\n[17] K. Guo, X. Xie, X. Xu, X. Xing, Compressing by learning in a low-rank and sparse decompo-\\nsition form, IEEE Access 7 (2019) 150823–150832.\\n[18] T. Xu, P. Yang, X. Zhang, C. Liu, LightweightNet: Toward fast and lightweight convolutional\\nneural networks via architecture distillation, Pattern Recognition 88 (2019) 272–284.\\n[19] X. Zhang, X. Zhou, M. Lin, J. Sun, Shuﬄenet: An extremely eﬃcient convolutional neural\\nnetwork for mobile devices, in: IEEE Conference on Computer Vision and Pattern Recognition\\n(CVPR), 2018, pp. 6848–6856.\\n[20] P. Molchanov, A. Mallya, S. Tyree, I. Frosio, J. Kautz, Importance estimation for neural\\nnetwork pruning, in: IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\\n2019, pp. 11264–11272.\\n[21] B. Hassibi, D. G. Stork, Second order derivatives for network pruning: Optimal brain surgeon,\\nin: Advances in Neural Information Processing Systems (NIPS), 1992, pp. 164–171.\\n23\\n [22] P. Molchanov, S. Tyree, T. Karras, T. Aila, J. Kautz, Pruning convolutional neural networks\\nfor resource eﬃcient transfer learning, in: Proceedings of the International Conference on\\nLearning Representations (ICLR), 2017.\\n[23] C. Yu, J. Wang, Y. Chen, X. Qin, Transfer channel pruning for compressing deep domain\\nadaptation models, International Journal of Machine Learning and Cybernetics 10 (2019)\\n3129–3144.\\n[24] C. Liu, H. Wu, Channel pruning based on mean gradient for accelerating convolutional neural\\nnetworks, Signal Processing 156 (2019) 84–91.\\n[25] X. Sun, X. Ren, S. Ma, H. Wang, meprop: Sparsiﬁed back propagation for accelerated deep\\nlearning with reduced overﬁtting, in: International Conference on Machine Learning (ICML),\\n2017, pp. 3299–3308.\\n[26] S. Han, J. Pool, J. Tran, W. J. Dally, Learning both weights and connections for eﬃcient\\nneural network, in: Advances in Neural Information Processing Systems (NIPS), 2015, pp.\\n1135–1143.\\n[27] S. Han, X. Liu, H. Mao, J. Pu, A. Pedram, M. A. Horowitz, W. J. Dally, EIE: eﬃcient\\ninference engine on compressed deep neural network,\\nin: International Symposium on\\nComputer Architecture (ISCA), 2016, pp. 243–254.\\n[28] W. Wen, C. Wu, Y. Wang, Y. Chen, H. Li, Learning structured sparsity in deep neural networks,\\nin: Advances in Neural Information Processing Systems (NIPS), 2016, pp. 2074–2082.\\n[29] H. Li, A. Kadav, I. Durdanovic, H. Samet, H. P. Graf, Pruning ﬁlters for eﬃcient convnets, in:\\nInternational Conference on Learning Representations, (ICLR), 2017.\\n[30] R. Yu, A. Li, C. Chen, J. Lai, V. I. Morariu, X. Han, M. Gao, C. Lin, L. S. Davis, NISP: pruning\\nnetworks using neuron importance score propagation, in: IEEE Conference on Computer\\nVision and Pattern Recognition (CVPR), 2018, pp. 9194–9203.\\n[31] J.-H. Luo, H. Zhang, H.-Y. Zhou, C.-W. Xie, J. Wu, W. Lin, ThiNet: Pruning CNN ﬁlters\\nfor a thinner net, IEEE Transactions on Pattern Analysis and Machine Intelligence 41 (2019)\\n2525–2538.\\n[32] J. Gan, W. Wang, K. Lu, Compressing the CNN architecture for in-air handwritten Chinese\\ncharacter recognition, Pattern Recognition Letters 129 (2020) 190 – 197.\\n[33] X. Dai, H. Yin, N. K. Jha, Nest: A neural network synthesis tool based on a grow-and-prune\\nparadigm, IEEE Transactions on Computers 68 (2019) 1487–1497.\\n[34] W. Samek, G. Montavon, A. Vedaldi, L. K. Hansen, K.-R. M¨uller (Eds.), Explainable AI:\\nInterpreting, Explaining and Visualizing Deep Learning, volume 11700 of Lecture Notes in\\nComputer Science, Springer, 2019.\\n[35] W. Samek, A. Binder, G. Montavon, S. Lapuschkin, K.-R. M¨uller, Evaluating the visualization\\nof what a deep neural network has learned, IEEE Transactions on Neural Networks and\\nLearning Systems 28 (2017) 2660–2673.\\n24\\n [36] S. Lazebnik, C. Schmid, J. Ponce, Beyond bags of features: Spatial pyramid matching for\\nrecognizing natural scene categories, in: IEEE Conference on Computer Vision and Pattern\\nRecognition (CVPR), 2006, pp. 2169–2178.\\n[37] L. Li, F. Li, What, where and who? Classifying events by scene and object recognition, in:\\nIEEE International Conference on Computer Vision (ICCV), 2007, pp. 1–8.\\n[38] J. Elson, J. R. Douceur, J. Howell, J. Saul, Asirra: a CAPTCHA that exploits interest-aligned\\nmanual image categorization, in: Proceedings of the 2007 ACM Conference on Computer and\\nCommunications Security (CCS), 2007, pp. 366–374.\\n[39] M. Nilsback, A. Zisserman, Automated ﬂower classiﬁcation over a large number of classes,\\nin: Sixth Indian Conference on Computer Vision, Graphics & Image Processing (ICVGIP),\\n2008, pp. 722–729.\\n[40] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy,\\nA. Khosla, M. S. Bernstein, A. C. Berg, F. Li, Imagenet large scale visual recognition\\nchallenge, International Journal of Computer Vision 115 (2015) 211–252.\\n[41] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recognition, in: 2016 IEEE\\nConference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA,\\nJune 27-30, 2016, 2016, pp. 770–778.\\n[42] H. Wang, Q. Zhang, Y. Wang, H. Hu, Structured probabilistic pruning',\n",
    "  'X. Zhang, S. Ren, J. Sun, Deep residual learning for image recognition, in: 2016 IEEE\\nConference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA,\\nJune 27-30, 2016, 2016, pp. 770–778.\\n[42] H. Wang, Q. Zhang, Y. Wang, H. Hu, Structured probabilistic pruning for convolutional neural\\nnetwork acceleration, in: British Machine Vision Conference (BMVC), 2018, p. 149.\\n[43] M. Guillemot, C. Heusele, R. Korichi, S. Schnebert, L. Chen, Breaking batch normalization\\nfor better explainability of deep neural networks through layer-wise relevance propagation,\\nCoRR abs/2002.11018 (2020).\\n[44] J. Liu, Y. Wang, Y. Qiao, Sparse deep transfer learning for convolutional neural network, in:\\nAAAI Conference on Artiﬁcial Intelligence, 2017, pp. 2245–2251.\\n[45] N. Murata, S. Yoshizawa, S. Amari, Network information criterion-determining the number\\nof hidden units for an artiﬁcial neural network model, IEEE Transactions on Neural Networks\\n5 (1994) 865–872.\\n[46] S. Bianco, R. Cadene, L. Celona, P. Napoletano, Benchmark analysis of representative deep\\nneural network architectures, IEEE Access 6 (2018) 64270–64277.\\n[47] S. Maji, E. Rahtu, J. Kannala, M. B. Blaschko, A. Vedaldi, Fine-Grained Visual Classiﬁcation\\nof Aircraft, Technical Report, 2013.\\n[48] C. Wah, S. Branson, P. Welinder, P. Perona, S. Belongie, The Caltech-UCSD Birds-200-2011\\nDataset, Technical Report CNS-TR-2011-001, California Institute of Technology, 2011.\\n[49] J. Krause, M. Stark, J. Deng, L. Fei-Fei, 3d object representations for ﬁne-grained categoriza-\\ntion, in: 4th International IEEE Workshop on 3D Representation and Recognition (3dRR-13),\\nSydney, Australia, 2013.\\n25\\n Pruning by Explaining: A Novel Criterion for\\nDeep Neural Network Pruning\\n— Supplementary Materials —\\nSupplementary Methods 1: Data Preprocessing\\nDuring ﬁne-tuning, images are resized to 256×256 and randomly cropped to 224×224 pixels,\\nand then horizontally ﬂipped with a random chance of 50% for data augmentation. For testing,\\nimages are resized to 224×224 pixels.\\nScene 15: The Scene 15 dataset contains about 4,485 images and consists of 15 natural scene\\ncategories obtained from COREL collection, Google image search and personal photographs [36].\\nWe ﬁne-tuned four diﬀerent models on 20% of the images from each class and achieved initial\\nTop-1 accuracy of 88.59% for VGG-16, 85.48% for AlexNet, 83.96% for ResNet-18, and 88.28%\\nfor ResNet-50, respectively.\\nEvent 8: Event-8 consists of 8 sports event categories by integrating scene and object recognition.\\nWe use 40% of the dataset’s images for ﬁne-tuning and the remaining 60% for testing. We adopted\\nthe common data augmentation method as in [26].\\nCats and Dogs: This is the Asirra dataset provided by Microsoft Research (from Kaggle).\\nThe given dataset for the competition (KMLC-Challenge-1) [38]. Training dataset contains 4,000\\ncolored images of dogs and 4,005 colored images of cats, while containing 2,023 test images. We\\nreached initial accuracies of 99.36% for VGG-16, 96.84% for AlexNet, 97.97% for ResNet-18, and\\n98.42% for ResNet-50 based on transfer learning approach.\\nOxford Flowers 102: The Oxford Flowers 102 dataset contains 102 species of ﬂower categories\\nfound in the UK, which is a collection with over 2,000 training and 6,100 test images [39]. We\\nﬁne-tuned models with pre-trained networks on ImageNet for transfer learning.\\nCIFAR-10: This dataset contains 50,000 training images and 10,000 test images spanning 10\\ncategories of objects. The resolution of each image is 32×32 pixels and therefore we resize the\\nimages as 224×224 pixels.\\nILSVRC 2012: In order to show the eﬀectiveness of the pruning criteria in the small sample\\nscenario, we pruned and tested all models on randomly selected k = 3 from 1000 classes and data\\nfrom the ImageNet corpus [40].\\nSupplementary Results 1: Additional results on toy data\\nSupplementary Table 1: Comparison of training accuracy after one-shot pruning one third of all ﬁlters w.r.t one of\\nthe four metrics on toy datasets, with n ∈ [1, 5, 20, 100] reference samples used for criteria computation for Weight,\\nGradient, Taylor and LRP. Compare to Figure 4.\\nDataset\\nUnpruned\\nPruned\\nn=1\\nn=5\\nn=20\\nn=100\\nW\\nT\\nG\\nL\\nT\\nG\\nL\\nT\\nG\\nL\\nT\\nG\\nL\\nmoon\\n99.90\\n99.60\\n79.80\\n83.07\\n85.01\\n84.70\\n86.07\\n99.86\\n86.99\\n85.87\\n99.85\\n94.77\\n93.53\\n99.85\\ncircle\\n100.00\\n97.10\\n68.35\\n69.21\\n70.23\\n87.18\\n82.23\\n99.89\\n91.87\\n85.36\\n100.00\\n97.04\\n90.88\\n100.00\\nmulti\\n94.95\\n91.00\\n34.28\\n34.28\\n62.98\\n77.34\\n67.96\\n91.85\\n83.21\\n77.39\\n91.59\\n84.76\\n82.68\\n91.25\\ni\\n Here, we discuss with Supplementary Table 2 the consistency of LRP-based neuron selection\\nacross reference sample sizes. One can assume that the larger the choice of n the less volatile is the\\nchoice of (un)important neurons by the criterion, as the inﬂuence of individual reference samples\\nis marginalized out. We therefore compare the ﬁrst and last ranked sets of neurons selected for\\na low (yet due to our observations suﬃcient) number n = 10 of reference samples to all other\\nreference sample set sizes m over all unique random seed combinations. For all comparisons of n×m\\n(except for m = 1) we observe a remarkable consistency in the selection of (un)important network\\nsubstructures. With an increasing m, we can see the consistency in neuron set selection gradually\\nincrease and then plateau for the “moon” and “circle” datasets, which means that the selected set of\\nneurons remains consistent for larger sets of reference samples from that point on. For the “mult”\\ntoy dataset, we observe a gradual yet minimal decrease in the set similarity scores for m ≥ 10,\\nwhich means that the results deviate from the selected neurons for n = 10, i.e. variability over the\\nneuron sets selected for n = 10 are the source of the volatility between n-reference and m-reference\\nselected neuron sets. In all cases, peak consistency is achieved at n ∈ {5, 10} reference samples,\\nidentifying low numbers of n ∈ {5, 10} as suﬃcient for consistently pruning our toy models.\\nSupplementary Table 2: A consistency comparison of neuron selection of LRP between reference sample sets sized n,\\naveraged over all 1225',\n",
    "  'and m-reference\\nselected neuron sets. In all cases, peak consistency is achieved at n ∈ {5, 10} reference samples,\\nidentifying low numbers of n ∈ {5, 10} as suﬃcient for consistently pruning our toy models.\\nSupplementary Table 2: A consistency comparison of neuron selection of LRP between reference sample sets sized n,\\naveraged over all 1225 unique random seed combinations. Higher values indicate higher consistency. We report results\\nfor n = 10 reference samples in comparison to m ∈ [1, 2,5, 10, 20, 50, 100, 200] reference samples per class and k = 250.\\nObservations for all other combinations of n × m, k ∈ [125, 500, 1000] and all other criteria are available on github3.\\nDataset\\nn\\nﬁrst-250\\nlast-250\\nm =\\n1\\n2\\n5\\n10\\n20\\n50\\n100\\n200\\n1\\n2\\n5\\n10\\n20\\n50\\n100\\n200\\nmoon\\n10\\n0.687\\n0.865\\n0.942\\n0.947\\n0.951\\n0.952\\n0.950\\n0.950\\n0.676\\n0.810\\n0.916\\n0.928\\n0.938\\n0.946\\n0.947\\n0.948\\ncircle\\n10\\n0.689\\n0.795\\n0.831\\n0.843\\n0.845\\n0.846\\n0.846\\n0.842\\n0.698\\n0.874\\n0.919\\n0.937\\n0.946\\n0.951\\n0.953\\n0.954\\nmult\\n10\\n0.142\\n0.625\\n0.773\\n0.791\\n0.779\\n0.765\\n0.763\\n0.762\\n0.160\\n0.697\\n0.890\\n0.942\\n0.940\\n0.936\\n0.934\\n0.933\\nSupplementary Results 2: Additional results for image processing neural networks\\nHere, we provide results for AlexNet and ResNet-18 — in addition to the VGG16 and ResNet-50\\nresults shown in the main paper — in Supplementary Figure 1 (cf. Figure 6), Supplementary Figure 2\\n(cf. Figure 5), and Supplementary Table 3 (cf. Table 3). These results demonstrate that the favorable\\npruning performance of our LRP criterion is not limited to any speciﬁc network architecture. We\\nremark that the results for CIFAR-10 show a larger robustness to higher pruning rates. This is due\\nto the fact that CIFAR-10 has the lowest resolution as dataset and little structure in its images as a\\nconsequence. The images contain components with predominantly very low frequencies. The ﬁlters,\\nwhich are covering higher frequencies, are expected to be mostly idle for CIFAR-10. This makes\\nthe pruning task less challenging. Therefore no method can clearly distinguish itself by a diﬀerent\\npruning strategy which addresses those ﬁlters which are covering the higher frequencies in images.\\nFurthermore, the ASSIRA dataset of cats and dogs may raise the concern that it is the MNIST\\nanalogue for pets, representing a rather simple problem, and for that reason the results might not be\\ntransferable to problems with larger variance. To validate our observation, we have chosen the more\\nlightweight [46] ResNet-50 model, and evaluated its pruning performance on three further datasets.\\nEach of the three datasets is composed as a binary discrimination problem obtained by fusing two\\ndatasets chosen from the following selection: FGVC aircraft [47], CUB birds [48] and Stanford cars\\n[49]. We have chosen these three datasets, as they are known from the literature, have an intrinsic\\nii\\n Supplementary Figure 1: Performance comparison of the proposed method (i.e. LRP) and other criteria on AlexNet and\\nResNet-18 with ﬁve datasets. Each point in the scatter plot corresponds to the performance at a speciﬁc pruning rate of\\ntwo criteria, where the vertical axis shows the performance of LRP and the horizontal axis the performance of one other\\ncriterion (compare to Supplementary Figure 2 that displays the same data for more than two criteria). The black dashed\\nline shows the set of points where models pruned by one of the compared criteria would exhibit identical performance\\nto LRP. For accuracy, higher values are better. For loss, lower values are better. Compare to Figure 6.\\nResNet-18\\nScene 15\\nCifar 10\\nCats and Dogs\\nEvent 8\\nOxford Flower 102\\nTest Accuracy\\nTest Accuracy\\nAlexNet\\nSupplementary Figure 2: Comparison of test accuracy in diﬀerent criteria as pruning rate increases on AlexNet (top)\\nand ResNet-18 (bottom) with ﬁve datasets. Pruning with ﬁne-tuning. Prematurely terminated lines indicate that during\\npruning, the respective criterion removed vital ﬁlters and thus disconnected the model input from the output. Compare\\nto Figure 5.\\nvariability as visible by their numbers of classes and a medium sample size. Most importantly,\\nwe know for these datasets that the object categories deﬁning their contents are matched by some\\nclasses in the ImageNet dataset which is used to initialize the weights of the ResNet-50 network.\\nSupplementary Figure 3 shows the results for the three composed discrimination problems,\\nwithout ﬁne-tuning after pruning. We can observe that each pruning method is able to remove a\\ncertain amount of network ﬁlters without notable loss of discrimination performance. Weight-based\\niii\\n Supplementary Table 3: A performance comparison between criteria (Weight, Taylor, Gradient with ℓ2-norm and LRP)\\nand the Unpruned model for AlexNet (top) and ResNet-18 (bottom) on ﬁve diﬀerent image benchmark datasets. Criteria\\nare evaluated at ﬁxed pruning rates per model and dataset, identiﬁed as ⟨dataset⟩@⟨percent pruned filters⟩%.\\nWe report test accuracy (in %), (training) loss (×10−2), number of remaining parameters (×107) and FLOPs (in Mega\\nMultiply-Accumulate Operations per Second (MMACS) for AlexNet and GMACS for ResNet-18) per forward pass.\\nFor all measures except accuracy, lower outcomes are better. Compare to Table 3.\\nAlexNet\\nScene 15 @ 55%\\nEvent 8 @ 55%\\nCats & Dogs @ 60%\\nU\\nW\\nT\\nG\\nL\\nU\\nW\\nT\\nG\\nL\\nU\\nW\\nT\\nG\\nL\\nLoss\\n2.49\\n2.78\\n2.31\\n2.46\\n2.02\\n1.16\\n1.67\\n1.21\\n1.37\\n1.00\\n0.50\\n0.77\\n0.78\\n0.87\\n0.70\\nAccuracy\\n85.48\\n78.43\\n79.40\\n78.63\\n80.76\\n94.89\\n88.10\\n88.62\\n88.42\\n90.41\\n96.84\\n95.86\\n95.23\\n94.89\\n95.81\\nParams\\n54.60\\n35.19\\n33.79\\n33.29\\n33.93\\n54.57\\n34.25\\n33.00\\n33.29\\n32.79\\n54.54\\n32.73\\n33.50\\n33.97\\n32.66\\nFLOPs\\n711.51\\n304.88\\n229.95\\n225.19\\n277.98\\n711.48\\n301.9\\n241.18\\n238.36\\n291.95\\n711.46\\n264.04\\n199.88\\n190.84\\n240.19\\nOxford Flower 102 @ 70%\\nCIFAR-10 @',\n",
    "  'Flower 102 @ 70%\\nCIFAR-10 @ 30%\\nU\\nW\\nT\\nG\\nL\\nU\\nW\\nT\\nG\\nL\\nLoss\\n5.15\\n4.83\\n3.39\\n3.77\\n3.01\\n1.95\\n2.44\\n2.46\\n2.46\\n2.33\\nAccuracy\\n78.74\\n63.93\\n64.10\\n64.11\\n65.69\\n87.83\\n90.03\\n89.48\\n89.58\\n89.87\\nParams\\n54.95\\n28.13\\n29.19\\n28.72\\n28.91\\n54.58\\n48.31\\n84.23\\n46.31\\n48.22\\nFLOPs\\n711.87\\n192.69\\n132.34\\n141.82\\n161.35\\n711.49\\n477.16\\n371.48\\n395.43\\n402.93\\nResNet-18\\nScene 15 @ 50%\\nEvent 8 @ 55%\\nCats & Dogs @ 45%\\nU\\nW\\nT\\nG\\nL\\nU\\nW\\nT\\nG\\nL\\nU\\nW\\nT\\nG\\nL\\nLoss\\n1.32\\n1.98\\n1.28\\n1.03\\n0.85\\n0.61\\n1.28\\n0.99\\n0.72\\n0.55\\n0.03\\n0.04\\n0.05\\n0.04\\n0.02\\nAccuracy\\n83.97\\n69.95\\n78.14\\n78.24\\n81.61\\n95.63\\n80.20\\n83.81\\n86.76\\n90.27\\n97.97\\n97.17\\n96.34\\n94.13\\n97.91\\nParams\\n11.18\\n4.63\\n4.91\\n4.96\\n4.52\\n11.18\\n3.99\\n4.17\\n4.26\\n3.89\\n11.18\\n4.88\\n5.18\\n5.15\\n5.04\\nFLOPs\\n1.82\\n1.30\\n1.16\\n1.10\\n1.27\\n1.82\\n1.22\\n1.11\\n1.07\\n1.20\\n1.82\\n1.36\\n1.22\\n1.21\\n1.36\\nOxford Flower 102 @ 70%\\nCIFAR-10 @ 30%\\nU\\nW\\nT\\nG\\nL\\nU\\nW\\nT\\nG\\nL\\nLoss\\n1.36\\n4.64\\n2.96\\n1.65\\n1.59\\n0.000\\n0.002\\n0.012\\n0.016\\n0.004\\nAccuracy\\n71.23\\n34.58\\n49.56\\n62.41\\n65.60\\n94.67\\n94.21\\n92.71\\n92.55\\n94.03\\nParams\\n11.23\\n2.19\\n3.07\\n3.07\\n2.45\\n11.17\\n6.88\\n7.85\\n7.56\\n6.62\\nFLOPs\\n1.82\\n0.95\\n0.72\\n0.73\\n0.93\\n0.56\\n0.46\\n0.42\\n0.41\\n0.47\\nSupplementary Figure 3: Comparison of pruning performance without a subsequent ﬁnetuning step for a ResNet-\\n50 network when pruned by criteria using weights, gradient, Taylor-expansion and LRP. Each dataset is a binary\\nclassiﬁcation problem created by combining two datasets taken from FGVC Aircraft [47], CUB-200-2011 birds [48]\\nand Stanford Cars [49], which are covered by similar classes in the ImageNet initialization. Results are the average\\nof 20 repetitions with randomly drawn samples. Each run relies on 20 samples for pruning, 10 from each of the two\\ndatasets, and 2048 samples for test accuracy evaluation. For a given repetition, all methods use the same set of samples\\nfor pruning and they use a set of samples for evaluation, which is again identical for all pruning methods, but disjoint\\nfrom the pruning set. Compare to Figure 10.\\npruning performs second best, while LRP-based pruning allows consistently to prune the largest\\nfraction of ﬁlters before starting to lose prediction accuracy.\\niv\\n When comparing cats versus dogs in Figure 10 against the three composed datasets in Supple-\\nmentary Figure 3, we observe that there is less redundant capacity which can be pruned away for the\\ncomposed datasets. This sanity check is in line with the higher variance of these composed datasets\\nas compared to cats versus dogs. As a side remark, this observation suggests the thought to measure\\nempirically dataset complexities with respect to a neural network by the area under the accuracy\\ngraph with respect to the amount of pruned ﬁlters.\\nv\\n'],\n",
    " 'running_summary': 'The paper proposes a novel pruning framework based on Layer-wise Relevance Propagation (LRP) to reduce redundancy while maintaining performance in deep CNNs. The method is evaluated on a broad range of computer vision datasets, and LRP-based criterion consistently outperforms all other criteria in all reference set sizes and datasets, and is most capable of preserving the relevant core of the learned network function and dismissing unimportant parts of the model during pruning. The consistency of LRP in selecting important neural network units for pruning and preservation is high compared to other methods, while remaining adaptive to the selection of reference samples for criterion computation. The experiments show that with a fixed pruning target of n% filters removed, LRP might not always result in the cheapest sub-network after pruning in terms of parameter count and FLOPs per inference, however, it consistently is able to identify the network components for removal and preservation leading to the best performing model after pruning. LRP is also capable of pruning any layer type efficiently, and as the pruning ratio increases, LRP-based pruning criterion can keep the test accuracy stable, even without fine-tuning.',\n",
    " 'chunk_questions': [[('What is the proposed criterion for CNN pruning?',\n",
    "    'The proposed criterion for CNN pruning is based on interpretability. It involves finding the most relevant units using relevance scores obtained from concepts of explainable AI.'),\n",
    "   ('What is Layer-wise Relevance Propagation (LRP)?',\n",
    "    'Layer-wise Relevance Propagation (LRP) is a novel pruning framework proposed in the paper. It is originally developed as an explanation method to assign importance scores to the different input dimensions of a neural network.'),\n",
    "   ('What is the problem associated with deep CNNs?',\n",
    "    'The problem associated with deep CNNs is high storage and computational costs.'),\n",
    "   ('How does the proposed method address the problem of high computational costs?',\n",
    "    'The proposed method addresses the problem of high computational costs by significantly reducing the computational complexity of deep neural networks.'),\n",
    "   ('What is transfer learning?',\n",
    "    'Transfer learning is a technique that improves the CNN performance by extracting the knowledge from the source tasks and applying it to a target task which has limited training data.'),\n",
    "   ('What is the performance of the proposed method on computer vision datasets?',\n",
    "    'The proposed method is evaluated on a broad range of computer vision datasets and can efficiently prune CNN models in transfer-learning setups while maintaining or even improving accuracy.')],\n",
    "  [('What is LRP?',\n",
    "    'LRP stands for Layer-wise Relevance Propagation. It computes relevance scores for every unit of the deep model and can be used as a pruning criterion.'),\n",
    "   ('What is the advantage of LRP criterion over previous criteria?',\n",
    "    'The LRP criterion outperforms previous criteria on all datasets if retraining is prohibited after pruning.'),\n",
    "   ('What is Scenario 1 described in the paper?',\n",
    "    'Scenario 1 is pruning pre-trained CNNs followed by subsequent fine-tuning.'),\n",
    "   ('What is Scenario 2 described in the paper?',\n",
    "    'Scenario 2 is a transfer learning scenario where a pretrained model needs to be transferred to a related problem with constraints such as insufficient data, computational power, or energy consumption.'),\n",
    "   ('What does the proposed pruning framework involve?',\n",
    "    'The proposed pruning framework involves removing redundant or irrelevant units from the model which are not critical for performance.'),\n",
    "   ('Does the proposed pruning framework have any limitations?',\n",
    "    'No, the proposed pruning framework is not limited to LRP and image data, but can be used with other explanation techniques and data types.')],\n",
    "  [('What is LRP-based pruning?',\n",
    "    'LRP-based pruning is a criterion for pruning neural network units based on relevance quantity computed with LRP.'),\n",
    "   ('What is the LRP-α1β0-rule?',\n",
    "    'The LRP-α1β0-rule is a particular variant of LRP that is rooted in DTD and computes relevance quantities used as pruning criteria.'),\n",
    "   ('What is the main characteristic of LRP?',\n",
    "    'The main characteristic of LRP is a backward pass through the network during which the network output is redistributed to all units of the network in a layer-by-layer fashion.'),\n",
    "   ('What is the purpose of network pruning?',\n",
    "    'The purpose of network pruning is to sparsify the network units by eliminating weights or filters that are non-informative according to a certain criterion.'),\n",
    "   ('What is the significance of the LRP-based pruning method proposed in the paper?',\n",
    "    'The LRP-based pruning method proposed in the paper can significantly reduce the computational complexity of deep neural networks while maintaining or even improving accuracy.')],\n",
    "  [('What is the LRP-αβ-rule?',\n",
    "    'The LRP-αβ-rule was developed with feedforward-DNNs with ReLU activations in mind and assumes positive (pre-softmax) logit activations flogit(x) > 0 for decomposition.'),\n",
    "   ('What are the advantages of LRP over other pruning techniques?',\n",
    "    'The technical advantages of LRP over other pruning techniques such as gradient-based or activation-based methods are (1) Localized relevance conservation implicitly ensures layer-wise regularized global redistribution of importances from each network unit. (2) By summing relevance within each (convolutional) ﬁlter channel, the LRP-based criterion is directly applicable as a measure of total relevance per node/ﬁlter, without requiring a post-hoc layer-wise renormalization, e.g., via lp norm. (3) The use of relevance scores is not restricted to a global application of pruning but can be easily applied to locally and (neuron- or ﬁlter-)group-wise constrained pruning without regularization.'),\n",
    "   ('How is the LRP-based assessment of neuron and filter importance performed?',\n",
    "    'The LRP-based assessment of neuron and filter importance is performed as a single LRP backward pass through the model, with an aggregation of relevance per filter channel as described above, for convolutional layers, and does not require additional normalization or regularization.'),\n",
    "   ('What is the aim of pruning the convolutional layers?',\n",
    "    'The aim of pruning the convolutional layers is to reduce FLOPs required during inference.'),\n",
    "   ('What is the aim of pruning the fully-connected layers?',\n",
    "    'The aim of pruning the fully-connected layers is to reduce the memory requirement.')],\n",
    "  [('What is the proposed pruning framework based on?',\n",
    "    'The proposed pruning framework is based on Layer-wise Relevance Propagation (LRP).'),\n",
    "   ('What is the advantage of LRP-based assessment of neuron and filter importance?',\n",
    "    'The LRP-based assessment of neuron and filter importance is performed as a single LRP backward pass through the model, and does not require additional normalization or regularization.'),\n",
    "   ('What is the LRP rule?',\n",
    "    'The LRP rule is a pruning criterion that works well with feedforward-DNNs and ReLU activations. It performs two separate relevance propagation steps per layer, exclusively considering activatory parts of forward propagated quantities and inhibitory parts, respectively. The rule is locally conservative and provides technical advantages over gradient-based or activation-based pruning techniques.'),\n",
    "   ('What is the performance of LRP-based criterion compared to other criteria?',\n",
    "    'LRP-based criterion consistently outperforms all other criteria in all reference set sizes and datasets, and is most capable of preserving the relevant core of the learned network function and dismissing unimportant parts of the model during pruning.'),\n",
    "   ('How are the pruning criteria computed?',\n",
    "    'The pruning criteria are computed using a sample of new datapoints unseen during training.')],\n",
    "  [('What is the proposed approach for pruning in the paper?',\n",
    "    'The paper proposes a novel pruning framework based on Layer-wise Relevance Propagation (LRP) to reduce redundancy while maintaining performance in deep CNNs.'),\n",
    "   ('What is the LRP rule?',\n",
    "    'The LRP rule works well with feedforward-DNNs and ReLU activations.'),\n",
    "   ('Which criterion consistently outperforms all others according to the paper?',\n",
    "    'LRP-based criterion consistently outperforms all other criteria in all reference set sizes and datasets.'),\n",
    "   ('What does Table 2 report?',\n",
    "    'Table 2 reports the self-similarity in neuron selection in the extremes of the ranking across random seeds for all criteria and toy settings.'),\n",
    "   ('What does the fragment say about the consistency of LRP in selecting important neural network units?',\n",
    "    'The consistency of LRP in selecting important neural network units for pruning and preservation is high compared to other methods, while remaining adaptive to the selection of reference samples for criterion computation.')],\n",
    "  [('What is the purpose of Layer-wise Relevance Propagation (LRP)?',\n",
    "    'The purpose of Layer-wise Relevance Propagation (LRP) is to reduce redundancy while maintaining performance in deep CNNs'),\n",
    "   ('What is the architecture of pre-trained deep CNN models discussed in the fragment?',\n",
    "    'The fragment discusses the architecture of various pre-trained deep CNN models such as VGG-16, AlexNet, ResNet-18, and ResNet-50'),\n",
    "   ('What is the process of canonization in LRP?',\n",
    "    'The process of canonization in LRP fuses a sequence of a convolution and a BN layer into a convolution layer with updated weights to make LRP scores implementation-invariant'),\n",
    "   ('How many public datasets were used in the experiments?',\n",
    "    'The experiments were performed on six public datasets'),\n",
    "   ('What are the pruning criteria evaluated on in Table 3?',\n",
    "    'The pruning criteria are evaluated on five different image benchmark datasets in Table 3'),\n",
    "   ('What does Figure 5 show?',\n",
    "    'Figure 5 shows the comparison of test accuracy in different criteria as pruning rate increases on VGG-16 and ResNet-50 with five datasets')],\n",
    "  [('What is the proposed pruning framework based on?',\n",
    "    'The proposed pruning framework is based on Layer-wise Relevance Propagation (LRP)'),\n",
    "   ('What kind of datasets were used to evaluate the performance of the proposed method?',\n",
    "    'A broad range of computer vision datasets were used to evaluate the performance of the proposed method'),\n",
    "   ('Which criterion outperforms all other criteria in all reference set sizes and datasets?',\n",
    "    'LRP-based criterion consistently outperforms all other criteria in all reference set sizes and datasets'),\n",
    "   ('What is the behavior observed for the pruning process with Event 8, Oxford Flower 102, and CIFAR-10 datasets?',\n",
    "    'Pruning leads to an initial performance increase, until a pruning rate of approx. 30% is reached. This behavior has been reported before in the literature and might stem from improvements of the model structure through elimination of filters related to classes in the source dataset that are not present in the target dataset anymore'),\n",
    "   ('What is the effect of pruning on the convolutional filters in earlier layers of the network?',\n",
    "    'As pruning rate increases, the convolutional filters in earlier layers that are associated with very generic features, such as edge and blob detectors, tend to generally be preserved as opposed to those in latter layers which are associated with abstract, task-specific features')],\n",
    "  [('What is the purpose of the experiment described in the fragment?',\n",
    "    'The experiment is a domain adaptation task from ImageNet to the Cats & Dogs dataset.'),\n",
    "   ('What is the pruning criterion used in the experiment?',\n",
    "    'The pruning criterion used in the experiment is Layer-wise Relevance Propagation (LRP), and it is compared to weight, Taylor, and gradient with l2-norm criteria.'),\n",
    "   ('Which neural network models are pruned in the experiment?',\n",
    "    'The experiment prunes VGG-16 and ResNets.'),\n",
    "   ('Can LRP-based pruning criterion maintain test accuracy while increasing the pruning ratio?',\n",
    "    'Yes, LRP-based pruning criterion can keep the test accuracy stable, even without fine-tuning, as the pruning ratio increases.'),\n",
    "   ('What is the benefit of using LRP for pruning?',\n",
    "    'LRP is capable of pruning any layer type efficiently, and it can preserve the relevant core of the learned network function and dismiss unimportant parts of the model during pruning.')],\n",
    "  [('What is LRP?',\n",
    "    'Layer-wise Relevance Propagation (LRP) is a novel pruning framework proposed to reduce redundancy while maintaining performance in deep CNNs.'),\n",
    "   ('What are the advantages of using LRP for pruning?',\n",
    "    'LRP-based criterion consistently outperforms all other criteria in all reference set sizes and datasets, and is most capable of preserving the relevant core of the learned network function and dismissing unimportant parts of the model during pruning.'),\n",
    "   ('What is the performance of LRP compared to other methods in selecting important neural network units for pruning and preservation?',\n",
    "    'The consistency of LRP in selecting important neural network units for pruning and preservation is high compared to other methods, while remaining adaptive to the selection of reference samples for criterion computation.'),\n",
    "   ('What is the effect of increasing pruning ratio on test accuracy using LRP-based pruning criterion?',\n",
    "    'LRP-based pruning criterion can keep the test accuracy stable, even without fine-tuning, as the pruning ratio increases.'),\n",
    "   ('Is LRP capable of pruning any layer type efficiently?',\n",
    "    'Yes, LRP is capable of pruning any layer type efficiently.'),\n",
    "   ('What is the drawback of LRP-based pruning criterion when compared to other criteria in terms of parameter count and FLOPs per inference?',\n",
    "    'With a fixed pruning target of n% filters removed, LRP might not always result in the cheapest sub-network after pruning in terms of parameter count and FLOPs per inference, however, it consistently is able to identify the network components for removal and preservation leading to the best performing model after pruning.')],\n",
    "  [('What is the proposed criterion for iterative pruning of CNNs?',\n",
    "    'The proposed criterion for iterative pruning of CNNs is based on the explanation method LRP.'),\n",
    "   ('What does removing units according to low LRP scores mean?',\n",
    "    'Removing units according to low LRP scores means discarding all aspects in the model that do not contribute relevance to its decision making.'),\n",
    "   ('What is the scalability of LRP?',\n",
    "    'LRP is scalable to general network structures and its computational cost is similar to the one of a gradient backward pass.'),\n",
    "   ('What does LRP criterion show in experiments?',\n",
    "    'In experiments, LRP criterion has shown favorable compression performance on a variety of datasets both with and without retraining after pruning.'),\n",
    "   ('What is the potential use of LRP besides pruning?',\n",
    "    'LRP can be used to visually interpret the model and explain individual decisions as intuitive relevance heatmaps.')],\n",
    "  [(\"What is the paper's proposed pruning framework based on?\",\n",
    "    \"The paper's proposed pruning framework is based on Layer-wise Relevance Propagation (LRP).\"),\n",
    "   ('What datasets were used to evaluate the LRP-based criterion?',\n",
    "    'A broad range of computer vision datasets were used to evaluate the LRP-based criterion.'),\n",
    "   ('What is the benefit of LRP compared to other methods in selecting important neural network units?',\n",
    "    'The consistency of LRP in selecting important neural network units for pruning and preservation is high compared to other methods, while remaining adaptive to the selection of reference samples for criterion computation.'),\n",
    "   ('Can LRP-based pruning criterion keep test accuracy stable without fine-tuning?',\n",
    "    'Yes, LRP-based pruning criterion can keep test accuracy stable even without fine-tuning as the pruning ratio increases.'),\n",
    "   ('What is the pruning target used in the experiments to evaluate LRP?',\n",
    "    'The experiments used a fixed pruning target of n% filters removed to evaluate LRP.'),\n",
    "   ('What is the cost of LRP compared to other methods in terms of parameter count and FLOPs per inference?',\n",
    "    'LRP might not always result in the cheapest sub-network after pruning in terms of parameter count and FLOPs per inference, even with a fixed pruning target of n% filters removed.')],\n",
    "  [('What is the aim of the paper?',\n",
    "    'The paper aims to propose a novel pruning framework based on Layer-wise Relevance Propagation (LRP) to reduce redundancy while maintaining performance in deep CNNs.'),\n",
    "   ('What is the consistency of LRP in selecting important neural network units for pruning and preservation?',\n",
    "    'The consistency of LRP in selecting important neural network units for pruning and preservation is high compared to other methods, while remaining adaptive to the selection of reference samples for criterion computation.'),\n",
    "   ('What is the capability of LRP-based pruning criterion as the pruning ratio increases?',\n",
    "    'As the pruning ratio increases, LRP-based pruning criterion can keep the test accuracy stable, even without fine-tuning.'),\n",
    "   ('What is the Scene 15 dataset?',\n",
    "    'The Scene 15 dataset contains about 4,485 images and consists of 15 natural scene categories obtained from COREL collection, Google image search and personal photographs.'),\n",
    "   ('What is the Cats and Dogs dataset?',\n",
    "    'The Cats and Dogs dataset contains 4,000 colored images of dogs and 4,005 colored images of cats for training and 2,023 test images.'),\n",
    "   ('What is the ILSVRC 2012 dataset?',\n",
    "    'The ILSVRC 2012 dataset contains 1000 classes and data from the ImageNet corpus, and all models were pruned and tested on randomly selected k = 3 from 1000 classes and data from the ImageNet corpus.'),\n",
    "   ('What does Supplementary Table 1 show?',\n",
    "    'Supplementary Table 1 shows a comparison of training accuracy after one-shot pruning one third of all filters with n reference samples used for criteria computation for Weight, Gradient, Taylor, and LRP.')],\n",
    "  [('What is the proposed pruning framework based on?',\n",
    "    'The proposed pruning framework is based on Layer-wise Relevance Propagation (LRP).'),\n",
    "   ('Does LRP consistently outperform other criteria?',\n",
    "    'Yes, LRP-based criterion consistently outperforms all other criteria in all reference set sizes and datasets.'),\n",
    "   ('What is LRP capable of?',\n",
    "    'LRP is capable of pruning any layer type efficiently.'),\n",
    "   ('Can LRP-based pruning criterion keep test accuracy stable?',\n",
    "    'Yes, LRP-based pruning criterion can keep the test accuracy stable, even without fine-tuning.'),\n",
    "   ('What is the consistency of LRP in selecting important neural network units for pruning and preservation compared to other methods?',\n",
    "    'The consistency of LRP in selecting important neural network units for pruning and preservation is high compared to other methods, while remaining adaptive to the selection of reference samples for criterion computation.')],\n",
    "  [('What is the paper about?',\n",
    "    'The paper proposes a novel pruning framework based on Layer-wise Relevance Propagation (LRP) to reduce redundancy while maintaining performance in deep CNNs.'),\n",
    "   ('Which criterion outperforms all other criteria in the experiments?',\n",
    "    'LRP-based criterion consistently outperforms all other criteria in all reference set sizes and datasets.'),\n",
    "   ('What is the advantage of LRP in selecting important neural network units for pruning and preservation?',\n",
    "    'The consistency of LRP in selecting important neural network units for pruning and preservation is high compared to other methods, while remaining adaptive to the selection of reference samples for criterion computation.'),\n",
    "   ('Can LRP prune any layer type efficiently?',\n",
    "    'Yes, LRP is capable of pruning any layer type efficiently.'),\n",
    "   ('What do the experiments show about LRP-based pruning criterion?',\n",
    "    'The experiments show that with a fixed pruning target of n% filters removed, LRP might not always result in the cheapest sub-network after pruning in terms of parameter count and FLOPs per inference, however, it consistently is able to identify the network components for removal and preservation leading to the best performing model after pruning.'),\n",
    "   ('What is the suggested method to measure dataset complexities with respect to a neural network?',\n",
    "    'The paper suggests measuring dataset complexities with respect to a neural network by the area under the accuracy graph with respect to the amount of pruned filters.')]]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9a14ad09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T16:55:18.166300Z",
     "start_time": "2023-05-22T16:55:18.147361Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from llama_index.data_structs.node import Node, DocumentRelationship\n",
    "from llama_index import LangchainEmbedding, ServiceContext, Document\n",
    "from llama_index import GPTTreeIndex, SimpleDirectoryReader\n",
    "from IPython.display import Markdown\n",
    "\n",
    "\n",
    "def create_index_with_nodes(chunks, embed_model, doc_id=None, ):\n",
    "    embed_model = LangchainEmbedding(embed_model)\n",
    "    prompt_helper = PromptHelper(max_input_size=4000, num_output=1, max_chunk_overlap=-1000)\n",
    "    llm_predictor = LLMPredictor(llm=OpenAI(temperature=0, model_name=\"gpt-3.5-turbo\", max_tokens=4000))\n",
    "    service_context = ServiceContext.from_defaults(\n",
    "        llm_predictor=llm_predictor,\n",
    "        embed_model=embed_model,\n",
    "        prompt_helper=prompt_helper,\n",
    "    )\n",
    "    nodes = []\n",
    "    if doc_id is None:\n",
    "        doc_id = [\"\"] * len(chunks)\n",
    "    elif isinstance(doc_id, str):\n",
    "        doc_id = [doc_id] * len(chunks)\n",
    "    else:\n",
    "        assert len(doc_id) == len(chunks) and isinstance(doc_id, (list, tuple))\n",
    "        doc_id = [str(d)+\"_\" for d in doc_id]\n",
    "    for ix, nt in enumerate(chunks):\n",
    "        node = Node(text=nt, doc_id= doc_id[ix] + str(ix))\n",
    "        if len(nodes)>1:\n",
    "            node.relationships[DocumentRelationship.PREVIOUS] = nodes[ix -1].get_doc_id()\n",
    "        nodes.append(node)\n",
    "\n",
    "    for ix, node in enumerate(nodes):\n",
    "        if ix + 1 < len(nodes):\n",
    "            node.relationships[DocumentRelationship.NEXT] = nodes[ix+1].get_doc_id()\n",
    "    \n",
    "    index = GPTVectorStoreIndex.from_documents(nodes, service_context=service_context)\n",
    "    return index\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "e0b7542a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-23T09:48:47.641044Z",
     "start_time": "2023-05-23T09:43:54.691108Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                                | 0/23 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'questions_and_answers': '\\n'\n",
      "                          'Q1. What is The Pile?\\n'\n",
      "                          'A1. The Pile is a large publicly available dataset '\n",
      "                          'composed of 24% web data, 9% Wikipedia, 4% GitHub, '\n",
      "                          'etc.\\n'\n",
      "                          '\\n'\n",
      "                          '\\n'\n",
      "                          'Q2. How does existing works determine domain '\n",
      "                          'weights?\\n'\n",
      "                          'A2. Existing works determine domain weights by '\n",
      "                          'using intuition or a set of downstream tasks.\\n'\n",
      "                          '\\n'\n",
      "                          '\\n'\n",
      "                          'Q3. How does The Pile determine domain weights?\\n'\n",
      "                          'A3. The Pile uses heuristically-chosen domain '\n",
      "                          'weights, which could be suboptimal.',\n",
      " 'summary': 'DoReMi: Optimizing Data Mixtures Speeds Up Language Model '\n",
      "            'Pretraining\\n'\n",
      "            'DoReMi first trains a small proxy model using group '\n",
      "            'distributionally robust optimization over domains to produce '\n",
      "            'domain weights (mixture proportions) without knowledge of '\n",
      "            'downstream tasks. It then resamples a dataset with these domain '\n",
      "            'weights and trains a larger, full-sized model.',\n",
      " 'summary_till_now': '* The composition of the pretraining data greatly '\n",
      "                     'affects the effectiveness of an LM.\\n'\n",
      "                     '* The Pile uses heuristically-chosen domain weights, '\n",
      "                     'which could be suboptimal.\\n'\n",
      "                     '* Existing LMs such as PaLM and GLaM tune the domain '\n",
      "                     'weights based on a set of downstream tasks, but requires '\n",
      "                     'training potentially thousands of LMs on different '\n",
      "                     'domain weights and risks overfitting to the particular '\n",
      "                     'set of downstream tasks.\\n'\n",
      "                     '\\n'\n",
      "                     'The DoReMi approach finds domain weights which result in '\n",
      "                     'good language models by first training a small proxy '\n",
      "                     'model using group distributionally robust optimization '\n",
      "                     'over domains to produce domain weights, and then using '\n",
      "                     'those domain weights to resample a dataset and train a '\n",
      "                     'larger, full-sized model.',\n",
      " 'title': 'Optimizing Data Mixtures Speeds Up Language Model Pretraining'}\n",
      "['Q1. What is The Pile?', 'A1. The Pile is a large publicly available dataset composed of 24% web data, 9% Wikipedia, 4% GitHub, etc.', 'Q2. How does existing works determine domain weights?', 'A2. Existing works determine domain weights by using intuition or a set of downstream tasks.', 'Q3. How does The Pile determine domain weights?', 'A3. The Pile uses heuristically-chosen domain weights, which could be suboptimal.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|██████▌                                                                                                                                                 | 1/23 [00:12<04:37, 12.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'questions_and_answers': 'Q1. How does DoReMi differ from other '\n",
      "                          'domain-weigthed language models?\\n'\n",
      "                          'A1. DoReMi uses distributionally robust '\n",
      "                          'optimization (DRO) to tune the domain weights '\n",
      "                          'without knowledge of downstream tasks.',\n",
      " 'summary': 'The DoReMi approach uses distributionally robust optimization to '\n",
      "            'tune domain weights for pretraining language models. First, '\n",
      "            'DoReMi trains a small reference model in a standard way. Then, it '\n",
      "            'trains a small distributionally robust language model (DRO-LM), '\n",
      "            'which minimizes the worst-case excess loss across all domains. '\n",
      "            'DoReMi then takes the averaged domain weights over DRO training '\n",
      "            'steps to train a larger LM (8B params). It consistently improves '\n",
      "            'LM training when varying the number of pretraining steps, '\n",
      "            'domains, or the number of training epochs.',\n",
      " 'summary_till_now': 'The Pile uses heuristically-chosen domain weights, which '\n",
      "                     'could be suboptimal. Existing LMs such as PaLM and GLaM '\n",
      "                     'tune the domain weights based on a set of downstream '\n",
      "                     'tasks, but requires training potentially thousands of '\n",
      "                     'LMs on different domain weights and risks overfitting to '\n",
      "                     'the particular set of downstream tasks.\\n'\n",
      "                     '\\n'\n",
      "                     'The DoReMi approach finds domain weights which result in '\n",
      "                     'good language models by first training a small proxy '\n",
      "                     'model using group distributionally robust optimization '\n",
      "                     'over domains to produce domain weights, and then using '\n",
      "                     'those domain weights to resample a dataset and train a '\n",
      "                     'larger, full-sized model.',\n",
      " 'title': 'Improving Language Model Pretraining with Domain Reweighting'}\n",
      "['Q1. How does DoReMi differ from other domain-weigthed language models?', 'A1. DoReMi uses distributionally robust optimization (DRO) to tune the domain weights without knowledge of downstream tasks.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|█████████████▏                                                                                                                                          | 2/23 [01:06<12:58, 37.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'questions_and_answers': '1. How does DoReMi approach find domain weights '\n",
      "                          'which result in good language models?\\n'\n",
      "                          'DoReMi finds domain weights which result in good '\n",
      "                          'language models by first training a small proxy '\n",
      "                          'model using group distributionally robust '\n",
      "                          'optimization over domains to produce domain '\n",
      "                          'weights, and then using those domain weights to '\n",
      "                          'resample a dataset and train a larger, full-sized '\n",
      "                          'model.\\n'\n",
      "                          '\\n'\n",
      "                          '2. What is the role of the initial domain weights '\n",
      "                          'in DoReMi approach?\\n'\n",
      "                          'The initial domain weights in DoReMi approach are '\n",
      "                          'the initial weights used to train the small '\n",
      "                          'reference model. These weights can be heuristically '\n",
      "                          'chosen or default uniform weights.',\n",
      " 'summary': 'The DoReMi approach finds domain weights which result in good '\n",
      "            'language models by first training a small proxy model using group '\n",
      "            'distributionally robust optimization over domains to produce '\n",
      "            'domain weights, and then using those domain weights to resample a '\n",
      "            \"dataset and train a larger, full-sized model. The Pile's default \"\n",
      "            'domain weights are tuned by 6.5% on generative few-shot tasks, '\n",
      "            'and DoReMi consistently improves LM training when varying the '\n",
      "            'sizes of proxy model and the main model trained with optimized '\n",
      "            'domain weights.',\n",
      " 'summary_till_now': 'The Pile uses heuristically-chosen domain weights, which '\n",
      "                     'could be suboptimal. Existing LMs such as PaLM and GLaM '\n",
      "                     'tune the domain weights based on a set of downstream '\n",
      "                     'tasks, but requires training potentially thousands of '\n",
      "                     'LMs on different domain weights and risks overfitting to '\n",
      "                     'the particular set of downstream tasks.\\n'\n",
      "                     '\\n'\n",
      "                     'The DoReMi approach finds domain weights which result in '\n",
      "                     'good language models by first training a small proxy '\n",
      "                     'model using group distributionally robust optimization '\n",
      "                     'over domains to produce domain weights, and then using '\n",
      "                     'those domain weights to resample a dataset and train a '\n",
      "                     'larger, full-sized model.\\n'\n",
      "                     '\\n'\n",
      "                     'current fragment:\\n'\n",
      "                     '\\n'\n",
      "                     'DoReMi is a technique that uses a small proxy model to '\n",
      "                     'optimize domain weights of a language modeling dataset, '\n",
      "                     'which then improves the training of a larger model. The '\n",
      "                     'proxy model is trained using the Group DRO optimizer in '\n",
      "                     'the distributionally robust language modeling (DRO-LM) '\n",
      "                     'framework, which minimizes the worst-case loss across '\n",
      "                     'domains.',\n",
      " 'title': 'DoReMi: Domain Reweighting with Minimax Optimization'}\n",
      "['1. How does DoReMi approach find domain weights which result in good language models?', 'DoReMi finds domain weights which result in good language models by first training a small proxy model using group distributionally robust optimization over domains to produce domain weights, and then using those domain weights to resample a dataset and train a larger, full-sized model.', '2. What is the role of the initial domain weights in DoReMi approach?', 'The initial domain weights in DoReMi approach are the initial weights used to train the small reference model. These weights can be heuristically chosen or default uniform weights.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|███████████████████▊                                                                                                                                    | 3/23 [01:20<08:51, 26.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'questions_and_answers': 'Q1. What is Group DRO optimizer?\\n'\n",
      "                          'A1. The Group DRO optimizer works by interleaving '\n",
      "                          'exponentiated gradient ascent updates on domain '\n",
      "                          'weights αt with gradient updates on the proxy model '\n",
      "                          'weights θt over training steps t.\\n'\n",
      "                          '\\n'\n",
      "                          'Q2. What is the intuition behind excess loss?\\n'\n",
      "                          'A2. Intuitively, the excess loss (lθ(x) − lref(x)) '\n",
      "                          'measures the headroom for the proxy model to '\n",
      "                          'improve, with respect to the reference model, on '\n",
      "                          'example x.\\n'\n",
      "                          '\\n'\n",
      "                          'Q3. What are the steps taken in Step 2 of DoReMi?\\n'\n",
      "                          'A3. Step 2 of DoReMi follows Algorithm 1, which '\n",
      "                          'samples a minibatch with uniform domain weights, '\n",
      "                          'computes the per-domain excess losses, updates the '\n",
      "                          'domain weights, and redistributes the weights to '\n",
      "                          'guarantee that each domain has a minimum weight.',\n",
      " 'summary': 'The DoReMi method uses a small proxy model to optimize domain '\n",
      "            'weights for a language modeling dataset, and then uses those '\n",
      "            'optimized weights to resample the training data and train a '\n",
      "            'larger, full-sized model. The proxy model is trained using the '\n",
      "            'Group DRO optimizer in the distributionally robust language '\n",
      "            'modeling (DRO-LM) framework, which minimizes the worst-case loss '\n",
      "            'across domains.',\n",
      " 'summary_till_now': 'DoReMi is a technique that uses a small proxy model to '\n",
      "                     'optimize domain weights of a language modeling dataset, '\n",
      "                     'which then improves the training of a larger model. The '\n",
      "                     'proxy model is trained using the Group DRO optimizer in '\n",
      "                     'the distributionally robust language modeling (DRO-LM) '\n",
      "                     'framework, which minimizes the worst-case loss across '\n",
      "                     'domains.',\n",
      " 'title': 'Improving Language Modeling with Distributionally Robust '\n",
      "          'Optimization\\n'\n",
      "          '\\n'\n",
      "          'final outcome:\\n'\n",
      "          'Improving Language Modeling with Distributionally Robust '\n",
      "          'Optimization'}\n",
      "['Q1. What is Group DRO optimizer?', 'A1. The Group DRO optimizer works by interleaving exponentiated gradient ascent updates on domain weights αt with gradient updates on the proxy model weights θt over training steps t.', 'Q2. What is the intuition behind excess loss?', 'A2. Intuitively, the excess loss (lθ(x) − lref(x)) measures the headroom for the proxy model to improve, with respect to the reference model, on example x.', 'Q3. What are the steps taken in Step 2 of DoReMi?', 'A3. Step 2 of DoReMi follows Algorithm 1, which samples a minibatch with uniform domain weights, computes the per-domain excess losses, updates the domain weights, and redistributes the weights to guarantee that each domain has a minimum weight.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|██████████████████████████▍                                                                                                                             | 4/23 [01:37<07:06, 22.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'questions_and_answers': '1. What is DoReMi?\\n'\n",
      "                          'DoReMi is a technique that uses a small proxy model '\n",
      "                          'to optimize domain weights of a language modeling '\n",
      "                          'dataset, which then improves the training of a '\n",
      "                          'larger model.\\n'\n",
      "                          '2. How does DoReMi work?\\n'\n",
      "                          'DoReMi works by using a proxy model that is trained '\n",
      "                          'using the Group DRO optimizer in the '\n",
      "                          'distributionally robust language modeling (DRO-LM) '\n",
      "                          'framework, which minimizes the worst-case loss '\n",
      "                          'across domains.\\n'\n",
      "                          '3. What datasets does DoReMi use?\\n'\n",
      "                          'DoReMi uses The Pile dataset and the GLaM dataset.\\n'\n",
      "                          '4. What improvements does DoReMi provide?\\n'\n",
      "                          'DoReMi improves perplexity on every domain, '\n",
      "                          'improves average downstream accuracy on generative '\n",
      "                          'one-shot tasks by 6.5%, and achieves the baseline '\n",
      "                          'accuracy 2.6x faster on The Pile. On the GLaM '\n",
      "                          'dataset, iterated DoReMi (round 2) attains '\n",
      "                          'comparable performance to domain weights tuned with '\n",
      "                          'downstream tasks.',\n",
      " 'summary': 'DoReMi is a technique that uses a small proxy model to optimize '\n",
      "            'domain weights of a language modeling dataset, which then '\n",
      "            'improves the training of a larger model. The proxy model is '\n",
      "            'trained using the Group DRO optimizer in the distributionally '\n",
      "            'robust language modeling (DRO-LM) framework, which minimizes the '\n",
      "            'worst-case loss across domains.',\n",
      " 'summary_till_now': 'DoReMi is a technique that uses a small proxy model to '\n",
      "                     'optimize domain weights of a language modeling dataset, '\n",
      "                     'which then improves the training of a larger model. The '\n",
      "                     'proxy model is trained using the Group DRO optimizer in '\n",
      "                     'the distributionally robust language modeling (DRO-LM) '\n",
      "                     'framework, which minimizes the worst-case loss across '\n",
      "                     'domains.',\n",
      " 'title': 'DoReMi Improves LM Training Efficiency and Performance'}\n",
      "['1. What is DoReMi?', 'DoReMi is a technique that uses a small proxy model to optimize domain weights of a language modeling dataset, which then improves the training of a larger model.', '2. How does DoReMi work?', 'DoReMi works by using a proxy model that is trained using the Group DRO optimizer in the distributionally robust language modeling (DRO-LM) framework, which minimizes the worst-case loss across domains.', '3. What datasets does DoReMi use?', 'DoReMi uses The Pile dataset and the GLaM dataset.', '4. What improvements does DoReMi provide?', 'DoReMi improves perplexity on every domain, improves average downstream accuracy on generative one-shot tasks by 6.5%, and achieves the baseline accuracy 2.6x faster on The Pile. On the GLaM dataset, iterated DoReMi (round 2) attains comparable performance to domain weights tuned with downstream tasks.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|█████████████████████████████████                                                                                                                       | 5/23 [01:46<05:20, 17.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'questions_and_answers': '\\n'\n",
      "                          '1. How does the DoReMi technique improve the '\n",
      "                          'downstream accuracy of language modeling datasets?\\n'\n",
      "                          'The proxy model is trained using the Group DRO '\n",
      "                          'optimizer in the distributionally robust language '\n",
      "                          'modeling (DRO-LM) framework, which minimizes the '\n",
      "                          'worst-case loss across domains.\\n'\n",
      "                          '\\n'\n",
      "                          '2. How does the DoReMi technique improve the '\n",
      "                          'training of a larger model?\\n'\n",
      "                          'The proxy model is trained using the Group DRO '\n",
      "                          'optimizer in the distributionally robust language '\n",
      "                          'modeling (DRO-LM) framework, which minimizes the '\n",
      "                          'worst-case loss across domains.',\n",
      " 'summary': 'DoReMi is a technique that uses a small proxy model to optimize '\n",
      "            'domain weights of a language modeling dataset, which then '\n",
      "            'improves the training of a larger model. The proxy model is '\n",
      "            'trained using the Group DRO optimizer in the distributionally '\n",
      "            'robust language modeling (DRO-LM) framework, which minimizes the '\n",
      "            'worst-case loss across domains. DoReMi improves both the '\n",
      "            'perplexity and downstream accuracy of 8B models trained on The '\n",
      "            'Pile and the GLaM dataset over their respective baseline domain '\n",
      "            'weights. Downstream accuracy improves on The Pile.',\n",
      " 'summary_till_now': 'DoReMi is a technique that uses a small proxy model to '\n",
      "                     'optimize domain weights of a language modeling dataset, '\n",
      "                     'which then improves the training of a larger model. The '\n",
      "                     'proxy model is trained using the Group DRO optimizer in '\n",
      "                     'the distributionally robust language modeling (DRO-LM) '\n",
      "                     'framework, which minimizes the worst-case loss across '\n",
      "                     'domains.\\n'\n",
      "                     '\\n'\n",
      "                     'current fragment:\\n'\n",
      "                     '\\n'\n",
      "                     'We show that DoReMi significantly improves both the '\n",
      "                     'perplexity and downstream accuracy of 8B models trained '\n",
      "                     'on The Pile and the GLaM dataset over their respective '\n",
      "                     'baseline domain weights. Downstream accuracy improves on '\n",
      "                     'The Pile.\\n'\n",
      "                     '\\n'\n",
      "                     'DoReMi improves downstream performance by 6.5% and '\n",
      "                     'achieves the baseline accuracy within 75k steps - 2.6x '\n",
      "                     'faster than the baseline (200k steps). Thus, DoReMi can '\n",
      "                     'train language models faster and achieve baseline '\n",
      "                     'accuracy within a shorter amount of time.',\n",
      " 'title': '\"DoReMi: Optimizing Domain Weights in Language Models\"'}\n",
      "['1. How does the DoReMi technique improve the downstream accuracy of language modeling datasets?', 'The proxy model is trained using the Group DRO optimizer in the distributionally robust language modeling (DRO-LM) framework, which minimizes the worst-case loss across domains.', '2. How does the DoReMi technique improve the training of a larger model?', 'The proxy model is trained using the Group DRO optimizer in the distributionally robust language modeling (DRO-LM) framework, which minimizes the worst-case loss across domains.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|███████████████████████████████████████▋                                                                                                                | 6/23 [01:58<04:29, 15.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'questions_and_answers': 'Q1. Does DoReMi achieve better performance than the '\n",
      "                          'baseline across all domains?\\n'\n",
      "                          'Yes, DoReMi achieves better performance than the '\n",
      "                          'baseline across all domains. Figure 3 (left) shows '\n",
      "                          'that DoReMi improves downstream performance by 6.5% '\n",
      "                          'and achieves the baseline accuracy within 75k steps '\n",
      "                          '- 2.6x faster than the baseline (200k steps). Thus, '\n",
      "                          'DoReMi can dramatically speed up training and '\n",
      "                          'improve downstream performance.\\n'\n",
      "                          '\\n'\n",
      "                          'Q2. How does DoReMi improve perplexity across all '\n",
      "                          'domains without a tradeoff?\\n'\n",
      "                          'Intuitively, the domains with the lowest and '\n",
      "                          'highest entropy can be downweighted without '\n",
      "                          'affecting the perplexity much. The domains with the '\n",
      "                          'lowest entropy statistically require few samples to '\n",
      "                          'learn, and the highest entropy domains have token '\n",
      "                          'distributions that are close to common uniform '\n",
      "                          'priors. Thus, DoReMi allocates less samples to fit '\n",
      "                          'the domains with the lowest and highest entropy, '\n",
      "                          'and transfers the samples to medium entropy domains '\n",
      "                          'to improve perplexity on all domains.\\n'\n",
      "                          '\\n'\n",
      "                          'Q3. How does DoReMi achieve performance of '\n",
      "                          'downstream-tuned weights on the GLaM dataset?\\n'\n",
      "                          'DoReMi employs iterated DoReMi on the GLaM dataset '\n",
      "                          'over 3 rounds. We find that the 2nd round and 3rd '\n",
      "                          'round domain weights are almost identical. Figure 3 '\n",
      "                          '(right) shows one-shot results for the first two '\n",
      "                          'rounds of iterated DoReMi. After the first round, '\n",
      "                          'the DoReMi main model has comparable downstream '\n",
      "                          'accuracy to the baseline (uniform domain weights), '\n",
      "                          'and after the second round, the DoReMi main model '\n",
      "                          'achieves comparable downstream accuracy to the '\n",
      "                          'downstream-tuned domain weights. Overall, domain '\n",
      "                          'reweighting has a smaller effect on GLaM, possibly '\n",
      "                          'because there are only 8 domains compared to 22 in '\n",
      "                          'The Pile.\\n'\n",
      "                          '\\n'\n",
      "                          'Q4. What is the effect of DoReMi on the GLaM '\n",
      "                          'dataset?\\n'\n",
      "                          'DoReMi has a smaller effect on GLaM, possibly '\n",
      "                          'because there are only 8 domains compared to 22 in '\n",
      "                          'The Pile. Inspecting the DoReMi domain weights, we '\n",
      "                          'find that DoReMi achieves similar domain weights as '\n",
      "                          'the downstream-tuned weights on the GLaM dataset. '\n",
      "                          'DoReMi is able to optimize domain weights without '\n",
      "                          'using downstream data, and recovers a similar set '\n",
      "                          'of domain weights to the downstream-tuned weights.',\n",
      " 'summary': 'We present DoReMi, a technique that uses a small proxy model to '\n",
      "            'optimize domain weights of a language modeling dataset, which '\n",
      "            'then improves the training of a larger model. DoReMi improves the '\n",
      "            'downstream accuracy and perplexity of 8B models trained on The '\n",
      "            'Pile and the GLaM dataset. DoReMi significantly reduces the '\n",
      "            'perplexity across all domains, despite allocating lower weight to '\n",
      "            'some domains. DoReMi (280M) converges within 3 rounds, with a '\n",
      "            'similar overall pattern to domain weights tuned on downstream '\n",
      "            'tasks. DoReMi is able to identify similar domain weights to those '\n",
      "            'of downstream-tuned weights starting from uniform initial domain '\n",
      "            'weights, without any use of downstream data.',\n",
      " 'summary_till_now': 'DoReMi is a technique that uses a small proxy model to '\n",
      "                     'optimize domain weights of a language modeling dataset, '\n",
      "                     'which then improves the training of a larger model. The '\n",
      "                     'proxy model is trained using the Group DRO optimizer in '\n",
      "                     'the distributionally robust language modeling (DRO-LM) '\n",
      "                     'framework, which minimizes the worst-case loss across '\n",
      "                     'domains.\\n'\n",
      "                     '\\n'\n",
      "                     'current fragment:\\n'\n",
      "                     '\\n'\n",
      "                     'We show that DoReMi significantly improves both the '\n",
      "                     'perplexity and downstream accuracy of 8B models trained '\n",
      "                     'on The Pile and the GLaM dataset over their respective '\n",
      "                     'baseline domain weights. Downstream accuracy improves on '\n",
      "                     'The Pile. DoReMi can improve training speed and improve '\n",
      "                     'downstream performance.\\n'\n",
      "                     '\\n'\n",
      "                     'DoReMi can reduce perplexity across all domains without '\n",
      "                     'a tradeoff. DoReMi can dramatically speed up training '\n",
      "                     'and improve downstream performance. DoReMi improves '\n",
      "                     'downstream performance by 6.5% and achieves the baseline '\n",
      "                     'accuracy within 75k steps - 2.6x faster than the '\n",
      "                     'baseline (200k steps).\\n'\n",
      "                     '\\n'\n",
      "                     '\\n'\n",
      "                     'Running summary:\\n'\n",
      "                     '\\n'\n",
      "                     'DoReMi is a technique that uses a small proxy model to '\n",
      "                     'optimize domain weights of a language modeling dataset, '\n",
      "                     'which then improves the training of a larger model. The '\n",
      "                     'proxy model is trained using the Group DRO optimizer in '\n",
      "                     'the distributionally robust language modeling (DRO-LM) '\n",
      "                     'framework, which minimizes the worst-case loss across '\n",
      "                     'domains. We show that DoReMi significantly improves both '\n",
      "                     'the perplexity and downstream accuracy of 8B models '\n",
      "                     'trained on The Pile and the GLaM dataset over their '\n",
      "                     'respective baseline domain weights. Downstream accuracy '\n",
      "                     'improves on The Pile.',\n",
      " 'title': '\"DoReMi Dramatically Speeds Up Language Model Training\"'}\n",
      "['Q1. Does DoReMi achieve better performance than the baseline across all domains?', 'Yes, DoReMi achieves better performance than the baseline across all domains. Figure 3 (left) shows that DoReMi improves downstream performance by 6.5% and achieves the baseline accuracy within 75k steps - 2.6x faster than the baseline (200k steps). Thus, DoReMi can dramatically speed up training and improve downstream performance.', 'Q2. How does DoReMi improve perplexity across all domains without a tradeoff?', 'Intuitively, the domains with the lowest and highest entropy can be downweighted without affecting the perplexity much. The domains with the lowest entropy statistically require few samples to learn, and the highest entropy domains have token distributions that are close to common uniform priors. Thus, DoReMi allocates less samples to fit the domains with the lowest and highest entropy, and transfers the samples to medium entropy domains to improve perplexity on all domains.', 'Q3. How does DoReMi achieve performance of downstream-tuned weights on the GLaM dataset?', 'DoReMi employs iterated DoReMi on the GLaM dataset over 3 rounds. We find that the 2nd round and 3rd round domain weights are almost identical. Figure 3 (right) shows one-shot results for the first two rounds of iterated DoReMi. After the first round, the DoReMi main model has comparable downstream accuracy to the baseline (uniform domain weights), and after the second round, the DoReMi main model achieves comparable downstream accuracy to the downstream-tuned domain weights. Overall, domain reweighting has a smaller effect on GLaM, possibly because there are only 8 domains compared to 22 in The Pile.', 'Q4. What is the effect of DoReMi on the GLaM dataset?', 'DoReMi has a smaller effect on GLaM, possibly because there are only 8 domains compared to 22 in The Pile. Inspecting the DoReMi domain weights, we find that DoReMi achieves similar domain weights as the downstream-tuned weights on the GLaM dataset. DoReMi is able to optimize domain weights without using downstream data, and recovers a similar set of domain weights to the downstream-tuned weights.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|██████████████████████████████████████████████▎                                                                                                         | 7/23 [02:19<04:40, 17.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'questions_and_answers': 'How does DoReMi improve downstream performance on '\n",
      "                          'the GLaM dataset?\\n'\n",
      "                          'DoReMi improves downstream performance by 6.5% and '\n",
      "                          'achieves the baseline accuracy within 75k steps - '\n",
      "                          '2.6x faster than the baseline (200k steps).\\n'\n",
      "                          '\\n'\n",
      "                          'How does the size of the proxy model affect the '\n",
      "                          'performance of the main model?\\n'\n",
      "                          'The gap between the proxy and main model increases '\n",
      "                          'with scale, as the 1B proxy model not only '\n",
      "                          'underperforms the 1B main model but also the 1B '\n",
      "                          'baseline model, while the 280M proxy model achieves '\n",
      "                          'better perplexity than the 280M baseline model on '\n",
      "                          '19/22 domains.\\n'\n",
      "                          '\\n'\n",
      "                          'Does DoReMi consistently improve downstream '\n",
      "                          'accuracy?\\n'\n",
      "                          'DoReMi consistently improves downstream accuracy '\n",
      "                          'across scales, with a similar 3% accuracy gap at '\n",
      "                          '200k steps at most scales (except for 510M).',\n",
      " 'summary': 'DoReMi consistently improves downstream accuracy across scales, '\n",
      "            'with a similar 2% accuracy gap at 200k steps at most scales '\n",
      "            '(except for 510M). DoReMi achieves the baseline accuracy 4x '\n",
      "            'faster on average across scales. The gap between proxy model and '\n",
      "            'main model increases with scale. The 1B proxy model not only '\n",
      "            'underperforms the 1B main model but also the 1B baseline model. '\n",
      "            'However, DoReMi is quite robust to any suboptimalities in the '\n",
      "            'minimax optimization procedure.',\n",
      " 'summary_till_now': 'DoReMi is a technique that uses a small proxy model to '\n",
      "                     'optimize domain weights of a language modeling dataset, '\n",
      "                     'which then improves the training of a larger model. The '\n",
      "                     'proxy model is trained using the Group DRO optimizer in '\n",
      "                     'the distributionally robust language modeling (DRO-LM) '\n",
      "                     'framework, which minimizes the worst-case loss across '\n",
      "                     'domains. We show that DoReMi significantly improves both '\n",
      "                     'the perplexity and downstream accuracy of 8B models '\n",
      "                     'trained on The Pile and the GLaM dataset over their '\n",
      "                     'respective baseline domain weights.\\n'\n",
      "                     '\\n'\n",
      "                     'current fragment:\\n'\n",
      "                     '\\n'\n",
      "                     'We show that DoReMi significantly improves both the '\n",
      "                     'perplexity and downstream accuracy of 8B models trained '\n",
      "                     'on The Pile and the GLaM dataset over their respective '\n",
      "                     'baseline domain weights. DoReMi improves training speed '\n",
      "                     'and improves downstream performance. DoReMi can reduce '\n",
      "                     'perplexity across all domains without a tradeoff. DoReMi '\n",
      "                     'can dramatically speed up training and improve '\n",
      "                     'downstream performance. DoReMi improves downstream '\n",
      "                     'performance by 6.5% and achieves the baseline accuracy '\n",
      "                     'within 75k steps - 2.6x faster than the baseline (200k '\n",
      "                     'steps).',\n",
      " 'title': 'DoReMi consistently improves downstream accuracy across model '\n",
      "          'scales'}\n",
      "['How does DoReMi improve downstream performance on the GLaM dataset?', 'DoReMi improves downstream performance by 6.5% and achieves the baseline accuracy within 75k steps - 2.6x faster than the baseline (200k steps).', 'How does the size of the proxy model affect the performance of the main model?', 'The gap between the proxy and main model increases with scale, as the 1B proxy model not only underperforms the 1B main model but also the 1B baseline model, while the 280M proxy model achieves better perplexity than the 280M baseline model on 19/22 domains.', 'Does DoReMi consistently improve downstream accuracy?', 'DoReMi consistently improves downstream accuracy across scales, with a similar 3% accuracy gap at 200k steps at most scales (except for 510M).']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|████████████████████████████████████████████████████▊                                                                                                   | 8/23 [02:34<04:11, 16.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'questions_and_answers': 'Q: How does DoReMi improve training speed?\\n'\n",
      "                          'A: The Group DRO optimizer is 2.6x faster than the '\n",
      "                          'baseline (200k steps).\\n'\n",
      "                          '\\n'\n",
      "                          'Q: How does DoReMi improve downstream performance?\\n'\n",
      "                          'A: It improves downstream performance by 6.5% and '\n",
      "                          'achieves the baseline accuracy within 75k steps.',\n",
      " 'summary': 'DoReMi is a technique that uses a small proxy model to optimize '\n",
      "            'domain weights of a language modeling dataset, which then '\n",
      "            'improves the training of a larger model. The proxy model is '\n",
      "            'trained using the Group DRO optimizer in the distributionally '\n",
      "            'robust language modeling (DRO-LM) framework, which minimizes the '\n",
      "            'worst-case loss across domains. We show that DoReMi significantly '\n",
      "            'improves both the perplexity and downstream accuracy of 8B models '\n",
      "            'trained on The Pile and the GLaM dataset over their respective '\n",
      "            'baseline domain weights.',\n",
      " 'summary_till_now': 'DoReMi is a technique that uses a small proxy model to '\n",
      "                     'optimize domain weights of a language modeling dataset, '\n",
      "                     'which then improves the training of a larger model. The '\n",
      "                     'proxy model is trained using the Group DRO optimizer in '\n",
      "                     'the distributionally robust language modeling (DRO-LM) '\n",
      "                     'framework, which minimizes the worst-case loss across '\n",
      "                     'domains. We show that DoReMi significantly improves both '\n",
      "                     'the perplexity and downstream accuracy of 8B models '\n",
      "                     'trained on The Pile and the GLaM dataset over their '\n",
      "                     'respective baseline domain weights.',\n",
      " 'title': 'DoReMi improves training speed and improves downstream performance.'}\n",
      "['Q: How does DoReMi improve training speed?', 'A: The Group DRO optimizer is 2.6x faster than the baseline (200k steps).', 'Q: How does DoReMi improve downstream performance?', 'A: It improves downstream performance by 6.5% and achieves the baseline accuracy within 75k steps.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 39%|███████████████████████████████████████████████████████████▍                                                                                            | 9/23 [02:46<03:30, 15.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'questions_and_answers': '1. How does DoReMi differ from other data selection '\n",
      "                          'methods?\\n'\n",
      "                          'Answer: DoReMi sets domain weights automatically '\n",
      "                          'with only 2 small LM training runs and does not '\n",
      "                          'make assumptions about the type of data '\n",
      "                          '(Wikipedia-like, etc.).\\n'\n",
      "                          '2. How does DoReMi use distributionally robust '\n",
      "                          'optimization to optimize the data for training '\n",
      "                          'larger models more efficiently?\\n'\n",
      "                          'Answer: DoReMi uses the Group DRO optimizer which '\n",
      "                          \"doesn't require subselection, unlike DRO-LM which \"\n",
      "                          'updates the model on a worst-case subset of each '\n",
      "                          'minibatch.',\n",
      " 'summary': 'DoReMi is a technique that uses a small proxy model to optimize '\n",
      "            'domain weights of a language modeling dataset, which then '\n",
      "            'improves the training of a larger model. The proxy model is '\n",
      "            'trained using the Group DRO optimizer in the distributionally '\n",
      "            'robust language modeling (DRO-LM) framework, which minimizes the '\n",
      "            'worst-case loss across domains. We show that DoReMi significantly '\n",
      "            'improves both the perplexity and downstream accuracy of 8B models '\n",
      "            'trained on The Pile and the GLaM dataset over their respective '\n",
      "            'baseline domain weights.',\n",
      " 'summary_till_now': 'DoReMi is a technique that uses a small proxy model to '\n",
      "                     'optimize domain weights of a language modeling dataset, '\n",
      "                     'which then improves the training of a larger model.\\n'\n",
      "                     '\\n'\n",
      "                     'current fragment:\\n'\n",
      "                     '\\n'\n",
      "                     'DoReMi sets domain weights automatically with only 2 '\n",
      "                     'small LM training runs and do not make assumptions about '\n",
      "                     'the type of data to prefer (Wikipedia-like, etc.).',\n",
      " 'title': 'DoReMi: Optimizing Language Modeling Datasets for Pretraining'}\n",
      "['1. How does DoReMi differ from other data selection methods?', 'Answer: DoReMi sets domain weights automatically with only 2 small LM training runs and does not make assumptions about the type of data (Wikipedia-like, etc.).', '2. How does DoReMi use distributionally robust optimization to optimize the data for training larger models more efficiently?', \"Answer: DoReMi uses the Group DRO optimizer which doesn't require subselection, unlike DRO-LM which updates the model on a worst-case subset of each minibatch.\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|█████████████████████████████████████████████████████████████████▋                                                                                     | 10/23 [02:56<02:56, 13.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'questions_and_answers': '1. What is DoReMi?\\n'\n",
      "                          'DoReMi is an algorithm that reweights data domains '\n",
      "                          'for training language models.\\n'\n",
      "                          '2. How does DoReMi work?\\n'\n",
      "                          'DoReMi sets domain weights automatically with only '\n",
      "                          '2 small LM training runs and do not make '\n",
      "                          'assumptions about the type of data to prefer '\n",
      "                          '(Wikipedia-like, etc.).\\n'\n",
      "                          '3. What are the benefits of using DoReMi?\\n'\n",
      "                          'A future direction for saving compute would be to '\n",
      "                          'stop running DoReMi at an early step and '\n",
      "                          'extrapolate the domain weights for the desired '\n",
      "                          'number of steps, since we found that most of the '\n",
      "                          'variation in the domain weights during a DoReMi run '\n",
      "                          'seems to occur in the beginning of training '\n",
      "                          '(Appendix Figure 8).',\n",
      " 'summary': 'DoReMi sets domain weights automatically with only 2 small LM '\n",
      "            'training runs and do not make assumptions about the type of data '\n",
      "            'to prefer (Wikipedia-like, etc.). A future direction for saving '\n",
      "            'compute would be to stop running DoReMi at an early step and '\n",
      "            'extrapolate the domain weights for the desired number of steps, '\n",
      "            'since we found that most of the variation in the domain weights '\n",
      "            'during a DoReMi run seems to occur in the beginning of training '\n",
      "            '(Appendix Figure 8). The choice of reference model can affect the '\n",
      "            'domain weights found by DoReMi. For example, iterated DoReMi '\n",
      "            '(Section 3) improves performance by using a reference model '\n",
      "            'trained on the tuned domain weights from a previous round of '\n",
      "            'DoReMi. Further directions include varying the reference model '\n",
      "            'size and using specialized reference models to optimizing domain '\n",
      "            'weights for a specific application area.',\n",
      " 'summary_till_now': 'DoReMi is a technique that uses a small proxy model to '\n",
      "                     'optimize domain weights of a language modeling dataset, '\n",
      "                     'which then improves the training of a larger model.\\n'\n",
      "                     '\\n'\n",
      "                     'current fragment:\\n'\n",
      "                     '\\n'\n",
      "                     'DoReMi sets domain weights automatically with only 2 '\n",
      "                     'small LM training runs and do not make assumptions about '\n",
      "                     'the type of data to prefer (Wikipedia-like, etc.).',\n",
      " 'title': '\"Automatic Domain Weight Optimization for Language Modeling\"'}\n",
      "['1. What is DoReMi?', 'DoReMi is an algorithm that reweights data domains for training language models.', '2. How does DoReMi work?', 'DoReMi sets domain weights automatically with only 2 small LM training runs and do not make assumptions about the type of data to prefer (Wikipedia-like, etc.).', '3. What are the benefits of using DoReMi?', 'A future direction for saving compute would be to stop running DoReMi at an early step and extrapolate the domain weights for the desired number of steps, since we found that most of the variation in the domain weights during a DoReMi run seems to occur in the beginning of training (Appendix Figure 8).']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|████████████████████████████████████████████████████████████████████████▏                                                                              | 11/23 [03:09<02:41, 13.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'questions_and_answers': 'Q: What is DoReMi?\\n'\n",
      "                          'A: DoReMi is a technique that uses a small proxy '\n",
      "                          'model to optimize domain weights of a language '\n",
      "                          'modeling dataset.',\n",
      " 'summary': 'DoReMi is a technique that uses a small proxy model to optimize '\n",
      "            'domain weights of a language modeling dataset, which then '\n",
      "            'improves the training of a larger model. DoReMi sets domain '\n",
      "            'weights automatically with only 2 small LM training runs and do '\n",
      "            'not make assumptions about the type of data to prefer '\n",
      "            '(Wikipedia-like, etc.).',\n",
      " 'summary_till_now': 'DoReMi is a technique that uses a small proxy model to '\n",
      "                     'optimize domain weights of a language modeling dataset, '\n",
      "                     'which then improves the training of a larger model.\\n'\n",
      "                     '\\n'\n",
      "                     'Current fragment:\\n'\n",
      "                     '\\n'\n",
      "                     'DoReMi sets domain weights automatically with only 2 '\n",
      "                     'small LM training runs and do not make assumptions about '\n",
      "                     'the type of data to prefer (Wikipedia-like, etc.).',\n",
      " 'title': 'DoReMi sets domain weights automatically with only 2 small LM '\n",
      "          'training runs.'}\n",
      "['Q: What is DoReMi?', 'A: DoReMi is a technique that uses a small proxy model to optimize domain weights of a language modeling dataset.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 52%|██████████████████████████████████████████████████████████████████████████████▊                                                                        | 12/23 [03:16<02:04, 11.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'questions_and_answers': 'Question1: What is DoReMi?\\n'\n",
      "                          'Answer 1: DoReMi sets domain weights automatically '\n",
      "                          'with only 2 small LM training runs and do not make '\n",
      "                          'assumptions about the type of data to prefer '\n",
      "                          '(Wikipedia-like, etc.).',\n",
      " 'summary': 'DoReMi sets domain weights automatically with only 2 small LM '\n",
      "            'training runs and do not make assumptions about the type of data '\n",
      "            'to prefer (Wikipedia-like, etc.).',\n",
      " 'summary_till_now': 'DoReMi is a technique that uses a small proxy model to '\n",
      "                     'optimize domain weights of a language modeling dataset, '\n",
      "                     'which then improves the training of a larger model.\\n'\n",
      "                     '\\n'\n",
      "                     'Current fragment:\\n'\n",
      "                     '\\n'\n",
      "                     'DoReMi sets domain weights automatically with only 2 '\n",
      "                     'small LM training runs and do not make assumptions about '\n",
      "                     'the type of data to prefer (Wikipedia-like, etc.).',\n",
      " 'title': 'DoReMi: Automatic Document Selection for Efficient Encoder '\n",
      "          'Pretraining'}\n",
      "['Question1: What is DoReMi?', 'Answer 1: DoReMi sets domain weights automatically with only 2 small LM training runs and do not make assumptions about the type of data to prefer (Wikipedia-like, etc.).']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|█████████████████████████████████████████████████████████████████████████████████████▎                                                                 | 13/23 [03:24<01:43, 10.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'questions_and_answers': 'Q1. What is DoReMi technique?\\n'\n",
      "                          'A1. DoReMi is a technique that uses a small proxy '\n",
      "                          'model to optimize domain weights of a language '\n",
      "                          'modeling dataset, which then improves the training '\n",
      "                          'of a larger model.',\n",
      " 'summary': 'DoReMi is a technique that uses a small proxy model to optimize '\n",
      "            'domain weights of a language modeling dataset, which then '\n",
      "            'improves the training of a larger model. Current fragment: DoReMi '\n",
      "            'sets domain weights automatically with only 2 small LM training '\n",
      "            'runs and do not make assumptions about the type of data to prefer '\n",
      "            '(Wikipedia-like, etc.).',\n",
      " 'summary_till_now': 'DoReMi is a technique that uses a small proxy model to '\n",
      "                     'optimize domain weights of a language modeling dataset, '\n",
      "                     'which then improves the training of a larger model.\\n'\n",
      "                     '\\n'\n",
      "                     'Current fragment:\\n'\n",
      "                     '\\n'\n",
      "                     'DoReMi sets domain weights automatically with only 2 '\n",
      "                     'small LM training runs and do not make assumptions about '\n",
      "                     'the type of data to prefer (Wikipedia-like, etc.).',\n",
      " 'title': 'DoReMi sets domain weights automatically with only 2 small LM '\n",
      "          'training runs and do not make assumptions about the type of data to '\n",
      "          'prefer (Wikipedia-like, etc.).'}\n",
      "['Q1. What is DoReMi technique?', 'A1. DoReMi is a technique that uses a small proxy model to optimize domain weights of a language modeling dataset, which then improves the training of a larger model.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 61%|███████████████████████████████████████████████████████████████████████████████████████████▉                                                           | 14/23 [03:32<01:27,  9.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'questions_and_answers': 'Q1. What is DoReMi?\\n'\n",
      "                          'A1. DoReMi is a technique that uses a small proxy '\n",
      "                          'model to optimize domain weights of a language '\n",
      "                          'modeling dataset, which then improves the training '\n",
      "                          'of a larger model.\\n'\n",
      "                          '\\n'\n",
      "                          'Q2. How does DoReMi work?\\n'\n",
      "                          'A2. DoReMi sets domain weights automatically with '\n",
      "                          'only 2 small LM training runs and do not make '\n",
      "                          'assumptions about the type of data to prefer '\n",
      "                          '(Wikipedia-like, etc.).',\n",
      " 'summary': 'DoReMi is a technique that uses a small proxy model to optimize '\n",
      "            'domain weights of a language modeling dataset, which then '\n",
      "            'improves the training of a larger model. DoReMi sets domain '\n",
      "            'weights automatically with only 2 small LM training runs and do '\n",
      "            'not make assumptions about the type of data to prefer '\n",
      "            '(Wikipedia-like, etc.).',\n",
      " 'summary_till_now': 'DoReMi is a technique that uses a small proxy model to '\n",
      "                     'optimize domain weights of a language modeling dataset, '\n",
      "                     'which then improves the training of a larger model.\\n'\n",
      "                     'DoReMi sets domain weights automatically with only 2 '\n",
      "                     'small LM training runs and do not make assumptions about '\n",
      "                     'the type of data to prefer (Wikipedia-like, etc.).',\n",
      " 'title': 'DoReMi: Automatic Domain Weighting for Language Modeling'}\n",
      "['Q1. What is DoReMi?', 'A1. DoReMi is a technique that uses a small proxy model to optimize domain weights of a language modeling dataset, which then improves the training of a larger model.', 'Q2. How does DoReMi work?', 'A2. DoReMi sets domain weights automatically with only 2 small LM training runs and do not make assumptions about the type of data to prefer (Wikipedia-like, etc.).']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|██████████████████████████████████████████████████████████████████████████████████████████████████▍                                                    | 15/23 [03:39<01:12,  9.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'questions_and_answers': '* What is DoReMi?\\n'\n",
      "                          'DoReMi is a technique that uses a small proxy model '\n",
      "                          'to optimize domain weights of a language modeling '\n",
      "                          'dataset, which then improves the training of a '\n",
      "                          'larger model.',\n",
      " 'summary': 'DoReMi consistently improves downstream accuracy across scales, '\n",
      "            'outperforming both baseline (uniform) domain weights and '\n",
      "            'downstream-tuned domain weights.',\n",
      " 'summary_till_now': 'DoReMi is a technique that uses a small proxy model to '\n",
      "                     'optimize domain weights of a language modeling dataset, '\n",
      "                     'which then improves the training of a larger model.',\n",
      " 'title': '\\n'\n",
      "          '\"DoReMi: A New Approach to Improve Language Modeling Data '\n",
      "          'Selection\"'}\n",
      "['* What is DoReMi?', 'DoReMi is a technique that uses a small proxy model to optimize domain weights of a language modeling dataset, which then improves the training of a larger model.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|█████████████████████████████████████████████████████████████████████████████████████████████████████████                                              | 16/23 [03:46<00:59,  8.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'questions_and_answers': 'Q: What are results across different scales on the '\n",
      "                          'GLaM dataset?\\n'\n",
      "                          'A: DoReMi is comparable or better than both the '\n",
      "                          'baseline (uniform) domain weights and '\n",
      "                          'downstream-tuned domain weights.\\n'\n",
      "                          '\\n'\n",
      "                          'Q: What is the size of reference/proxy models in '\n",
      "                          'the Pile?\\n'\n",
      "                          'A: The reference/proxy models are 70M, 150M, 280M, '\n",
      "                          'and 1B.\\n'\n",
      "                          '\\n'\n",
      "                          'Q: What is the size of main model trained on The '\n",
      "                          'Pile?\\n'\n",
      "                          'A: 8B (8 billion)',\n",
      " 'summary': 'DoReMi is a technique that uses a small proxy model to optimize '\n",
      "            'domain weights of a language modeling dataset, which then '\n",
      "            'improves the training of a larger model. The technique has been '\n",
      "            'tested on the GLaM dataset and The Pile, with results showing '\n",
      "            'significant improvement over baseline domain weights.',\n",
      " 'summary_till_now': 'DoReMi is a technique that uses a small proxy model to '\n",
      "                     'optimize domain weights of a language modeling dataset, '\n",
      "                     'which then improves the training of a larger model.',\n",
      " 'title': 'DoReMi Improves Downstream Performance on GLaM and The Pile'}\n",
      "['Q: What are results across different scales on the GLaM dataset?', 'A: DoReMi is comparable or better than both the baseline (uniform) domain weights and downstream-tuned domain weights.', 'Q: What is the size of reference/proxy models in the Pile?', 'A: The reference/proxy models are 70M, 150M, 280M, and 1B.', 'Q: What is the size of main model trained on The Pile?', 'A: 8B (8 billion)']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                       | 17/23 [03:54<00:48,  8.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'questions_and_answers': 'Question 1:\\n'\n",
      "                          'Which DoReMi model improves downstream performance '\n",
      "                          'significantly over the baseline domain weights '\n",
      "                          'across all generative one-shot tasks?\\n'\n",
      "                          'DoReMi (1B->8B)\\n'\n",
      "                          '\\n'\n",
      "                          'Question 2:\\n'\n",
      "                          'Which DoReMi model decreases the worst-case '\n",
      "                          'perplexity and improves the average perplexity '\n",
      "                          'across 22 domains from The Pile?\\n'\n",
      "                          'DoReMi (280M->8B)\\n'\n",
      "                          '\\n'\n",
      "                          'Question 3:\\n'\n",
      "                          'Which DoReMi model improves the worst-case and '\n",
      "                          'average perplexity of the baseline domain weights '\n",
      "                          'in all cases?\\n'\n",
      "                          'DoReMi (150M->8B)',\n",
      " 'summary': 'DoReMi is a technique that uses a small proxy model to optimize '\n",
      "            'domain weights of a language modeling dataset, which then '\n",
      "            'improves the training of a larger model. Table 5 shows the '\n",
      "            'accuracies on one-shot generative tasks for various '\n",
      "            'reference/proxy model sizes from 70M to 1B. All DoReMi models '\n",
      "            'improve downstream performance significantly over the baseline.',\n",
      " 'summary_till_now': 'DoReMi is a technique that uses a small proxy model to '\n",
      "                     'optimize domain weights of a language modeling dataset, '\n",
      "                     'which then improves the training of a larger model.\\n'\n",
      "                     '\\n'\n",
      "                     'current fragment of document:\\n'\n",
      "                     'Per-task exact-match accuracies for generative one-shot '\n",
      "                     'tasks. All DoReMi models improve downstream performance '\n",
      "                     'significantly over the baseline domain weights.',\n",
      " 'title': 'Summary of perplexity results across scales'}\n",
      "['Which DoReMi model improves downstream performance significantly over the baseline domain weights across all generative one-shot tasks?', 'Which DoReMi model decreases the worst-case perplexity and improves the average perplexity across 22 domains from The Pile?', 'Which DoReMi model improves the worst-case and average perplexity of the baseline domain weights in all cases?']\n",
      "['What are the different models of DoReMi?', 'Per-task exact-match accuracies for generative one-shot tasks. All DoReMi models improve downstream performance significantly over the baseline domain weights.', 'How does DoReMi improve downstream performance?', 'Per-task exact-match accuracies for generative one-shot tasks. All DoReMi models improve downstream performance significantly over the baseline domain weights.', 'What are the perplexity results for DoReMi?', 'Table 5 shows a summary of per-domain perplexities for DoReMi across 4 scales (280M, 510M, 760M, 1B). Here, the reference/proxy models are the same size as the main model trained with DoReMi domain weights. On average, DoReMi improves perplexity on 18.25 out of 22 domains from The Pile. The worst-case perplexity is always reduced (or comparable in the 510M case) with respect to the baseline domain weights.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                | 18/23 [04:14<00:58, 11.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'questions_and_answers': 'Question 1:\\n'\n",
      "                          'How was DoReMi trained?\\n'\n",
      "                          'DoReMi was trained using a proxy model to optimize '\n",
      "                          'the domain weights of the language modeling '\n",
      "                          'dataset.\\n'\n",
      "                          '\\n'\n",
      "                          'Question 2:\\n'\n",
      "                          'How did DoReMi improve downstream performance?\\n'\n",
      "                          'DoReMi improved downstream performance by '\n",
      "                          'optimizing the domain weights of the language '\n",
      "                          'modeling dataset.',\n",
      " 'summary': 'DoReMi is a technique that uses a small proxy model to optimize '\n",
      "            'domain weights of a language modeling dataset, which then '\n",
      "            'improves the training of a larger model. Per-task exact-match '\n",
      "            'accuracies for generative one-shot tasks all improve '\n",
      "            'significantly over the baseline domain weights.',\n",
      " 'summary_till_now': 'DoReMi is a technique that uses a small proxy model to '\n",
      "                     'optimize domain weights of a language modeling dataset, '\n",
      "                     'which then improves the training of a larger model.\\n'\n",
      "                     '\\n'\n",
      "                     'current fragment of document:\\n'\n",
      "                     '\\n'\n",
      "                     'Per-task exact-match accuracies for generative one-shot '\n",
      "                     'tasks. All DoReMi models improve downstream performance '\n",
      "                     'significantly over the baseline domain weights.',\n",
      " 'title': 'DoReMi improves downstream performance significantly over baseline '\n",
      "          'domain weights'}\n",
      "['How was DoReMi trained?', 'DoReMi was trained using a proxy model to optimize the domain weights of the language modeling dataset.', 'How did DoReMi improve downstream performance?', 'DoReMi improved downstream performance by optimizing the domain weights of the language modeling dataset.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                          | 19/23 [04:21<00:41, 10.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'questions_and_answers': 'Q: How does DoReMi improve downstream performance?\\n'\n",
      "                          'A: By using a small proxy model to optimize domain '\n",
      "                          'weights of a language modeling dataset, which then '\n",
      "                          'improves the training of a larger model.',\n",
      " 'summary': 'DoReMi is a technique that uses a small proxy model to optimize '\n",
      "            'domain weights of a language modeling dataset, which then '\n",
      "            'improves the training of a larger model. Per-task exact-match '\n",
      "            'accuracies for generative one-shot tasks. All DoReMi models '\n",
      "            'improve downstream performance significantly over the baseline '\n",
      "            'domain weights.',\n",
      " 'summary_till_now': 'DoReMi is a technique that uses a small proxy model to '\n",
      "                     'optimize domain weights of a language modeling dataset, '\n",
      "                     'which then improves the training of a larger model.\\n'\n",
      "                     '\\n'\n",
      "                     'current fragment of document:\\n'\n",
      "                     '\\n'\n",
      "                     'Per-task exact-match accuracies for generative one-shot '\n",
      "                     'tasks. All DoReMi models improve downstream performance '\n",
      "                     'significantly over the baseline domain weights.\\n'\n",
      "                     '\\n'\n",
      "                     '\\n'\n",
      "                     'coherent running summary:\\n'\n",
      "                     '\\n'\n",
      "                     'Per-task exact-match accuracies for generative one-shot '\n",
      "                     'tasks. All DoReMi models improve downstream performance '\n",
      "                     'significantly over the baseline domain weights.',\n",
      " 'title': 'DoReMi Results on One-shot Tasks'}\n",
      "['Q: How does DoReMi improve downstream performance?', 'A: By using a small proxy model to optimize domain weights of a language modeling dataset, which then improves the training of a larger model.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 87%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                   | 20/23 [04:28<00:28,  9.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'questions_and_answers': '\\n'\n",
      "                          'Q1. What is DoReMi?\\n'\n",
      "                          'A1. DoReMi is a technique that uses a small proxy '\n",
      "                          'model to optimize domain weights of a language '\n",
      "                          'modeling dataset, which then improves the training '\n",
      "                          'of a larger model.',\n",
      " 'summary': 'DoReMi is a technique that uses a small proxy model to optimize '\n",
      "            'domain weights of a language modeling dataset, which then '\n",
      "            'improves the training of a larger model. The per-task exact-match '\n",
      "            'accuracies for generative one-shot tasks are presented. The '\n",
      "            'DoReMi models significantly outperform the baseline domain '\n",
      "            'weights.',\n",
      " 'summary_till_now': 'DoReMi is a technique that uses a small proxy model to '\n",
      "                     'optimize domain weights of a language modeling dataset, '\n",
      "                     'which then improves the training of a larger model.',\n",
      " 'title': 'DoReMi Improves Downstream Performance in Generative One-shot Tasks'}\n",
      "['Q1. What is DoReMi?', 'A1. DoReMi is a technique that uses a small proxy model to optimize domain weights of a language modeling dataset, which then improves the training of a larger model.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 91%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊             | 21/23 [04:37<00:18,  9.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'questions_and_answers': '1. Why are domain weights optimized?\\n'\n",
      "                          'To reallocate samples to domains that need them and '\n",
      "                          'reduce error.\\n'\n",
      "                          '2. Is there positive transfer between domains?\\n'\n",
      "                          'Yes, positive transfer between domains can '\n",
      "                          'contribute to scenarios where reweighting results '\n",
      "                          'in no tradeoff across domains.',\n",
      " 'summary': 'DoReMi is a technique that uses a small proxy model to optimize '\n",
      "            'domain weights of a language modeling dataset, which then '\n",
      "            'improves the training of a larger model. The no-tradeoff example '\n",
      "            'uses a simple setting with three domains and 3 vocabulary tokens. '\n",
      "            'It uses a symmetric Dirichlet prior (preferring a uniform token '\n",
      "            'distribution), where λz(x) = 1/3 for all tokens x and domains z. '\n",
      "            'Therefore, the parameter error is decreasing in the number of '\n",
      "            'samples n2 for domain z = 2.',\n",
      " 'summary_till_now': 'DoReMi is a technique that uses a small proxy model to '\n",
      "                     'optimize domain weights of a language modeling dataset, '\n",
      "                     'which then improves the training of a larger model.',\n",
      " 'title': 'DoReMi - Domain Reweighting for Language Modeling'}\n",
      "['1. Why are domain weights optimized?', 'To reallocate samples to domains that need them and reduce error.', '2. Is there positive transfer between domains?', 'Yes, positive transfer between domains can contribute to scenarios where reweighting results in no tradeoff across domains.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍      | 22/23 [04:44<00:08,  8.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'questions_and_answers': 'Question 1:\\n'\n",
      "                          'How does DoReMi achieve domain reweighting?\\n'\n",
      "                          'Answer 1:\\n'\n",
      "                          'It uses a small proxy model to optimize domain '\n",
      "                          'weights of a language modeling dataset.',\n",
      " 'summary': 'DoReMi is a technique that uses a small proxy model to optimize '\n",
      "            'domain weights of a language modeling dataset, which then '\n",
      "            'improves the training of a larger model. DoReMi iteratively '\n",
      "            'updates domain weights based on a closed-form estimator and an '\n",
      "            'online version of the same estimator. The closed-form estimator '\n",
      "            'is computed independently on a validation set, while the online '\n",
      "            'estimator is calculated over a minibatch of examples. DoReMi is '\n",
      "            'compared to a model trained with baseline domain weights, which '\n",
      "            'are uniform over the 3 domains. The domain weights returned by '\n",
      "            'DoReMi are [0.39, 0.61, 0.0], which correspond to our intuitions '\n",
      "            '— the first domain (non-noisy) is increased a small amount, the '\n",
      "            'third domain (noisy) is decreased to 0 weight, and most of the '\n",
      "            'weight is allocated to the second domain.',\n",
      " 'summary_till_now': 'DoReMi is a technique that uses a small proxy model to '\n",
      "                     'optimize domain weights of a language modeling dataset, '\n",
      "                     'which then improves the training of a larger model.',\n",
      " 'title': '\"DoReMi: A Technique for Optimizing Domain Weights in Language '\n",
      "          'Modeling\"'}\n",
      "['How does DoReMi achieve domain reweighting?', 'It uses a small proxy model to optimize domain weights of a language modeling dataset.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 23/23 [04:52<00:00, 12.72s/it]\n"
     ]
    }
   ],
   "source": [
    "# doc_text = PDFReaderTool()(\"https://arxiv.org/pdf/2305.10429.pdf\")\n",
    "full_summary = LongSummarizer()(doc_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "6dd78790",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T09:36:53.154736Z",
     "start_time": "2023-05-24T09:36:53.129013Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1. What is the motivation behind Domain Reweighting with Minimax Optimization (DoReMi)?\n",
      "A1. The motivation behind DoReMi is to find domain weights which lead to models that perform well on all domains by minimizing the worst-case loss over domains. DoReMi uses distributionally robust optimization (DRO) to tune the domain weights without knowledge of downstream tasks.\n",
      "\n",
      "Q2. What is the role of the small reference model in DoReMi?\n",
      "A2. The small reference model in DoReMi serves as a baseline level of difficulty of each example/domain. It is trained on the initial domain weights and captures the baseline performance of the language model.\n",
      "\n",
      "Q3. How does DoReMi train the small proxy model using Group DRO?\n",
      "A3. DoReMi trains the small proxy model using Group DRO by optimizing the worst-case loss over domains. The model is trained to minimize the excess loss with respect to the reference model across all domains. This is done by dynamically updating the domain weights according to the loss on each domain for rescaling the training objective.\n",
      "\n",
      "Q4. What are the advantages of using DoReMi on The Pile dataset?\n",
      "A4. DoReMi improves perplexity across all domains on The Pile dataset, even when it downweights a domain. It improves average few-shot downstream accuracy by 6.5% over a baseline model trained using The Pile’s default domain weights and reaches the baseline accuracy with 2.6x fewer training steps. The tuned domain weights for The Pile are also published to improve future LMs trained with The Pile.\n",
      "\n",
      "Q5. How does DoReMi perform when varying the sizes of the proxy model and the main model trained with optimized domain weights?\n",
      "A5. DoReMi consistently improves LM training when varying the sizes of the proxy model and the main model trained with optimized domain weights. This indicates that DoReMi is robust to changes in the size of the language model.\n",
      "\n",
      "Q6. What is the downside of existing works that determine domain weights based on a set of downstream tasks?\n",
      "A6. Existing works that determine domain weights based on a set of downstream tasks require training potentially thousands of LMs on different domain weights and risks overfitting to the particular set of downstream tasks. This is because the optimal domain weights for a given set of downstream tasks may not generalize well to new tasks or datasets.\n",
      "\n",
      "Q7. How does DoReMi optimize domain weights compared to a naive worst-case approach?\n",
      "A7. A naive worst-case approach would upweight the domains with the most noisy data, as every domain has a different optimal loss. DoReMi optimizes the worst-case excess loss, which is the loss gap between the model being evaluated and a pretrained reference model. This makes the domain perplexities comparable and leads to domain weights that perform well on all domains.\n"
     ]
    }
   ],
   "source": [
    "# prev sec, # next sec, # this sec. Full summary, chunked summary (\" \".join(full_summary[\"chunked_summary\"]))\n",
    "# Questions which will help a scientist understand this work\n",
    "# Questions which will help a scientist to use this research for their own stuff.\n",
    "# Thought provoking questions\n",
    "# Criticisms and short comings and debatable decisions made in this work\n",
    "# Cross section questions\n",
    "\n",
    "class QuestionAnswerGenerator:\n",
    "    def __init__(self, chunk_size=1000):\n",
    "        self.chunk_size=1000\n",
    "        self.name = \"QuestionAnswerGenerator\"\n",
    "        self.description = \"\"\"\n",
    "QuestionAnswerGenerator:\n",
    "    This tool generates questions for chunks of the text document.\n",
    "\n",
    "    Input params/args: \n",
    "        long_document (str): document to generate questions.\n",
    "        full_summary_doc (dict): summary dict from LongSummarizer\n",
    "\n",
    "    Returns: \n",
    "        List[Tuple[str]]: questions and answers for each chunk.\n",
    "\n",
    "    Usage:\n",
    "        `questions_and_answers = QuestionAnswerGenerator()(long_document=\"long text document\", full_summary_doc=<full_summary from LongSummarizer>) # Note: this tool needs to be initialized first.`\n",
    "    \"\"\"\n",
    "        self.prompt = PromptTemplate(\n",
    "            input_variables=[\"full_summary\", \"current_section\", \"next_section\", \"previous_section\",  \n",
    "                             \"previous_questions_and_answers\"],\n",
    "            template=\"\"\" \n",
    "We want to generate questions and answers from current given section/fragment/chunk of a larger work document. \n",
    "The summary of the larger document is given below:\n",
    "{full_summary}\n",
    "\n",
    "The previous section is given below:\n",
    "\"{previous_section}\"\n",
    "\n",
    "The current section/fragment is given below:\n",
    "\"{current_section}\"\n",
    "\n",
    "The next section is given below:\n",
    "\"{next_section}\"\n",
    "\n",
    "\n",
    "We also provide a set of questions and answers we had earlier created from the current section below:\n",
    "{previous_questions_and_answers}\n",
    "\n",
    "Based on these information you are requested to generate further questions and answers to help understand this work deeper.\n",
    "Generate the following type of questions and their answers.\n",
    "- Complex Questions and answers which help in deeply understanding this work.\n",
    "- Questions which will help a scientist/researcher understand this work.\n",
    "- Questions which will help a scientist to use this work document for their own stuff.\n",
    "- Thought provoking questions which can be opinionated\n",
    "- Questions which require knowledge of multiple sections and reading of whole document.\n",
    "- Questions regarding criticisms and short comings and debatable decisions made in this work.\n",
    "\n",
    "Guidelines to follow:\n",
    "- Ensure that your answers are long, elaborate, detailed and provide deeper insights. \n",
    "- Ensure that your questions are complex and difficult to answer.\n",
    "- No short answers.\n",
    "- Generate each question and answer in a separate line.\n",
    "\n",
    "Questions and Answers:\n",
    "\n",
    "\"\"\",\n",
    "        )\n",
    "    def __call__(self, long_document, full_summary):\n",
    "        \n",
    "        chunks = full_summary[\"chunks\"]\n",
    "        fsum = full_summary[\"running_summary\"]\n",
    "        long_sum = full_summary[\"full_length_summary\"] # \"expanded_summary\"\n",
    "        previous_questions_and_answers = full_summary[\"chunk_questions\"]\n",
    "        \n",
    "        prompts = []\n",
    "        for ix, (pqa, chunk) in enumerate(zip(previous_questions_and_answers, chunks)):\n",
    "            previous_section = \"No previous section. We are on first section.\" if ix == 0 else chunks[ix - 1]\n",
    "            next_section = \"No next section. We are on last section.\" if ix == len(chunks)-1 else chunks[ix + 1]\n",
    "            prompt = self.prompt.format(full_summary=fsum, current_section=chunk, \n",
    "                                        previous_section=previous_section, next_section=next_section, \n",
    "                                        previous_questions_and_answers=pqa)\n",
    "            prompts.append(prompt)\n",
    "        def printed_gpt_call(**kwargs):\n",
    "            rsp = callGpt.get_turbo_call()(**kwargs)\n",
    "#             print(rsp)\n",
    "            return rsp\n",
    "        calls = [{\"text\": p, \"temperature\": 0.7, \"num_tokens\": 4000} for p in prompts]\n",
    "        responses = call_api_parallel(calls, printed_gpt_call, max_workers=2)\n",
    "        chunk_questions = []\n",
    "        for ix, (qna_resp, call) in enumerate(zip(responses, calls)):\n",
    "            qna_success = False\n",
    "            while not qna_success:\n",
    "                qna = [qa.strip() for qa in qna_resp.split(\"\\n\") if len(qa.strip()) > 0]\n",
    "                # any line with less than 3 words.\n",
    "                qna = [qa for qa in qna if len(qa.split())>2]\n",
    "                print(qna)\n",
    "                qna_success = len(qna) % 2 == 0\n",
    "                if not qna_success:\n",
    "                    qna_resp = callGpt.get_turbo_call()(**call)\n",
    "            qna = concat_array_two_at_a_time(qna)\n",
    "            chunk_questions.append(qna)\n",
    "        return chunk_questions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd4b26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criticisms and short comings and debatable decisions made in this work # Limitations, criticisms, future work\n",
    "# Read section by section\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36bba82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T09:56:10.872114Z",
     "start_time": "2023-05-24T09:36:56.143130Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "qa_generations = QuestionAnswerGenerator()(doc_text, full_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ff6da80f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T17:49:18.402285Z",
     "start_time": "2023-05-22T17:49:18.382942Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"answer\": \"In this work, reweighting refers to the process of optimizing domain weights in a dataset to improve language models trained on that dataset. The approach used for reweighting is called Domain Reweighting with Minimax Optimization (DoReMi). It involves obtaining a small reference model, training a proxy model with Group DRO to obtain domain weights, and ultimately, training a large model on the optimized domain weights. In a simple language modeling example, reweighting the training data from different domains resulted in improved perplexity across all domains.\",\n",
      "    \"further_questions\": [\n",
      "        \"How does the DoReMi algorithm work in detail?\",\n",
      "        \"What are the key steps involved in DoReMi?\",\n",
      "        \"How is the performance of the model measured after reweighting?\"\n",
      "    ],\n",
      "    \"short_answer_summary\": \"Reweighting in this work means optimizing domain weights using the DoReMi approach to improve language models trained on a dataset, resulting in improved perplexity across all domains.\",\n",
      "    \"read_next_fragment\": False,\n",
      "    \"read_previous_fragment\": False,\n",
      "    \"related_questions\": [\n",
      "        \"What are the main benefits of using the DoReMi approach?\",\n",
      "        \"How does reweighting with DoReMi compare to other reweighting techniques?\",\n",
      "        \"What are the potential applications of the DoReMi approach in language modeling?\"\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print('{\\n    \"answer\": \"In this work, reweighting refers to the process of optimizing domain weights in a dataset to improve language models trained on that dataset. The approach used for reweighting is called Domain Reweighting with Minimax Optimization (DoReMi). It involves obtaining a small reference model, training a proxy model with Group DRO to obtain domain weights, and ultimately, training a large model on the optimized domain weights. In a simple language modeling example, reweighting the training data from different domains resulted in improved perplexity across all domains.\",\\n    \"further_questions\": [\\n        \"How does the DoReMi algorithm work in detail?\",\\n        \"What are the key steps involved in DoReMi?\",\\n        \"How is the performance of the model measured after reweighting?\"\\n    ],\\n    \"short_answer_summary\": \"Reweighting in this work means optimizing domain weights using the DoReMi approach to improve language models trained on a dataset, resulting in improved perplexity across all domains.\",\\n    \"read_next_fragment\": False,\\n    \"read_previous_fragment\": False,\\n    \"related_questions\": [\\n        \"What are the main benefits of using the DoReMi approach?\",\\n        \"How does reweighting with DoReMi compare to other reweighting techniques?\",\\n        \"What are the potential applications of the DoReMi approach in language modeling?\"\\n    ]\\n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "471d799b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T08:55:19.299434Z",
     "start_time": "2023-05-24T08:55:19.237285Z"
    }
   },
   "outputs": [],
   "source": [
    "# One or more doc/pdf reader\n",
    "\n",
    "# Literature survey tool\n",
    "\n",
    "# Generate longer text by doing lookups\n",
    "\n",
    "# Running Summarize a Long Document\n",
    "\n",
    "# Generate long answers for a query on a document by multiple look-ups and long generation\n",
    "\n",
    "# Generate long answers for a query/request from multiple supported documents (multi-pdf) by multiple look-ups and long generation\n",
    "\n",
    "# Analyse multiple documents together, with ability to generate cross document questions and answer by seeing across documents.\n",
    "\n",
    "# To give better answers -> Refine the answer by adding more details as well as explaining jargons and side concerns\n",
    "\n",
    "# To give better answers -> Should we read the next document fragment and previous fragment.\n",
    "\n",
    "# To give better answers -> Should we reformulate the question or answer few other related questions\n",
    "\n",
    "# documents = [Document(t) for t in full_summary[\"chunks\"]]\n",
    "\n",
    "class LongContextualQuestionAnswering:\n",
    "    def __init__(self, ):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self):\n",
    "        # there can be many ways of long form generation -> top down from a TOC\n",
    "        # Running Summary (or historian) + Next step generation.\n",
    "        # Cognizance of how long do we need to generate.\n",
    "        pass\n",
    "    \n",
    "class LongContextualTextGeneration:\n",
    "    def __init__(self, ):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self):\n",
    "        # there can be many ways of long form generation -> top down from a TOC\n",
    "        # Running Summary (or historian) + Next step generation.\n",
    "        # Cognizance of how long do we need to generate.\n",
    "        pass\n",
    "\n",
    "\n",
    "class MultiDocIndex:\n",
    "    pass\n",
    "\n",
    "# criticisms and enhanced sectional full summary\n",
    "    \n",
    "class DocIndex:\n",
    "    def __init__(self, doc_id, doc_source, doc_filetype, doc_type, doc_text=None, full_summary=None):\n",
    "        self.long_summarizer = LongSummarizer()\n",
    "        self.doc_id = doc_id\n",
    "        self.doc_source = doc_source\n",
    "        self.doc_filetype = doc_filetype\n",
    "        self.doc_type = doc_type\n",
    "        \n",
    "        if full_summary is None:\n",
    "            if doc_filetype == \"pdf\" and \"http\" in doc_source and doc_text is None:\n",
    "                doc_text = PDFReaderTool()(doc_source)\n",
    "            assert doc_text\n",
    "            lsum = self.long_summarizer(doc_text)\n",
    "        else:\n",
    "            lsum = full_summary\n",
    "        self.raw_texts = lsum['chunks']\n",
    "        self.running_summary = lsum[\"running_summary\"]\n",
    "        self.summaries_with_previous_context = lsum['chunked_summary']\n",
    "        self.chunk_titles = lsum[\"title\"]\n",
    "        self.chunk_questions_and_answers = lsum[\"chunk_questions\"]\n",
    "        \n",
    "        self.answered_questions = []\n",
    "        \n",
    "        sec_ids, qna = zip(*[[ix, str(ix)+\"\\n\"+qa[0]+\"\\n\"+ qa[1]] for ix, qsec in enumerate(lsum[\"chunk_questions\"]) for qa in qsec])\n",
    "        print(len(sec_ids), len(qna), type(sec_ids))\n",
    "        assert len(sec_ids) == len(qna)\n",
    "        qna_index = create_index_with_nodes(qna, openai_embed, doc_id=sec_ids, )\n",
    "        \n",
    "        # Use a tree index on the full document.\n",
    "        \n",
    "        raw_index = create_index_with_nodes(self.raw_texts, openai_embed,  )\n",
    "        summary_index = create_index_with_nodes(self.summaries_with_previous_context, openai_embed, )\n",
    "        \n",
    "        \n",
    "        self.raw_index = raw_index.as_retriever(\n",
    "            verbose=True, \n",
    "            retriever_mode=\"embedding\"\n",
    "        )\n",
    "        self.summary_index = summary_index.as_retriever(\n",
    "            verbose=True, \n",
    "            retriever_mode=\"embedding\"\n",
    "        )\n",
    "        self.qna_index = qna_index.as_retriever(\n",
    "            verbose=True, \n",
    "            retriever_mode=\"embedding\"\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.previous_qna_history = []\n",
    "        self.short_answer_prompt = PromptTemplate(\n",
    "            input_variables=[\"query\", \"fragment\", \"summary\", \"questions_answers\", \"full_summary\", \"answer_format\"],\n",
    "            template=\"\"\"\n",
    "You will help answer a question or information request from context (text chunks of larger document) you are provided. \n",
    "The provided context is part/fragment of a larger main document.\n",
    "\n",
    "Answer the question or information request below:\n",
    "\n",
    "{query}\n",
    "\n",
    "You are given the summary of the larger main document below:\n",
    "\n",
    "{full_summary}\n",
    "\n",
    "You are given few text chunks from within the document to answer this question as below:\n",
    "\n",
    "{fragment}\n",
    "\n",
    "Next, You are given few question and answer pairs from the document below:\n",
    "\n",
    "{questions_answers}\n",
    "\n",
    "You are also given summarized text chunks of certain parts of document below:\n",
    "\n",
    "{summary}\n",
    "\n",
    "Answer elaborately providing as much detail as possible.\n",
    "\n",
    "Provide the below details in your response:\n",
    "- elaborate answer\n",
    "- further questions: what other questions we can ask to answer better (questions which elaborate the asked question/provided answer)\n",
    "- short answer summary: short summary of answer\n",
    "- whether to read next and previous fragment\n",
    "- related questions: related questions which may be similar to this question\n",
    "\n",
    "Your answer must be a python dictionary only of below format:\n",
    "\n",
    "{answer_format}\n",
    "\n",
    "\n",
    "\"\"\",\n",
    "        )\n",
    "        self.short_query_response_format = \"\"\"\n",
    "{\n",
    "    \"answer\": <elaborate answer to the question>, \n",
    "    \"further_questions\": <List[str], what other questions we can ask to answer better (questions which elaborate the asked question/provided answer)>,\n",
    "    \"short_answer_summary\": <short summary of answer>,\n",
    "    \"read_next_fragment\": <bool, should we read the next fragment from document to improve the answer>,\n",
    "    \"read_previous_fragment\": <bool, should we read the next previous from document to improve the answer>,\n",
    "    \"related_questions\": <List[str], related questions which may be similar to this question (not questions which elaborate the asked question/provided answer)>\n",
    "}\n",
    "        \"\"\"\n",
    "        \n",
    "        self.long_query_response_format = \"\"\"\n",
    "    {\n",
    "        \"answer\": <further elaborated and extended answer to the question based on previous answer with much more details and wide coverage>, \n",
    "        \"further_questions\": <List[str], what other questions we can ask to answer better (questions which elaborate the asked question/provided answer)>,\n",
    "        \"short_answer_summary\": <short summary of answer>,\n",
    "        \"related_questions\": <List[str], related questions which may be similar to this question (not questions which elaborate the asked question/provided answer)>\n",
    "    }\n",
    "            \"\"\"\n",
    "\n",
    "        self.long_answer_query = PromptTemplate(\n",
    "                input_variables=[\"query\", \"previous_answer\", \"more_questions_and_answers\", \"full_summary\", \"answer_format\"],\n",
    "                template=\"\"\"\n",
    "You will help write a long and extended answer (building and expanding on previous answer with much more details and wide coverage) for a question or information request below:\n",
    "\n",
    "The question or information request is given below:\n",
    "\n",
    "{query}\n",
    "\n",
    "Previous answer for this question is given below:\n",
    "{previous_answer}\n",
    "\n",
    "\n",
    "You are given the summary of the larger main document below:\n",
    "\n",
    "{full_summary}\n",
    "\n",
    "\n",
    "\n",
    "You are also given few related questions and answers which you can use as additional information as below:\n",
    "\n",
    "{more_questions_and_answers}\n",
    "\n",
    "\n",
    "\n",
    "Answer elaborately providing as much detail as possible.\n",
    "\n",
    "Provide the below details in your response:\n",
    "- elaborate and expanded answer which provides more details than previous answer while keeping all details from previous answer. \n",
    "- To write an extended and further elaborated answer think of multiple different angles that this question can be thought from. \n",
    "- Add clarifications and answers to other questions from \"related questions and answers\" into this as well. Provide details on jargons and difficult to understand terms in your \"answer\" as well.\n",
    "- use details from \"few related questions and answers\" and add them in your \"answer\", you can cover side details in your answer which do not directly relate to the main answer as well. \n",
    "- when looking at \"few related questions and answers\" add more details and elaborations to this \"answer\" from \"few related questions and answers\".\n",
    "- Try to answer any question that may arise from the answer you write in the answer itself.\n",
    "- further questions: what other questions we can ask to answer better (questions which elaborate the asked question/provided answer)\n",
    "- short answer summary: short summary of answer\n",
    "- related questions: related questions which may be similar to this question\n",
    "\n",
    "Your answer must be a python dictionary only of below format:\n",
    "\n",
    "{answer_format}\n",
    "\n",
    "    \"\"\",\n",
    "            )\n",
    "        \n",
    "    \n",
    "    def get_short_answer(self, query):\n",
    "        # Give out similar questions and answers\n",
    "        qna_nodes = [n.node for n in self.qna_index.retrieve(query)]\n",
    "        raw_nodes = [n.node for n in self.raw_index.retrieve(query)]\n",
    "        raw_node_ids = [n.relationships[DocumentRelationship.SOURCE].strip() for n in raw_nodes]\n",
    "        qna_doc_ids = [n.relationships[DocumentRelationship.SOURCE].split(\"_\")[0] for n in qna_nodes]\n",
    "        additional_qna_doc_ids = [n for n in qna_doc_ids if n not in raw_node_ids]\n",
    "        \n",
    "        additional_text_qna = \"\\n\".join([self.summaries_with_previous_context[int(i)] for i in additional_qna_doc_ids])\n",
    "        summary_nodes = [n.node for n in self.summary_index.retrieve(query)]\n",
    "        summary_text = \"\\n\".join([n.text for n in summary_nodes]) + \"\\n\" + additional_text_qna\n",
    "        qna_text = \"\\n\".join([n.text for n in qna_nodes])\n",
    "        raw_text = \"\\n\".join([n.text for n in raw_nodes])\n",
    "        \n",
    "        prompt = self.short_answer_prompt.format(query=query, fragment=raw_text, summary=summary_text, \n",
    "                                        questions_answers=qna_text, full_summary=self.running_summary,\n",
    "                                        answer_format=self.short_query_response_format)\n",
    "        answers = eval(callGpt.get_hard_call()(prompt, temperature=0.7))\n",
    "        answers[\"raw_node_ids\"] = raw_node_ids\n",
    "        answers[\"query\"] = query\n",
    "        return answers\n",
    "    # start with the short answer\n",
    "    # Expand using new queries\n",
    "    # Expand same query.\n",
    "    # search the document for other search terms\n",
    "\n",
    "    # suggest requery for better results.\n",
    "    # what other questions can we ask of this work to answer this user query in more details. \n",
    "    # We want to continue the answer, A quick summary of answer till now is given.   \n",
    "    \n",
    "    def get_long_answer(self, query, previous_answer=None, \n",
    "                        further_questions_answers=None, \n",
    "                        related_questions_answers=None, more_answers=None):\n",
    "        # Give out similar questions and answers\n",
    "        if previous_answer is None:\n",
    "            previous_answer = self.get_short_answer(query)\n",
    "            pprint(previous_answer)\n",
    "        \n",
    "        if further_questions_answers is None:\n",
    "            further_questions = previous_answer[\"further_questions\"]\n",
    "            further_questions_answers = []\n",
    "            for qs in further_questions:\n",
    "                qs = self.get_short_answer(qs)\n",
    "                further_questions_answers.append(qs)\n",
    "            pprint(further_questions_answers)\n",
    "        \n",
    "        if related_questions_answers is None:\n",
    "            related_questions = previous_answer[\"related_questions\"]\n",
    "            related_questions_answers = []\n",
    "            for qs in related_questions:\n",
    "                qs = self.get_short_answer(qs)\n",
    "                related_questions_answers.append(qs)\n",
    "            pprint(related_questions_answers) \n",
    "        \n",
    "        raw_nodes = list(dind.raw_index._docstore.docs.values())\n",
    "        if more_answers is None:\n",
    "            more_answers = []\n",
    "            for rnid in previous_answer[\"raw_node_ids\"]:\n",
    "                rnid = int(rnid)\n",
    "                if rnid - 1 >= 0:\n",
    "                    more_answers.append(ContextualAnswer()(query, raw_nodes[rnid - 1].text))\n",
    "                if (rnid + 1) < len(raw_nodes):\n",
    "                    more_answers.append(ContextualAnswer()(query, raw_nodes[rnid + 1].text))\n",
    "            print(len(more_answers))\n",
    "            pprint(more_answers)\n",
    "        \n",
    "        more_questions_and_answers = [eqs[\"query\"] + \"\\n\" + eqs[\"answer\"] for eqs in (further_questions_answers + related_questions_answers)]\n",
    "        more_questions_and_answers = \"\\n\\n\".join(more_questions_and_answers)\n",
    "#         pprint(more_questions_and_answers)    \n",
    "        more_answers = \"\\n\\n\".join(more_answers)\n",
    "                \n",
    "        prompt = self.long_answer_query.format(query=query, previous_answer=previous_answer[\"answer\"] + \"\\n\" + more_answers, \n",
    "                                              more_questions_and_answers=more_questions_and_answers, full_summary=self.running_summary, \n",
    "                                              answer_format=self.long_query_response_format)\n",
    "        answers = eval(callGpt.get_hard_call()(prompt, temperature=0.9))\n",
    "        answers[\"query\"] = query\n",
    "        answers[\"more_answers\"] = more_answers\n",
    "        answers[\"further_questions_answers\"] = further_questions_answers\n",
    "        answers[\"related_questions_answers\"] = related_questions_answers\n",
    "        answers[\"previous_answer\"] = previous_answer\n",
    "        return answers\n",
    "        \n",
    "    def save_answer(self, query, answer):\n",
    "        pass\n",
    "    \n",
    "    def ask_follow_up(self, query, previous_query, previous_answer, previous_long_answer=None):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "# Search web (use the web preview details as well)\n",
    "# Add asked questions and answers to existing index\n",
    "# L2 index -> Create Cross elaborate and long answer needing section questions which take full summary + current section + prev section + next section -> Create hard and thought provoking questions -> Create Answers\n",
    "\n",
    "# L3 Index -> index references and citations of this work\n",
    "# L3 index -> Cross document questions and answers\n",
    "\n",
    "# L4 index -> other research thoughts like criticisms etc.\n",
    "\n",
    "# L5 index -> our chatgpt format of reading paper based summary.\n",
    "\n",
    "\n",
    "# Semantic reader -> attaches ability to read in pdf view while using our tool\n",
    "# Use answered questions as indexing as well.\n",
    "\n",
    "# User interface, like chatgpt\n",
    "# threads for a user , each thread is a paper or group of papers are connected by some logic\n",
    "# individual papers are indexed once, mostly by arxiv pdf links (L1 and L2 index)\n",
    "# L3 index is created later\n",
    "# every user can see what other arxiv pdf exist in the database\n",
    "# Every user can look at what thread collections exist. A thread collection of papers can be forked. The conversation in the thread can't be seen but pre-generated questions can be seen.\n",
    "# Index most popular tweeted papers and when someone asks a question on their own paper, you can also suggest them other similar recent papers.\n",
    "# how to give a read-only demo to a user without the user giving openai api key.\n",
    "\n",
    "class ConversationHistory:\n",
    "    \n",
    "    # Instructions to remember\n",
    "    # Context / Conversation summary\n",
    "    # Summary of last single to-fro conversation\n",
    "    # bullet point salient notes\n",
    "    def add_to_index(self, text):\n",
    "        # What if add to history itself recieves too long prompt\n",
    "        pass\n",
    "    def get_from_index(self, query):\n",
    "        pass\n",
    "    def get_all(self):\n",
    "        pass\n",
    "    def _get_entity_linked(self, query):\n",
    "        pass\n",
    "    def _get_kg_based(self, query):\n",
    "        pass\n",
    "    def _get_reverse_query_based_history(self, query):\n",
    "        pass\n",
    "    def _get_vector_search_over_history(self, query):\n",
    "        pass\n",
    "    def _get_toc_search_over_history(self, query):\n",
    "        pass\n",
    "    def _get_text_index_search_over_history(self, query):\n",
    "        pass\n",
    "    def _get_toc_of_history(self,):\n",
    "        pass\n",
    "    def _get_running_summary_of_history(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "76209f95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T08:58:19.847662Z",
     "start_time": "2023-05-24T08:56:18.814555Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 48 <class 'tuple'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.DocIndex at 0x7fe0e186e430>"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this work, reweighting refers to the process of adjusting the domain weights in a language modeling dataset to improve the training of a larger model. The technique employed in this work is called Domain Reweighting with Minimax Optimization (DoReMi). This method leverages distributionally robust optimization (DRO) to optimize the domain weights without explicit knowledge of downstream tasks. The aim is to improve language models trained on a dataset by allocating samples from one domain to another in order to reduce the parameter error for all domains. This involves downweighting the very noisy (high entropy/difficulty) domain 3 to reallocate samples to the other domains 1 and 2.\n",
      "\n",
      "DoReMi uses a small proxy model in the Group DRO optimizer to upweight domains with high excess loss, thus scaling up the proxy model’s gradient update on examples from these domains. The iterative process in updating domain weights, using a closed-form and an online version of the same estimator, allows for rapid convergence. This is achieved by running a loop over T iterative model updates and computing the per-domain excess log-perplexities using a fixed, independent evaluation set of 30 examples.\n",
      "\n",
      "This technique results in several specific performance improvements, such as a 6.5% improvement in downstream accuracy and a 2.6x faster convergence compared to the baseline. Additionally, the proxy model helps optimize domain weights, leading to better performance in terms of perplexity and downstream accuracy on datasets like The Pile and GLaM. The main purpose of using a proxy model in DoReMi is to optimize domain weights, allowing the larger model to train more efficiently and achieve better performance across various practical applications, including more accurate language models and faster training times.\n",
      "================================================================================\n",
      "In the given work, reweighting refers to the process of optimizing domain weights in a language modeling dataset to enhance the training of a larger model. The technique employed is called Domain Reweighting with Minimax Optimization (DoReMi), which operates in conjunction with distributionally robust optimization (DRO) to fine-tune domain weights in the absence of downstream task knowledge. DoReMi uses a small, computationally efficient proxy model to determine the optimal domain weights, subsequently improving the training and performance of the larger model compared to utilizing baseline (uniform) domain weights. The purpose of using a proxy model within DoReMi is to enable better performance in terms of downstream accuracy and perplexity on datasets such as The Pile and GLaM.\n",
      "\n",
      "The reweighting process sees the domain weights being updated as part of the Group DRO optimizer, which upweights domains with higher excess loss. To carry out the domain reweighting, a closed-form estimator and its online version are utilized to iteratively update the domain weights. This is done at the second step of the DoReMi domain reweighting algorithm. The closed-form estimator, which computes the mean of the posterior distribution conditioned on the data for each domain z, is given by the formula ˆθz(x) = (1 / (nz + sz)) * (λz(x) + sum(1[Xz[i] = x] for i in range(1, nz+1))). The online estimator is subsequently calculated as an iterative version of the closed-form estimator, which progresses in real-time throughout a minibatch of examples during the domain reweighting step.\n",
      "\n",
      "DoReMi's performance improvements are prominent, resulting in a 6.5% increase in downstream accuracy, achieving the baseline accuracy 2.6 times faster (within 75k steps as opposed to 200k steps). This leads to a marked acceleration in training and downstream performance optimization. Moreover, DoReMi reduces perplexity across all domains without tradeoffs, with models of various sizes exhibiting significant improvements over the baseline.\n",
      "\n",
      "The iterative process of updating domain weights in DoReMi functions by looping through T iterative model updates, running for T = 500 steps with a minibatch size of 1 over n = 500 training examples and a domain weight update rate η = 0.5. This streamlined method allows DoReMi to be highly beneficial in training larger language models more efficiently, improving downstream performance and reducing perplexity across all domains.\n",
      "================================================================================\n",
      "In this work, reweighting refers to the process of adjusting the domain weights in a language modeling dataset to improve the training of a larger model. Domain Reweighting with Minimax Optimization (DoReMi) is the technique employed to optimize these domain weights, using a small proxy model and leveraging distributionally robust optimization (DRO). The optimization is based on the closed-form estimator and its online version, which are iteratively updated to allocate more appropriate weights to each domain within the dataset. This results in better performance and improved training of the larger model when compared to using baseline (uniform) domain weights. Specifically, DoReMi improves downstream accuracy by 6.5% and achieves the baseline accuracy within 75k steps, which is 2.6x faster than the baseline (200k steps). The closed-form estimator is calculated for each domain by computing the mean of the posterior distribution conditioned on the data, while the online estimator is calculated as an iterative version of the closed-form estimator, updated in an online fashion over a minibatch of examples during the domain reweighting step. The proxy model is used for optimizing the domain weights, allowing the larger model to have better performance in terms of perplexity and downstream accuracy on datasets like The Pile and GLaM. The iterative process in updating domain weights involves running a loop over T iterative model updates for T = 500 steps using a minibatch size of 1 over the n = 500 training examples with a domain weight update rate η = 0.5. DoReMi can be beneficial in several practical applications, such as improving the training of larger language models, reducing perplexity across all domains without a tradeoff, and speeding up training by achieving baseline accuracy within fewer steps.\n",
      "================================================================================\n",
      "In this work, reweighting refers to the process of adjusting domain weights in a language modeling dataset to enhance the training of a larger model. The technique employed, called Domain Reweighting with Minimax Optimization (DoReMi), leverages distributionally robust optimization (DRO) to adjust domain weights without prior knowledge of downstream tasks. DoReMi uses a small proxy model within the distributionally robust language modeling (DRO-LM) framework, which is trained using a Group DRO optimizer to minimize the worst-case loss across domains. This proxy model helps optimize domain weights, resulting in improved downstream performance and reduced perplexity in datasets such as The Pile and GLaM. The iterative process of updating domain weights in DoReMi involves running a loop with T iterative model updates, where the average is computed in an online fashion. The process runs for T = 500 steps using a minibatch size of 1 over n = 500 training examples with a domain weight update rate η = 0.5. The per-domain excess log-perplexities used in Algorithm 1 are computed using a fixed, independent evaluation set of 30 examples. When compared to the baseline, DoReMi demonstrates several specific performance improvements, including a 6.5% increase in downstream accuracy, achieving baseline accuracy within 75k steps, which is 2.6x faster than the baseline, and reduced perplexity across all domains without a trade-off. Practical applications of DoReMi include more efficient training of larger language models, leading to better downstream performance and reduced perplexity across all domains.\n",
      "================================================================================\n",
      "In this work, reweighting refers to the process of optimizing domain weights in a language modeling dataset to improve the training of a larger model. The technique used is called Domain Reweighting with Minimax Optimization (DoReMi), which employs a small proxy model to optimize these domain weights in a distributionally robust optimization (DRO) framework. By iteratively updating the weights based on a closed-form estimator and an online version of the same estimator, DoReMi manages to allocate more appropriate weights to each domain within the dataset. This results in better performance and improved training of the larger model when compared to using baseline (uniform) domain weights.\n",
      "\n",
      "The closed-form estimator is computed for each domain by calculating the mean of the posterior distribution conditioned on the data. The online estimator, on the other hand, is an iterative version of the closed-form estimator, computed in an online fashion during the domain reweighting step of the algorithm. This iterative process runs for T = 500 steps using a minibatch size of 1 over the n = 500 training examples with a domain weight update rate η = 0.5.\n",
      "\n",
      "DoReMi shows significant performance improvements when compared to the baseline. It improves downstream accuracy by 6.5% and achieves the baseline accuracy within 75k steps, which is 2.6x faster than the baseline (200k steps). This results in dramatically sped-up training and improved downstream performance across all domains without a tradeoff. DoReMi can be beneficial in several practical applications, such as improving the training of larger language models by setting domain weights automatically with only two small language model training runs, resulting in a more efficient training process and better downstream performance on datasets like The Pile and GLaM.\n"
     ]
    }
   ],
   "source": [
    "dind = DocIndex(\"1\", \"https://arxiv.org/pdf/1912.08881.pdf\", \n",
    "                \"pdf\", \n",
    "                \"scientific_article\", doc_text, full_summary)\n",
    "dind\n",
    "\n",
    "previous_answer = {'answer': 'In this work, reweighting refers to the process of adjusting the '\n",
    "           'domain weights in a language modeling dataset to improve the '\n",
    "           'training of a larger model. DoReMi, the technique used in the '\n",
    "           'work, employs a small proxy model to optimize these domain '\n",
    "           'weights. By iteratively updating the weights based on a '\n",
    "           'closed-form estimator and an online version of the same estimator, '\n",
    "           'DoReMi manages to allocate more appropriate weights to each domain '\n",
    "           'within the dataset. This results in better performance and '\n",
    "           'improved training of the larger model when compared to using '\n",
    "           'baseline (uniform) domain weights.',\n",
    " 'further_questions': ['How is the closed-form estimator calculated?',\n",
    "                       'How is the online estimator calculated?',\n",
    "                       'What are the specific performance improvements '\n",
    "                       'realized when using DoReMi?'],\n",
    " 'query': 'What does reweighting mean in this work?',\n",
    " 'raw_node_ids': ['22', '2'],\n",
    " 'read_next_fragment': False,\n",
    " 'read_previous_fragment': False,\n",
    " 'related_questions': ['What is the purpose of using a proxy model in DoReMi?',\n",
    "                       'How does the iterative process work in updating domain '\n",
    "                       'weights?',\n",
    "                       'What are some practical applications where DoReMi can '\n",
    "                       'be beneficial?'],\n",
    " 'short_answer_summary': 'Reweighting in this work means adjusting domain '\n",
    "                         'weights in a language modeling dataset using DoReMi, '\n",
    "                         'a technique that employs a small proxy model to '\n",
    "                         'optimize these weights and improve the training of a '\n",
    "                         'larger model.'}\n",
    "\n",
    "further_questions_answers=[{'answer': 'The closed-form estimator is calculated for each domain z by '\n",
    "            'computing the mean of the posterior distribution conditioned on '\n",
    "            'the data. For all x in {1, ..., m}, the estimator is given by the '\n",
    "            'formula ˆθz(x) = (1 / (nz + sz)) * (λz(x) + sum(1[Xz[i] = x] for '\n",
    "            'i in range(1, nz+1))). Here, nz is the number of samples from '\n",
    "            'domain z, sz is the sum of pseudocounts, and λz(x) is the '\n",
    "            'pseudo-count for each token.',\n",
    "  'further_questions': ['What is the role of the Dirichlet prior in the '\n",
    "                        'closed-form estimator?',\n",
    "                        'How does the closed-form estimator relate to the '\n",
    "                        'parameter error?',\n",
    "                        'What is the difference between the closed-form '\n",
    "                        'estimator and the online estimator?'],\n",
    "  'query': 'How is the closed-form estimator calculated?',\n",
    "  'raw_node_ids': ['20', '21'],\n",
    "  'read_next_fragment': False,\n",
    "  'read_previous_fragment': False,\n",
    "  'related_questions': ['What is the purpose of using a proxy model in DoReMi?',\n",
    "                        'How does DoReMi optimize domain weights in language '\n",
    "                        'modeling datasets?'],\n",
    "  'short_answer_summary': 'The closed-form estimator is calculated by '\n",
    "                          'computing the mean of the posterior distribution '\n",
    "                          'conditioned on the data for each domain z using the '\n",
    "                          'given formula.'},\n",
    " {'answer': 'The online estimator in DoReMi is calculated as an iterative '\n",
    "            'version of the closed-form estimator. It is computed in an online '\n",
    "            'fashion over a minibatch of examples during the domain '\n",
    "            'reweighting step of the algorithm. For each model update at step '\n",
    "            't on an example x from domain z, the pseudo-count ˆθz(x) is '\n",
    "            'increased by the current domain weight αt corresponding to domain '\n",
    "            'z. The per-domain excess log-perplexities used in Algorithm 1 are '\n",
    "            'computed using a fixed, independent evaluation set of 30 '\n",
    "            'examples.',\n",
    "  'further_questions': ['How does the closed-form estimator work in DoReMi?',\n",
    "                        'How do domain weights affect the performance of the '\n",
    "                        'larger model?'],\n",
    "  'query': 'How is the online estimator calculated?',\n",
    "  'raw_node_ids': ['22', '8'],\n",
    "  'read_next_fragment': False,\n",
    "  'read_previous_fragment': False,\n",
    "  'related_questions': ['What is the purpose of the proxy model in DoReMi?',\n",
    "                        'How does DoReMi improve language model pretraining?'],\n",
    "  'short_answer_summary': 'The online estimator is an iterative version of the '\n",
    "                          'closed-form estimator, computed over a minibatch of '\n",
    "                          'examples during the domain reweighting step.'},\n",
    " {'answer': 'DoReMi is a technique that uses a small proxy model to optimize '\n",
    "            'domain weights of a language modeling dataset, improving the '\n",
    "            'training of a larger model. When comparing DoReMi with the '\n",
    "            'baseline, it shows several specific performance improvements. '\n",
    "            'DoReMi improves downstream accuracy by 6.5% and achieves the '\n",
    "            'baseline accuracy within 75k steps, which is 2.6x faster than the '\n",
    "            'baseline (200k steps). This results in dramatically sped-up '\n",
    "            'training and improved downstream performance. In addition, DoReMi '\n",
    "            'reduces the perplexity across all domains without a tradeoff. '\n",
    "            'Table 5 demonstrates that all DoReMi models, with various '\n",
    "            'reference/proxy model sizes ranging from 70M to 1B, improve '\n",
    "            'downstream performance significantly over the baseline.',\n",
    "  'further_questions': ['How does DoReMi optimize domain weights?',\n",
    "                        'What is the role of Group DRO optimizer in DoReMi?',\n",
    "                        'How does DoReMi perform on different datasets, such '\n",
    "                        'as The Pile and GLaM?'],\n",
    "  'query': 'What are the specific performance improvements realized when using '\n",
    "           'DoReMi?',\n",
    "  'raw_node_ids': ['6', '17'],\n",
    "  'read_next_fragment': False,\n",
    "  'read_previous_fragment': False,\n",
    "  'related_questions': ['How does DoReMi compare to other domain weight '\n",
    "                        'optimization techniques?',\n",
    "                        'What are the applications of DoReMi in real-world '\n",
    "                        'scenarios?',\n",
    "                        'What are the limitations of DoReMi?'],\n",
    "  'short_answer_summary': 'DoReMi improves downstream accuracy by 6.5%, '\n",
    "                          'achieves baseline accuracy 2.6x faster, and reduces '\n",
    "                          'perplexity across all domains without a tradeoff.'}]\n",
    "\n",
    "\n",
    "related_questions_answers = [{'answer': 'The purpose of using a proxy model in DoReMi is to optimize '\n",
    "            'domain weights of a language modeling dataset, which in turn '\n",
    "            'improves the training of a larger model. The proxy model is '\n",
    "            'trained using the Group DRO optimizer in the distributionally '\n",
    "            'robust language modeling (DRO-LM) framework, which minimizes the '\n",
    "            'worst-case loss across domains. By using a small proxy model to '\n",
    "            'optimize domain weights, it allows the larger model to have '\n",
    "            'better performance in terms of perplexity and downstream accuracy '\n",
    "            'on datasets like The Pile and GLaM.',\n",
    "  'further_questions': ['How does the Group DRO optimizer work?',\n",
    "                        'What is the distributionally robust language modeling '\n",
    "                        '(DRO-LM) framework?',\n",
    "                        'How is the performance of the larger model affected '\n",
    "                        'by different proxy model sizes?'],\n",
    "  'query': 'What is the purpose of using a proxy model in DoReMi?',\n",
    "  'raw_node_ids': ['2', '7'],\n",
    "  'read_next_fragment': False,\n",
    "  'read_previous_fragment': False,\n",
    "  'related_questions': ['What are the key components of the DoReMi technique?',\n",
    "                        'How does DoReMi compare to other techniques for '\n",
    "                        'optimizing domain weights?',\n",
    "                        'What are the benefits of using DoReMi for language '\n",
    "                        'modeling tasks?'],\n",
    "  'short_answer_summary': 'A proxy model in DoReMi optimizes domain weights in '\n",
    "                          'a language modeling dataset to improve the training '\n",
    "                          'of a larger model.'},\n",
    " {'answer': 'The iterative process in updating domain weights in DoReMi '\n",
    "            'involves running a loop over T iterative model updates. In the '\n",
    "            'domain reweighting step (Step 2, Algorithm 1), the average is '\n",
    "            'computed in an online fashion. The process runs for T = 500 steps '\n",
    "            'using a minibatch size of 1 over the n = 500 training examples '\n",
    "            'with a domain weight update rate η = 0.5. For the model update at '\n",
    "            'step t on an example x from domain z, the pseudo-count ˆθz(x) is '\n",
    "            'increased by the current domain weight αt corresponding to domain '\n",
    "            'z. The per-domain excess log-perplexities are computed using a '\n",
    "            'fixed, independent evaluation set of 30 examples instead of using '\n",
    "            'the examples in the minibatch.',\n",
    "  'further_questions': ['How are the initial domain weights chosen?',\n",
    "                        'How does the choice of reference model affect the '\n",
    "                        'domain weights?'],\n",
    "  'query': 'How does the iterative process work in updating domain weights?',\n",
    "  'raw_node_ids': ['22', '6'],\n",
    "  'read_next_fragment': False,\n",
    "  'read_previous_fragment': False,\n",
    "  'related_questions': ['What is the impact of domain reweighting on model '\n",
    "                        'performance?',\n",
    "                        'How does DoReMi improve language model training?'],\n",
    "  'short_answer_summary': 'The iterative process in DoReMi involves a loop '\n",
    "                          'over T iterative model updates, computing the '\n",
    "                          'average in an online fashion, and updating the '\n",
    "                          'domain weights using the current domain weight and '\n",
    "                          'the pseudo-count.'},\n",
    " {'answer': 'DoReMi, a technique that optimizes domain weights in a language '\n",
    "            'modeling dataset using a small proxy model, can be beneficial in '\n",
    "            'several practical applications. It improves the training of '\n",
    "            'larger language models by setting domain weights automatically '\n",
    "            'with only two small language model training runs. This results in '\n",
    "            'a more efficient training process, leading to better downstream '\n",
    "            'performance, as shown in the experiments conducted on The Pile '\n",
    "            'and the GLaM dataset. Additionally, DoReMi can significantly '\n",
    "            'reduce perplexity across all domains without a tradeoff, and it '\n",
    "            'can speed up training by achieving baseline accuracy within fewer '\n",
    "            'steps compared to traditional methods.',\n",
    "  'further_questions': ['How does DoReMi compare to other domain weighting '\n",
    "                        'techniques in terms of computational cost?',\n",
    "                        'What specific tasks have shown the most improvement '\n",
    "                        'using DoReMi in language modeling?'],\n",
    "  'query': 'What are some practical applications where DoReMi can be '\n",
    "           'beneficial?',\n",
    "  'raw_node_ids': ['9', '6'],\n",
    "  'read_next_fragment': False,\n",
    "  'read_previous_fragment': False,\n",
    "  'related_questions': ['How does DoReMi handle varying reference model sizes?',\n",
    "                        'How do domain weights found by DoReMi transfer across '\n",
    "                        'different model scales and compute budgets?'],\n",
    "  'short_answer_summary': 'DoReMi can be beneficial in improving the training '\n",
    "                          'of larger language models, speeding up the training '\n",
    "                          'process, and reducing perplexity across all domains '\n",
    "                          'without a tradeoff.'}]\n",
    "\n",
    "more_answers = ['In this work, reweighting refers to the allocation of samples from one '\n",
    " 'domain to another in order to reduce the parameter error for all domains. '\n",
    " 'The very noisy (high entropy/difficulty) domain 3 is downweighted in order '\n",
    " 'to reallocate samples to the other domains 1 and 2.',\n",
    " 'In this work, reweighting refers to the optimization of domain weights to '\n",
    " 'improve language models trained on a dataset. The algorithm used for this '\n",
    " 'optimization is called Domain Reweighting with Minimax Optimization '\n",
    " '(DoReMi), which leverages distributionally robust optimization (DRO) to tune '\n",
    " 'the domain weights without knowledge of downstream tasks. The optimized '\n",
    " 'domain weights are then used to train a large language model.',\n",
    " 'In this work, reweighting refers to the process of updating domain weights '\n",
    " 'in the Group DRO optimizer to upweight domains with high excess loss, which '\n",
    " 'scales up the proxy model’s gradient update on examples from these domains. '\n",
    " 'This is done in Step 2 of the DoReMi domain reweighting algorithm.']\n",
    "\n",
    "\n",
    "\n",
    "ans5 = dind.get_long_answer(\"What does reweighting mean in this work?\", previous_answer=previous_answer, \n",
    "                           further_questions_answers=further_questions_answers, \n",
    "                           related_questions_answers=related_questions_answers, more_answers=more_answers)\n",
    "print(ans5[\"answer\"])\n",
    "print(\"=\"*80)\n",
    "print(ans4[\"answer\"])\n",
    "print(\"=\"*80)\n",
    "print(ans3[\"answer\"])\n",
    "print(\"=\"*80)\n",
    "print(ans2[\"answer\"])\n",
    "print(\"=\"*80)\n",
    "print(ans[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "2fa397bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-23T10:02:46.630902Z",
     "start_time": "2023-05-23T10:00:51.740839Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer')).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 48 <class 'tuple'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.DocIndex at 0x7fe0911db4f0>"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['22', '2'] ['22', '2'] []\n",
      "{\n",
      "    \"answer\": \"In this work, reweighting refers to the process of adjusting the domain weights in a language modeling dataset to improve the training of a larger model. DoReMi, the technique used, employs a small proxy model to optimize these domain weights. It iteratively updates the domain weights using a closed-form estimator and an online version of the same estimator, calculated over a minibatch of examples. The resulting domain weights are then used to resample a dataset and train a larger, full-sized model. For instance, in the provided example, DoReMi returns domain weights [0.39, 0.61, 0.0], where the first domain (non-noisy) is increased slightly, the third domain (noisy) is decreased to 0 weight, and most of the weight is allocated to the second domain.\", \n",
      "    \"further_questions\": [\n",
      "        \"How does the closed-form estimator work in reweighting?\",\n",
      "        \"How does the online estimator work in reweighting?\",\n",
      "        \"What is the role of the small proxy model in DoReMi?\"\n",
      "    ],\n",
      "    \"short_answer_summary\": \"Reweighting in this work refers to adjusting domain weights in a language modeling dataset using DoReMi, a technique that employs a small proxy model and iterative updates to optimize the domain weights, ultimately improving the training of a larger model.\",\n",
      "    \"read_next_fragment\": False,\n",
      "    \"read_previous_fragment\": False,\n",
      "    \"related_questions\": [\n",
      "        \"What is the purpose of reweighting in language modeling?\",\n",
      "        \"How does DoReMi compare to other reweighting techniques?\"\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "dind = DocIndex(\"1\", \"https://arxiv.org/pdf/1912.08881.pdf\", \n",
    "                \"pdf\", \n",
    "                \"scientific_article\", doc_text, full_summary)\n",
    "dind\n",
    "\n",
    "ans = dind.get_short_answer(\"What does reweighting mean in this work?\")\n",
    "print(ans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "de93af82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T16:15:02.037574Z",
     "start_time": "2023-05-22T16:15:02.024857Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"answer\": \"The paper suggests a novel pruning framework based on Layer-wise Relevance Propagation (LRP) to reduce redundancy while maintaining performance in deep CNNs. LRP computes relevance scores for every unit of the deep model and uses these scores as a pruning criterion. The LRP-based pruning procedure involves a standard forward pass, backward propagation using LRP propagation rules, and pruning by eliminating irrelevant units with respect to the relevance quantity R obtained via LRP. This method is efficient, easily scalable to generic network structures, and can significantly reduce the computational complexity of deep neural networks. The experiments show that LRP-based pruning criterion can keep the test accuracy stable even without fine-tuning, as the pruning ratio increases.\",\n",
      "\n",
      "    \"further_questions\": [\n",
      "        \"How does LRP compute relevance scores for each unit?\",\n",
      "        \"What are the LRP propagation rules used in the pruning process?\",\n",
      "        \"How do the results of LRP-based pruning compare to other pruning methods in terms of accuracy and computational complexity?\"\n",
      "    ],\n",
      "\n",
      "    \"short_answer_summary\": \"The suggested pruning method is based on Layer-wise Relevance Propagation (LRP), which computes relevance scores for every unit and uses them as a pruning criterion. This method efficiently eliminates irrelevant units and maintains test accuracy even without fine-tuning.\",\n",
      "\n",
      "    \"read_next_fragment\": False,\n",
      "    \"read_previous_fragment\": False,\n",
      "\n",
      "    \"related_questions\": [\n",
      "        \"How does LRP-based pruning affect the computational complexity of deep neural networks?\",\n",
      "        \"What are the advantages of LRP-based pruning over other pruning methods?\",\n",
      "        \"In what scenarios is LRP-based pruning particularly useful?\"\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print('{\\n    \"answer\": \"The paper suggests a novel pruning framework based on Layer-wise Relevance Propagation (LRP) to reduce redundancy while maintaining performance in deep CNNs. LRP computes relevance scores for every unit of the deep model and uses these scores as a pruning criterion. The LRP-based pruning procedure involves a standard forward pass, backward propagation using LRP propagation rules, and pruning by eliminating irrelevant units with respect to the relevance quantity R obtained via LRP. This method is efficient, easily scalable to generic network structures, and can significantly reduce the computational complexity of deep neural networks. The experiments show that LRP-based pruning criterion can keep the test accuracy stable even without fine-tuning, as the pruning ratio increases.\",\\n\\n    \"further_questions\": [\\n        \"How does LRP compute relevance scores for each unit?\",\\n        \"What are the LRP propagation rules used in the pruning process?\",\\n        \"How do the results of LRP-based pruning compare to other pruning methods in terms of accuracy and computational complexity?\"\\n    ],\\n\\n    \"short_answer_summary\": \"The suggested pruning method is based on Layer-wise Relevance Propagation (LRP), which computes relevance scores for every unit and uses them as a pruning criterion. This method efficiently eliminates irrelevant units and maintains test accuracy even without fine-tuning.\",\\n\\n    \"read_next_fragment\": False,\\n    \"read_previous_fragment\": False,\\n\\n    \"related_questions\": [\\n        \"How does LRP-based pruning affect the computational complexity of deep neural networks?\",\\n        \"What are the advantages of LRP-based pruning over other pruning methods?\",\\n        \"In what scenarios is LRP-based pruning particularly useful?\"\\n    ]\\n}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d4c086",
   "metadata": {},
   "source": [
    "# Other Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16a5264",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class QuestionGeneration:\n",
    "    def __init__(self):\n",
    "        self.name = \"QuestionGeneration\"\n",
    "        self.description = \"\"\"\n",
    "QuestionGeneration:\n",
    "    This tool takes a text document and summarizes it into a shorter version while preserving the main points and context. Useful when the document is too long and needs to be shortened before further processing.\n",
    "\n",
    "    Input params/args: \n",
    "        long_document (str): document to summarize.\n",
    "\n",
    "    Returns: \n",
    "        str: summarized_document.\n",
    "\n",
    "    Usage:\n",
    "        `summary = LongSummarizer()(text_document=\"document to summarize\") # Note: this tool needs to be initialized first.`\n",
    "    \"\"\"\n",
    "        \n",
    "        self.prompt = PromptTemplate(\n",
    "            input_variables=[\"document\"],\n",
    "            template=\"\"\"Write as many valid and important question-answer pairs as can be answered/derived from the document below:\n",
    "\n",
    "{document}\n",
    "\n",
    "Separate the question-answer pairs by newline \\\\n and also put each question and answer in a newline.\n",
    "\n",
    "Questions and Answers:\n",
    "\n",
    "\"\"\",\n",
    "        )\n",
    "        \n",
    "    def __call__(self, document):\n",
    "        prompt = self.prompt.format(document=document)\n",
    "        try:\n",
    "            resp = call_ai21(prompt, temperature=0.5)\n",
    "            resp = resp.split('\\n')\n",
    "            assert len(resp)%2==0\n",
    "        except:\n",
    "            resp = call_ai21(prompt, temperature=0.4)\n",
    "            resp = resp.split('\\n')\n",
    "            assert len(resp)%2==0\n",
    "        resp = concat_array_two_at_a_time(resp)\n",
    "        return resp\n",
    "        \n",
    "        \n",
    "\n",
    "[QuestionGeneration()(chk) + QuestionGeneration()(chk_sum) for chk, chk_sum in zip(full_summary[\"chunks\"], full_summary[\"chunked_summary\"])]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "a74a326e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-23T16:50:11.244090Z",
     "start_time": "2023-05-23T16:50:11.181752Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "class DecisionMakerTool:\n",
    "    def __init__(self):\n",
    "        self.name = \"DecisionMakerTool\"\n",
    "        self.description = \"\"\"\n",
    "DecisionMakerTool:\n",
    "    This tool helps in making decisions. If you have some alternative action choices, based on some context or prior information and need to make a decision where decision can be a choice from set of options, then this tool is useful. This DecisionMakerTool is especially useful if the decision or choice cannot be made using python if-else but rather needs language support and more nuanced intelligence and world knowledge. Can also be used as a multi-choice reading comprehension QnA tool.\n",
    "\n",
    "    Input params/args: \n",
    "        context (str): Context on which decision/choice is to be made.\n",
    "        options (str): str representating options in format of `<option_number>: <option_name>` for each option, separated by commas.\n",
    "\n",
    "    Returns: \n",
    "        dict: {\"choice_reason\": <str, reason of making the choice, pros and cons, other thoughts>, \"choosen_option\": <choosen option as int>}\n",
    "\n",
    "    Usage:\n",
    "        `choice_decision = DecisionMakerTool()(context=\"Should a person stay awake at night.\", options=[\"1: No, 2: Yes\"]) # Note: this tool needs to be initialized first.`\n",
    "        `choice_decision = DecisionMakerTool()(context=\"You are given two stories about a monk who had to survive starvation below. first story: {story_1} , second story:  {story_2}, choose the story which best motivates a person suffering from starvation.\", options=[\"1: first story, 2: second story\"])`\n",
    "\n",
    "    \"\"\"\n",
    "        \n",
    "        self.prompt = PromptTemplate(\n",
    "            input_variables=[\"context\", \"options\"],\n",
    "            template=\"\"\"\n",
    "You are a helpful decision making tool. Your responsibility is to make a decision (from a set of options) given some query/context and a set of options. You will give a single number output that signifies your decision for the query/context.\n",
    "\n",
    "The context for which you need to make a decision or choose an option is given below:\n",
    "\n",
    "{context}\n",
    "\n",
    "The options you can take is given below in format of `<option_number>: <option_name>` for each option, separated by commas:\n",
    "\n",
    "{options}\n",
    "\n",
    "Choose one option from the given options (a single number) and also provide a reason for your choice. \n",
    "Your answer is a python dictionary which has two keys (choice_reason: <str, option number and then reason of making the choice, why this particular option over other options, pros and cons, other thoughts> and choosen_option: <choosen option as int>).\n",
    "Just output a python dict with these two keys (choice_reason and choosen_option) and their values only.\n",
    "\n",
    "\n",
    "\"\"\",\n",
    "        )\n",
    "        \n",
    "    def __call__(self, context, options):\n",
    "        prompt = self.prompt.format(context=context, options=options)\n",
    "        return eval(callGpt.get_turbo_call()(prompt, temperature=0))\n",
    "    \n",
    "\n",
    "class CallLargeLanguageModelWithInstructionsTool:\n",
    "    def __init__(self):\n",
    "        self.name = \"CallLargeLanguageModelWithInstructionsTool\"\n",
    "        self.description = \"\"\"\n",
    "CallLargeLanguageModelWithInstructionsTool:\n",
    "    This tool takes instructions and some data and passes them on to a Large Language Model for further processing. It can do language oriented tasks like summarization, question-answer, question-generation, etc, it cannot do web-search, mathematics and other non-language oriented tasks. Limitation- This tool can only generate or write only 2000 words at one time. It also has input length limit, where it can only take 2000 words at a time. Giving this tool more than 2000 words in either `instructions` or `data` will result in error.\n",
    "\n",
    "    Input params/args: \n",
    "        instructions (str): instructions to the language model on what to do with given data dictionary.\n",
    "        data (dict): data dictionary which language model uses along with instructions to produce some useful result. (Optional)\n",
    "\n",
    "    Returns: \n",
    "        str: model_result (model_result is less than 2000 words always)\n",
    "\n",
    "    Usage:\n",
    "        `model_result = CallLargeLanguageModelWithInstructionsTool()(instructions=\"instructions to language model\", data=<data_dictionary for using in format string of instruction>) # Note: this tool needs to be initialized first. # model_result is less than 2K words always`\n",
    "        `dog_owner_name_text = CallLargeLanguageModelWithInstructionsTool()(instructions=\"get the name of the owner of dog from the sentence: {text}\", data={\"text\": \"the pretty brown dog was owned by Mr. Miles\"}) # length of instructions + data should be less than 2K words always`\n",
    "\n",
    "    \"\"\"\n",
    "    def __call__(self, instructions, data=dict()):\n",
    "        try:\n",
    "            prompt = instructions.format(**data)\n",
    "        except:\n",
    "            prompt = instructions + \" \\n\\n Data for following instructions in python dictionary format: \\n\\n \" + str(data)\n",
    "        return callGpt.get_turbo_call()(prompt, temperature=0.3)\n",
    "\n",
    "\n",
    "class ContextualReader:\n",
    "    def __init__(self):\n",
    "        self.name = \"ContextualReader\"\n",
    "        self.description = \"\"\"\n",
    "ContextualReader:\n",
    "    This tool takes a context/query/instruction, and a text document. It reads the document based on the context or query instruction and outputs only parts of document relevant to the query. Useful when the document is too long and you need to store a short contextual version of it for answering the user request. Sometimes rephrasing the query/question/user request before asking the ContextualReader helps ContextualReader provide better results. You can also specify directives to ContextualReader like \"return numbers only\", along with the query for better results.\n",
    "\n",
    "    Input params/args: \n",
    "        context_user_query (str): instructions or query on how to read the document to provide contextually useful content from the document.\n",
    "        text_document (str): document to read and provide information from using context_user_query.\n",
    "\n",
    "    Returns: \n",
    "        str: contextual_content_from_document\n",
    "\n",
    "    Usage:\n",
    "        `contextual_content_from_document = ContextualReader()(context_user_query=\"instructions on how to read document\", text_document=\"document to read\") # Note: this tool needs to be initialized first.`\n",
    "\n",
    "    \"\"\"\n",
    "        \n",
    "        self.prompt = PromptTemplate(\n",
    "            input_variables=[\"context\", \"document\"],\n",
    "            template=\"\"\"\n",
    "You are given a request/context/instruction which specifies what needs to be done and any other specific instructions as below \\n\n",
    "{context}\n",
    "\n",
    "Next, you are also given a document which you have to read and gather more context and information to answer the question/instruction. \n",
    "Remember you don't need to answer the user's question now, you just need to gather more information which could possibly help in answering the user's question from this document given.\n",
    "Gather the information in a very concise succint way like a scientist, not like a novelist.\n",
    "Document is given below:\n",
    "{document}\n",
    "\"\"\",\n",
    "        )\n",
    "        \n",
    "    def get_one(self, context, document):\n",
    "        prompt = self.prompt.format(context=context, document=document)\n",
    "        return callGpt.get_turbo_call()(prompt, temperature=0.4)\n",
    "        \n",
    "    \n",
    "    def get_one_with_exception(self, context, document):\n",
    "        try:\n",
    "            cleaned_text = self.get_one(context, document)\n",
    "            \n",
    "            return cleaned_text\n",
    "        except Exception as e:\n",
    "            exp_str = str(e)\n",
    "            too_long = \"maximum context length\" in exp_str and \"your messages resulted in\" in exp_str\n",
    "            if too_long:\n",
    "                return \" \".join([self.get_one_with_exception(context, st) for st in split_text(document)])\n",
    "            raise e\n",
    "            \n",
    "    def __call__(self, context_user_query, text_document, chunk_size=3400):\n",
    "        import functools\n",
    "        part_fn = functools.partial(self.get_one_with_exception, context_user_query)\n",
    "        return process_text(text_document, chunk_size, part_fn)\n",
    "    \n",
    "    \n",
    "    \n",
    "class ContextualSummarizer(ContextualReader):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.name = \"ContextualSummarizer\"\n",
    "        self.description = \"\"\"\n",
    "ContextualSummarizer:\n",
    "    Similar to Contextual_Reader, but guarantees much smaller text outputs due to summarisation of inputs. This tool takes a context/query/instruction, and a text document. It summarises the document based on the context/query/instruction and outputs only parts of document relevant to user query. Very Useful when the contextual document is too long and you need to store a short contextual version of it.\n",
    "\n",
    "    Input params/args: \n",
    "        context_user_query (str): instructions or query on how to read the document to provide summary from the document.\n",
    "        text_document (str): document to read and sumarize from using context_user_query.\n",
    "\n",
    "    Returns: \n",
    "        str: summary_from_document\n",
    "\n",
    "    Usage:\n",
    "        `summary_from_document = ContextualSummarizer()(context_user_query=\"instructions on how to read document\", text_document=\"document to summarize\") # Note: this tool needs to be initialized first.`\n",
    "\n",
    "    \"\"\"\n",
    "        self.prompt = PromptTemplate(\n",
    "            input_variables=[\"context\", \"document\"],\n",
    "            template=\"\"\"\n",
    "You are given a context/instruction which specifies what is needed and any other specific instructions as below \\n\n",
    "{context}\n",
    "\n",
    "Next, you are also given a document which you have to read and gather more context and information to answer the question/instruction. \n",
    "Remember you don't need to answer the user's question now, you just need to gather more information which could possibly help in answering the user's question from this document given.\n",
    "Gather the information in a concise way like a scientist, not like a novelist. Ensure to provide short, point wise, summarised version, we intend the output to be small but still capture all details pertaining to \"{context}\".\n",
    "Document is given below:\n",
    "{document}\n",
    "\"\"\",\n",
    "        )\n",
    "        \n",
    "    \n",
    "\n",
    "class FuseInformation:\n",
    "    def __init__(self,):\n",
    "        self.name = \"FuseInformation\"\n",
    "        self.description = \"\"\"\n",
    "FuseInformation:\n",
    "    This tool takes a context/query/instruction, and two text documents, it then reads the two documents based on the context or query instruction and outputs only parts of the documents relevant to context/instruction. Useful when you have multiple documents and their combined length is too long and you need to store a short contextual version of both documents. Limitation - This tool can only generate or write only 2000 words at one time.\n",
    "\n",
    "    Input params/args: \n",
    "        context_user_query (str): instructions or query on how to read the documents and fuse them.\n",
    "        first_document (str): first document to read.\n",
    "        second_document (str): second document to read.\n",
    "\n",
    "    Returns: \n",
    "        str: fused_content_both_documents\n",
    "\n",
    "    Usage:\n",
    "        `fused_content_both_documents = FuseInformation()(context_user_query=\"instructions on how to read document\", first_document=\"first document to read\", second_document=\"second document to read\") # Note: this tool needs to be initialized first.`\n",
    "\n",
    "    \"\"\"\n",
    "        \n",
    "        self.prompt = PromptTemplate(\n",
    "            input_variables=[\"context\", \"first_document\", \"second_document\"],\n",
    "            template=\"\"\"\n",
    "You are given a request/context/query which specifies what needs to be done and any other specific instructions as below \\n\n",
    "{context}\n",
    "\n",
    "Next, you are also given a two documents which you have to read and gather more context and information to answer the question. \n",
    "Remember you don't need to answer the user's question now, you just need to gather more information which could possibly help in answering the user's question from this document given.\n",
    "Also remember to read both documents and find relevant information from both of them.\n",
    "Documents are given below:\n",
    "First Document:\n",
    "{first_document}\n",
    "\n",
    "\\n\\n\n",
    "Second Document:\n",
    "{second_document}\n",
    "\"\"\",\n",
    "        )\n",
    "    def __call__(self, context_user_query, first_document, second_document):\n",
    "        prompt = self.prompt.format(context=context_user_query, first_document=first_document, second_document=second_document)\n",
    "        return callGpt.get_hard_call()(prompt, temperature=0.2)\n",
    "        \n",
    "    \n",
    "class ContextualAnswer:\n",
    "    def __init__(self):\n",
    "        self.name = \"ContextualAnswer\"\n",
    "        self.description = \"\"\"\n",
    "ContextualAnswer:\n",
    "    This tool takes a context/query/instruction, and one text document, it then reads the document based on the context/query/instruction and provides an answer to the query/instruction using the document and its own knowledge. If no information/answer is found on requested query it says \"no answer\" in output.\n",
    "\n",
    "    Input params/args: \n",
    "        context_user_query (str): instructions or query on how to read the document to provide an answer from the document.\n",
    "        text_document (str): document to read and answer from.\n",
    "\n",
    "    Returns: \n",
    "        str: answer\n",
    "\n",
    "    Usage:\n",
    "        `answer = ContextualAnswer()(context_user_query=\"instructions on how to read document and answer\", text_document=\"document to read\") # Note: this tool needs to be initialized first.`\n",
    "\n",
    "    \"\"\"\n",
    "        \n",
    "        self.prompt = PromptTemplate(\n",
    "            input_variables=[\"context\", \"document\"],\n",
    "            template=\"\"\"\n",
    "You are given a context/instruction/query which specifies what is needed and any other specific instructions as below \\n\n",
    "\n",
    "{context}\n",
    "\n",
    "Next, you are also given a document which has some context which can help in answering the query or help find the right information.\n",
    "Remember if you can't answer the question given your own knowledge and the document, say that you can't answer.\n",
    "You can use the provided document as a support for your answer but you can also use your own prior knowledge. \n",
    "Answer the query based on the document and usually keep answers `short` unless asked to `elaborate`. If no information/answer is found on requested query say \"no answer\" in your output.\n",
    "Document is given below:\n",
    "{document}\n",
    "\n",
    "Answer:\n",
    "\n",
    "\"\"\",\n",
    "        )\n",
    "    def __call__(self, context_user_query, text_document):\n",
    "        prompt = self.prompt.format(context=context_user_query, document=text_document)\n",
    "        return callGpt.get_turbo_call()(prompt, temperature=0.2)\n",
    "    \n",
    "class ExtractInformationTool:\n",
    "    def __init__(self):\n",
    "        self.name = \"ExtractInformationTool\"\n",
    "        self.description = \"\"\"\n",
    "ExtractInformationTool:\n",
    "    This tool takes a context/query/instruction, and one text document, it then reads the document based on the context/query/instruction and extracts a single piece of information. If no information/answer is found on requested query it says \"no answer\" in output. To use this tool ask it pin pointed (not vague) questions (e.g. if you need bottle capacity, ask - \"capacity in litres\" not \"size of the bottle\",). Another rule is to ask just for one ( or a single piece of ) information (e.g. don't ask for name and place in same Tool call, if you need two pieces of information, call the tool twice with separate query each time).\n",
    "\n",
    "    Input params/args: \n",
    "        context_user_query (str): instructions or query on what information to extract from document.\n",
    "        text_document (str): document to read and extract information.\n",
    "\n",
    "    Returns: \n",
    "        str: answer\n",
    "\n",
    "    Usage:\n",
    "        `answer = ExtractInformationTool()(context_user_query=\"instructions on how to read document and extract a single piece of information\", text_document=\"document to read\") # Note: this tool needs to be initialized first.`\n",
    "\n",
    "    \"\"\"\n",
    "        \n",
    "        self.prompt = PromptTemplate(\n",
    "            input_variables=[\"context\", \"document\"],\n",
    "            template=\"\"\"\n",
    "You are given a context/instruction/query which specifies what is needed and any other specific instructions as below \\n\n",
    "{context}\n",
    "\n",
    "Next, you are also given a document which has some context which can help in answering the query or help find the right information.\n",
    "You need to extract the information from the provided document only. \n",
    "Usually the extracted information should be very short (one word or few word answers are preferred). \n",
    "If you are asked a number just give the number, if you are asked a name, just give the name, in general just the information or answer, no platitudes or preambles. \n",
    "Example: if answer is \"The number of people on train is 67\" -> just output \"67\".\n",
    "If no information/answer is found on requested query say \"no answer\" in your output.\n",
    "Document is given below:\n",
    "{document}\n",
    "\n",
    "\"\"\",\n",
    "        )\n",
    "    def __call__(self, context_user_query, text_document):\n",
    "        prompt = self.prompt.format(context=context_user_query, document=text_document)\n",
    "        return callGpt.get_turbo_call()(prompt, temperature=0.2)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "class AgentLM:\n",
    "    pass\n",
    "\n",
    "class ImageCaptioningTool:\n",
    "    # caption + description + objects present + fine-grained-caption\n",
    "    pass\n",
    "\n",
    "class GenerateImageTool:\n",
    "    pass\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93e1415",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Planner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d819fb84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T12:56:41.153142Z",
     "start_time": "2023-05-20T12:56:41.115439Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Apart from the steps to solve this problem also elucidate any new tool that could be useful for solving this task apart from the tools already provided.\n",
    "# Apart from the steps to solve this problem also elucidate any new tool that could be useful for solving this task apart from the tools already provided.\n",
    "\n",
    "\n",
    "\n",
    "def build_preamble(tense):\n",
    "    planner_preamble = \"\"\"\n",
    "You are given a request context/query/instruction which specifies what the user needs and any other specific user instructions as below \\n\n",
    "{context}\n",
    "\n",
    "Next, you are also given a set of tools ({tool_names}), which you can use (one or more times) to provide solution to user query/instruction. \n",
    "For most queries using one or more tools will help in providing better response to the user. Details about the tools and how to use them is as follows:\n",
    "{tools}\n",
    "\n",
    "Note that some tools provide `List[str]` or other list types as outputs (i.e multiple outputs) while other tools only take `str` or other non-list-types as inputs. Use `Step Calls` for this.\n",
    "All tools which provide str output ensure that the length of output is within the handling size of the system. String lengths can exceed handling length in case we summarise them.\n",
    "\"\"\" + \"\"\"You {tense} requested to come up with a step-by-step plan/strategy on how to answer the user query/instruction. \n",
    "You {tense} also given below rules to follow for developing this plan.\n",
    "- Your plan must be composed of small atomic actionable steps which can be executed in code using python and the above tools. \n",
    "- Each step in plan must map to few lines of code or usage of one tool. \n",
    "- Ensure to break the problem down to simple steps in your plan with each step having one goal or single responsibility.\n",
    "- Usually prefer using given tools in python code, rather than writing simple python code to do things.\n",
    "- You can change the query/request/instruction initially provided when passing it to the tools to better suit the tools input requirement.\n",
    "- In certain steps, you might just want to execute python code without uisng any tools described above. You can just say `Step Action: Python` for that step. \n",
    "- For steps where any tool is not needed (python code is enough), just mention the logic or step in simple english.\n",
    "- In certain steps, you might just want to execute python code without uisng any tools described above. You can just say `Step Action: Python` for that step. \n",
    "- If the request can be served by simple language generation, then just generate the language and `print` it in code.\n",
    "\n",
    "    \"\"\".format(tense=tense)\n",
    "    return planner_preamble\n",
    "\n",
    "class Planner:\n",
    "    def __init__(self, tools, model=\"gpt-4\"):\n",
    "        self.tools = tools\n",
    "        self.model = model\n",
    "        self.tools_info = \" \\n \".join([t.description for t in self.tools])\n",
    "        self.tools_names = \", \".join([t.name for t in self.tools])\n",
    "        self.prompt = PromptTemplate(\n",
    "            input_variables=[\"context\", \"tools\", \"tool_names\"],\n",
    "            template=build_preamble(tense=\"are\") + \"\"\"\n",
    "\n",
    "First, think about the request context/query/instruction and see what needs to be done. Is it asking a question, a multi-step question, request for code, request for short/long writing, information, requesting multiple items in same query, or something else.\n",
    "Write down a simple plan in english words as paragraph. \n",
    "Then, Write down a step wise plan in simple english using the tool names \n",
    "Next, Enumerate each step to take from your above plan in the below format.\n",
    "---\n",
    "Step: The order or rank of this step in the overall execution plan.\n",
    "    Calls: This has options `once`, `times_previous_output`,if current step accepts single input while previous step which is input to this provided multiple outputs, `N-times` where N is decided by you. \n",
    "    Reason: Why should we take this step or how this step can help us.\n",
    "    Action: Name of the Tool we can use in this step.\n",
    "    Input: Input to this step, a step can take input from any earlier step. Just write `Step N` (where is a number less than this step), if you want this step to take input from any previous step.\n",
    "    Expected Output: Expectations about the output of this step. Datatype and length, will it be a single word or number? or is it a long text output, or is it a list etc.\n",
    "---\n",
    "\n",
    "\"\"\",\n",
    "        )\n",
    "        \n",
    "        self.improve_prompt = PromptTemplate(\n",
    "            input_variables=[\"context\", \"tools\", \"plan\", \"critic\", \"tool_names\", \"code_output\", \"code\", \"variable_values\"],\n",
    "            template=build_preamble(tense=\"were\") + \"\"\"\n",
    "\n",
    "You came up with the below original plan for our user query or instruction:\n",
    "\n",
    "{plan}\n",
    "\n",
    "This plan/strategy has some flaws, it can possibly violate the rules we stated above, it is also possible that the plan may fail due to unforeseen errors.\n",
    "\n",
    "Flaws we found in plan:\n",
    "\n",
    "{critic}\n",
    "\n",
    "We also executed this plan in code and got the below code output:\n",
    "\n",
    "{code_output}\n",
    "\n",
    "This output came from code given below:\n",
    "\n",
    "{code}\n",
    "\n",
    "We also got some variables values from the above code as below:\n",
    "\n",
    "{variable_values}\n",
    "\n",
    "If any of code output, code, and variable values mentioned above are empty then you can ignore them, but if they have unexpected values then you should take them into consideration while planning.\n",
    "\n",
    "Try to think of any unforeseen errors we may hit when we convert this plan to python code as well.\n",
    "Now lets try to write an improved plan or strategy based on criticisms above as well as any additional improvements we can think of.\n",
    "\n",
    "First, write down a simple improved plan in english words. Also mention what improvements you made over original plan in a bullet list.\n",
    "Write down a new, improved and flaws corrected, step wise plan in simple english using the tool names, and for steps where any tool is not needed (python code is enough), just mention the logic or step in simple english.\n",
    "Next, Enumerate each step to take from your new plan in the below format (similar format as original plan, but steps may differ).\n",
    "---\n",
    "Step: The order or rank of this step in the overall execution plan.\n",
    "    Calls: This has options `once`, `times_previous_output`,if current step accepts single input while previous step which is input to this provided multiple outputs, `N-times` where N is decided by you. \n",
    "    Reason: Why should we take this step or how this step can help us.\n",
    "    Action: Name of the Tool we can use in this step.\n",
    "    Input: Input to this step, a step can take input from any earlier step. Just write `Step N` (where is a number less than this step), if you want this step to take input from any previous step.\n",
    "    Expected Output: Expectations about the output of this step. Datatype and length, will it be a single word or number? or is it a long text output, or is it a list etc.\n",
    "---\n",
    "\n",
    "\"\"\",\n",
    "        )\n",
    "    def __call__(self, context):\n",
    "        plan = self._create_plan(context)\n",
    "#         critic = self._critic_plan(context, plan)\n",
    "#         plan = self._improve_plan(context, plan, critic)\n",
    "        return plan\n",
    "    \n",
    "    def _create_plan(self, context):\n",
    "        prompt = self.prompt.format(context=context, tools=self.tools_info, tool_names=self.tools_names)\n",
    "        plan = callGpt.get_hard_call()(prompt, temperature=0.4)\n",
    "        return plan\n",
    "    \n",
    "    def _improve_plan(self, context, plan, critic, \n",
    "                      code_output, code, variable_values):\n",
    "        prompt = self.improve_prompt.format(context=context, \n",
    "                                            plan=plan, critic=critic, \n",
    "                                            tools=self.tools_info, \n",
    "                                            tool_names=self.tools_names, code_output=code_output, \n",
    "                                            code=code, variable_values=variable_values)\n",
    "        new_plan = callGpt.get_hard_call()(prompt, temperature=0.2)\n",
    "        return new_plan\n",
    "\n",
    "def short_code_preamble():\n",
    "    preamble = \"\"\"\n",
    "You are given a User request context or user instructions as below \\n\n",
    "{context}\n",
    "\n",
    "\n",
    "You are also given a set of tools which you can use. \n",
    "For most queries using one or more tools will help in providing better response to the user. Details about the tools and how to use them is as follows:\n",
    "{tools}\n",
    "\n",
    "Some tools provide `List[str]` or other list types as outputs (i.e multiple outputs) while other tools only take `str` or other non-list-types as inputs. Use `Step Calls` for this.\n",
    "All tools which provide str output ensure that the length of output is within the handling size of the system. String lengths can exceed handling length in case we summarise them.\n",
    "Further, you are also given a plan to solve this user request or query using above tools. The plan is as follows:\n",
    "{plan}\n",
    "    \"\"\"\n",
    "    return preamble\n",
    "\n",
    "def build_code_preamble():\n",
    "    preamble = short_code_preamble() + \"\"\"\n",
    "\n",
    "Plans are generally guidance on how to write code but they are not perfectly correct, you can use the plan as a weak guidance.\n",
    "Guidance/Rules for coding:\n",
    "- Usually prefer using given tools in python code, rather than writing simple python code to do things.\n",
    "- You can change the query/request/instruction initially provided when passing it to the tools to better suit the tools input requirement.\n",
    "- come up with a python code which we can use to execute the plan in a python program.\n",
    "- You can modify the user query before feeding it to any tools in the code itself. You can also modify intermediate results to better suit the tools that you will use next.\n",
    "- Within your code use comments to describe which lines roughly belong to which step in the plan above, and what these lines intend to do.\n",
    "- In certain steps, you might just want to execute python code without uisng any tools described above. Steps named as `Step Action: Python` in the plan are usually such steps.\n",
    "- Print results of important intermediate variables so that we can inspect and debug code easily. when printing a variable, print its name as well.\n",
    "- Just write pure code, which can be copy-pasted directly. Write code without using ``` signs.\n",
    "So if your code is like\n",
    "\n",
    "```python\n",
    "# code here\n",
    "```\n",
    "\n",
    "Then just output\n",
    "\n",
    "# code here\n",
    "\n",
    "\n",
    "Few code example below:\n",
    "---\n",
    "A sample code for an unrelated plan looks as below.\n",
    "\n",
    "```python\n",
    "query=\"Why should I avoid overfitting in machine learning?\"\n",
    "query_response = Search(query)\n",
    "from tqdm import tqdm\n",
    "\n",
    "full_context = None\n",
    "for link in tqdm(query_response):\n",
    "    page_stuff = GetWebPage()(link, context=query)\n",
    "    page_stuff = ContextualReader()(query, page_stuff)\n",
    "    if full_context is not None:\n",
    "        \n",
    "        if TextLengthCheck(full_context + page_stuff):\n",
    "            full_context = full_context + \" \\n\\n \" + page_stuff\n",
    "            \n",
    "        else:\n",
    "            # Fuse information of two documents since they are too long to concatenate, shorten them as needed.\n",
    "            page_context = ContextualSummariser()(page_context)\n",
    "            if TextLengthCheck(full_context + page_stuff):\n",
    "                full_context = FuseInformation()(query, full_context, page_context)\n",
    "            else:\n",
    "                full_context = ContextualSummariser()(full_context)\n",
    "                full_context = FuseInformation()(query, full_context, page_context)\n",
    "    else:\n",
    "        full_context = page_stuff\n",
    "\n",
    "answer = ContextualAnswer()(query, full_context)\n",
    "print(answer)\n",
    "\n",
    "```\n",
    "\n",
    "Another example, which failed :\n",
    "```\n",
    "query = \"Nikki Minaj birth year\"\n",
    "query_response = Search(query)\n",
    "\n",
    "full_context = None\n",
    "for link in tqdm(query_response):\n",
    "    page_stuff = GetWebPage()(link, context=query)\n",
    "    page_stuff = ContextualReader()(query, page_stuff)\n",
    "    if full_context is not None:\n",
    "        \n",
    "        if TextLengthCheck(full_context + page_stuff):\n",
    "            full_context = full_context + \" \\n \" + page_stuff\n",
    "            \n",
    "        else:\n",
    "            # Fuse information of two documents since they are too long to concatenate, shorten them as needed.\n",
    "            page_context = ContextualSummariser()(page_context)\n",
    "            if TextLengthCheck(full_context + page_stuff):\n",
    "                full_context = FuseInformation()(query, full_context, page_context)\n",
    "            else:\n",
    "                full_context = ContextualSummariser()(full_context)\n",
    "                full_context = FuseInformation()(query, full_context, page_context)\n",
    "    else:\n",
    "        full_context = page_stuff\n",
    "\n",
    "birth_year = ContextualAnswer()(query, full_context)\n",
    "squared_birth_year = MathTool()(\"%s ** 2\" % (birth_year))\n",
    "print(squared_birth_year)\n",
    "```\n",
    "\n",
    "this example failed due to below line and reason. As can be seen in this example, you can modify the user query before feeding it to any tools in the code itself.\n",
    "\n",
    "`birth_year = ContextualAnswer()(query, full_context)` this line gave a string. Instead if the query was modified before this line such as \n",
    "```\n",
    "query = query + \"\\n\\n\" + \" Only provide birth year, nothing else, just a number only.\"\n",
    "birth_year = ContextualAnswer()(query, full_context)\n",
    "```\n",
    "---\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    return preamble\n",
    "\n",
    "class PlantoCode:\n",
    "    def __init__(self, tools, model=\"gpt-4\"):\n",
    "        self.tools = tools\n",
    "        self.tools_info = \" \\n \".join([t.description for t in self.tools])\n",
    "        self.prompt = PromptTemplate(\n",
    "            input_variables=[\"context\", \"plan\", \"tools\"],\n",
    "            template=build_code_preamble(),\n",
    "        )\n",
    "        \n",
    "        self.correction_prompt = PromptTemplate(\n",
    "            input_variables=[\"context\", \"plan\", \"tools\", \"previous_code\", \"error_hint\", \"previous_output\"],\n",
    "            template=build_code_preamble() + \"\"\"\n",
    "\n",
    "You were requested to come up with a python code for the plan and you wrote the below code (previous code):\n",
    "```\n",
    "{previous_code}\n",
    "```\n",
    "\n",
    "This code did not run properly. Some guesses about possible errors: \n",
    "\n",
    "{error_hint}\n",
    "\n",
    "Output of previous code is also given:\n",
    "\n",
    "{previous_output}\n",
    "\n",
    "Now Lets try to rewrite the code to get better results this time.\n",
    "In certain steps, you might just want to execute python code without uisng any tools described above. Steps named as `Step Action: Python` in the plan are usually such steps.\n",
    "Within your code use comments to describe which lines roughly belong to which step in the plan above.\n",
    "\n",
    "\n",
    "\n",
    "\"\"\",\n",
    "        )\n",
    "        \n",
    "        self.judge_dict = \"\"\"\n",
    "{\n",
    "    \"success\": <bool True/False, if everything succeeded>, \n",
    "    \"output\": <final output if code succeeded>, \n",
    "    \"is_code_failure\": <bool True/False whether the problem was caused due to code failure>, \n",
    "    \"code_failure_reason\":\"possible reason of why code failed\", \n",
    "    \"is_plan_failure\": <bool True/False whether the problem was caused due to plan failure>,\n",
    "    \"plan_failure_reason\": \"why the plan failed and which step possibly failed or is responsible for failure\",\n",
    "}\n",
    "\"\"\"\n",
    "        self.judge_prompt = PromptTemplate(\n",
    "            input_variables=[\"context\", \"plan\", \"tools\", \"code\", \"stdout\", \"error_hint\", \"previous_output\", \"judge_dict\"],\n",
    "            template=short_code_preamble()+\"\"\"\n",
    "\n",
    "You were requested to come up with a python code for the plan and you wrote the below code:\n",
    "```\n",
    "{code}\n",
    "```\n",
    "\n",
    "This code provided the following stdout on the console:\n",
    "\n",
    "{stdout}\n",
    "\n",
    "We also have possible exception stack trace from this code as below:\n",
    "\n",
    "{error_hint}\n",
    "\n",
    "Output of code is also given:\n",
    "\n",
    "{previous_output}\n",
    "\n",
    "Based on these, we need to find three things, \n",
    "1. Did the code succeed and output the expected answer. \n",
    "2. If the code failed, then was it a failure of code, due to exception or wrong code usage.\n",
    "3. Was it a failure of our plan instead since the plan itself maybe too hard to execute or incorrect or other reasons.\n",
    "\n",
    "Finally, your output should only be a python dictionary (with keys as \"success\", \"output\", \"is_code_failure\", \"code_failure_reason\", \"is_plan_failure\", \"plan_failure_reason\") as below:\n",
    "\n",
    "{judge_dict}\n",
    "\n",
    "Only one of is_plan_failure , is_code_failure has to be True, if success is False. \n",
    "Return only a python dictionary, nothing else.\n",
    "\"\"\",\n",
    "        )\n",
    "    def __call__(self, context, plan):\n",
    "        prompt = self.prompt.format(context=context, plan=plan, tools=self.tools_info)\n",
    "        code = callGpt.get_hard_call()(prompt, temperature=0.7)\n",
    "        \n",
    "        return code\n",
    "    \n",
    "    \n",
    "    def make_correction(self, context, plan, previous_code, error_hint, previous_output):\n",
    "        prompt = self.correction_prompt.format(context=context, plan=plan, tools=self.tools_info, \n",
    "                                               previous_code=previous_code, error_hint=error_hint, previous_output=previous_output)\n",
    "        return callGpt.get_hard_call()(prompt, temperature=0.7)\n",
    "    \n",
    "    def is_output_expected(self, context, plan, code, stdout, error_hint, previous_output):\n",
    "        prompt = self.judge_prompt.format(context=context, plan=plan, tools=self.tools_info, \n",
    "                                          code=code, stdout=stdout, \n",
    "                                          error_hint=error_hint, \n",
    "                                          previous_output=previous_output, judge_dict=self.judge_dict)\n",
    "        return callGpt.get_hard_call()(prompt, temperature=0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59b06551",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T12:56:41.770282Z",
     "start_time": "2023-05-20T12:56:41.725917Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Plans are generally guidance on how to write code but they are not perfectly correct.\n",
    "# Apart from the steps to solve this problem also elucidate any new tool that could be useful for solving this task apart from the tools already provided.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "class StepbyStepPlanExecutor:\n",
    "    def __init__(self, tools, model=\"gpt-4\"):\n",
    "        self.tools = tools\n",
    "        self.model = model\n",
    "        self.tools_info = \" \\n \".join([t.description for t in self.tools])\n",
    "        self.prompt_dict_format = \"\"\"\n",
    "{\"step\": <the step for which we wrote code>, \"code\": <full detailed python code for next step>, \"expectation\": <output expectation we have from code>, \"final_variable\": <Name of variable in which the final output of this stage/step is stored>, \"more_steps_needed\": <bool, whether we need more steps after this or this is the final step>}\n",
    "        \"\"\"\n",
    "        self.prompt = PromptTemplate(\n",
    "            input_variables=[\"context\", \"plan\", \"tools\", \n",
    "                             \"full_code\", \"code_till_now\", \n",
    "                             \"last_output_sample\", \"last_step_no\", \"prompt_dict_format\"],\n",
    "            template=\"\"\"\n",
    "You are given a request/context/query/instruction or user instructions as below \\n\n",
    "{context}\n",
    "\n",
    "\n",
    "You are also given a set of tools which you can use. Details about the tools and how to use them is as follows:\n",
    "{tools}\n",
    "\n",
    "You can change the query/request/instruction initially provided when passing it to the tools to better suit the tools input requirement.\n",
    "Usually prefer using given tools in python code, rather than writing simple python code to do things.\n",
    "Further, you are also given a plan to solve this user request or query using above tools. The plan is as follows:\n",
    "{plan}\n",
    "\n",
    "Plans are generally guidance on how to write code but they are not perfectly correct.\n",
    "\n",
    "You can change the query/request/instruction initially provided when passing it to the tools to better suit the tools input requirement.\n",
    "The full code to execute this plan using the tools we have was written by a novice programmer and maybe incorrect. The full code is as follows:\n",
    "{full_code}\n",
    "\n",
    "We have executed upto the following steps (Last executed step):\n",
    "{last_step_no}\n",
    "\n",
    "We are running this code step by step and we have currently executed upto the given below fragment of code. \n",
    "{code_till_now}\n",
    "\n",
    "The output of the last step from the steps that we have already ran looks as below\n",
    "\"{last_output_sample}\"\n",
    "\n",
    "If no steps are executed till now then the fragment of code will have \"No steps executed\" and output of the last step will have \"No output\" and Last executed step will be 0.\n",
    "\n",
    "Lets think of the code for the next step. \n",
    "Don't do any imports in the code, all imports are taken care of. Skip `from xxx import yyy` type statements.\n",
    "Provide output in a python dictionary only. We only want a python dictionary as an output of this step. \n",
    "The format for the python dictionary, in which output should be provided, is as below (output dictionary looks like below):\n",
    "\n",
    "{prompt_dict_format}\n",
    "\n",
    "\"\"\",\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.critic_output_dict = \"\"\"\n",
    "{\"step\": {last_step_no}, \"Output as expected\": True/False, \"Reasoning\": <actual reasoning>, \"Suspected Code Error\": <suspected error in code or otherwise in our execution>, \"Correction in words\": <correction needed>, \"Step to go back\": <step no to go back>}\n",
    "        \"\"\"\n",
    "        \n",
    "        self.critic_prompt = PromptTemplate(\n",
    "            input_variables=[\"context\", \"plan\", \"tools\", \"full_code\", \"code_till_now\",\"last_step_no\", \"current_step_code\", \"current_output\", \"critic_output_dict\"],\n",
    "            template=\"\"\"\n",
    "You were given a request/context/query/instruction or instructions as below \\n\n",
    "{context}\n",
    "\n",
    "You were also given a set of tools which you can use. Details about the tools and how to use them is as follows:\n",
    "{tools}\n",
    "\n",
    "You can change the query/request/instruction initially provided when passing it to the tools to better suit the tools input requirement.\n",
    "Usually prefer using given tools in python code, rather than writing simple python code to do things.\n",
    "Further, you were also given a plan to solve this user request or query using above tools. The plan is as follows:\n",
    "{plan}\n",
    "\n",
    "The full code to execute this plan using the tools we have was written by a novice programmer and maybe incorrect. The full code is as follows:\n",
    "{full_code}\n",
    "\n",
    "We have executed upto the following steps (Last executed step / most recent step / current executed step):\n",
    "{last_step_no}\n",
    "\n",
    "We are running this code step by step and we have currently executed upto the given below fragment of code. \n",
    "{code_till_now}\n",
    "\n",
    "with the last executed code fragment being:\n",
    "{current_step_code}\n",
    "\n",
    "The output of the current step looks as below\n",
    "\"{current_output}\"\n",
    "\n",
    "If no steps are executed till now then the fragment of code will have \"No steps executed\" and output of the last step will have \"No output\" and Last executed step will be 0.\n",
    "\n",
    "Now, Lets critically evaluate if the output of the current step looks correct and as expected. \n",
    "Before providing your analysis, provide non-code textual information about what may have went wrong and how we may approach it.\n",
    "Guide your analysis in the below format and provide output only in a python dictionary. \n",
    "Make sure your output is only a valid python dictionary with the keys in the structure below.\n",
    "\n",
    "Thought Analysis framework:\n",
    "---\n",
    "Step: {last_step_no}\n",
    "Output as expected: True/False (True if output is as expected, else False)\n",
    "Reasoning: Why this output is expected or not expected.\n",
    "Suspected Code Error: Error we suspect might have occured in code, maybe in code syntax, maybe in our assumptions about the tools, or maybe runtime errors, or any other unexpected error, elaborate.\n",
    "Correction in words: normal non-code explanation of the correction we will try to make.\n",
    "Step to go back: Step number to which we should go back to correct, if only current step needs correcting then write the current step number({last_step_no})\n",
    "---\n",
    "\n",
    "output dictionary looks like this:\n",
    "{critic_output_dict}\n",
    "\"\"\",\n",
    "        )\n",
    "        \n",
    "        # TODO: codes till now only need from which step to which step it is, not full code.\n",
    "        self.suggestion_dict = \"\"\"\n",
    "{\"step\": {last_step_no}, \"Suspected Code Error\": <code error we suspect happened>, \"Correction in words we will make\": <textual description of the correction>, \"Step to go back\":<step number which we intend to change>, \"corrected_code\": <corrected python code till this step, this step included>, \"final_variable\": <Name of variable in which the final output of corrected code will be stored>, \"more_steps_needed\": <bool, whether we need more steps after this or this is the final step>}\n",
    "        \"\"\"\n",
    "        self.suggest_prompt = PromptTemplate(\n",
    "            input_variables=[\"context\", \"plan\", \"full_code\", \"tools\", \"last_step_no\", \"code_fails\",\"suggestion_dict\"],\n",
    "            template=\"\"\"\n",
    "You were given a request/context/query/instruction or other instructions as below \\n\n",
    "{context}\n",
    "\n",
    "You were also given a set of tools which you can use. Details about the tools and how to use them is as follows:\n",
    "{tools}\n",
    "\n",
    "You can change the query/request/instruction initially provided when passing it to the tools to better suit the tools input requirement.\n",
    "Usually prefer using given tools in python code, rather than writing simple python code to do things.\n",
    "Further, you were also given a plan to solve this user request or query using above tools. The plan is as follows:\n",
    "{plan}\n",
    "\n",
    "The full code to execute this plan using the tools we have was written by a novice programmer and was incorrect. The full code is as follows:\n",
    "{full_code}\n",
    "\n",
    "We have executed upto the following steps (Last executed step / most recent step / current executed step):\n",
    "{last_step_no}\n",
    "\n",
    "We are trying to write correct code to run this plan above. Till now we have tried below codes which are given along with the outputs/errors and hypothesized failure reasons.\n",
    "\n",
    "{code_fails}\n",
    "\n",
    "Before providing your analysis, provide non-code textual information about what may have went wrong and how we may approach it.\n",
    "Give your analysis in the below format. \n",
    "Along with analysis you are also expected to output corrected code, to correct code you may need to go back multiple steps, as such mention the step to go back to, and also a full corrected code from that point till current step, with the current step as well.\n",
    "Remember you need to write only the corrected code till the current step (including the current step code). You don't need to write full code till the end.\n",
    "Think of your output in below framework, but finally provide the output as a python dictionary only.\n",
    "---\n",
    "Step: {last_step_no}\n",
    "Suspected Code Error: Error we suspect might have occured in code, maybe in code syntax, maybe in our assumptions about the tools, or maybe runtime errors, elaborate.\n",
    "Correction in words we will make: normal non-code explanation of the correction we will try to make.\n",
    "Step to go back: Step number to which we should go back to correct, if only current step needs correcting then write the current step number({last_step_no})\n",
    "Corrected Code:\n",
    "```python\n",
    "# Corrected Code only from the step we needed to correct till the current step, including the current step.\n",
    "```\n",
    "final_variable: <Name of variable in which the final output of this stage is stored>\n",
    "---\n",
    "Don't do any imports in the code, all imports are taken care of. Skip `from xxx import yyy` type statements.\n",
    "Remember to output corrected code only from the step that needs correction, not from beginning of the actual code.\n",
    "Output Python dictionary structure:\n",
    "{suggestion_dict}\n",
    "\n",
    "\"\"\",\n",
    "        )\n",
    "    def get_next_step_code(self, context, plan, full_code, code_till_now, last_step_no, last_output_sample):\n",
    "        prompt = self.prompt.format(context=context, plan=plan, full_code=full_code, \n",
    "                                    code_till_now=code_till_now, last_step_no=last_step_no, \n",
    "                                    last_output_sample=last_output_sample, tools=self.tools_info, \n",
    "                                    prompt_dict_format=self.prompt_dict_format)\n",
    "        return callGpt.get_hard_call()(prompt, temperature=0.2)\n",
    "        \n",
    "    def verify_step_code_output(self, context, plan, full_code, code_till_now, \n",
    "                                last_step_no, current_step_code, current_output):\n",
    "        prompt = self.critic_prompt.format(context=context, plan=plan, full_code=full_code, \n",
    "                                    code_till_now=code_till_now, last_step_no=last_step_no, \n",
    "                                           current_step_code=current_step_code, critic_output_dict=self.critic_output_dict,\n",
    "                                           current_output=current_output, tools=self.tools_info)\n",
    "        return callGpt.get_hard_call()(prompt, temperature=0.2)\n",
    "    def suggest_corrections(self, context, plan, full_code, last_step_no, codes_till_now, \n",
    "                            errors_or_outputs_till_now,\n",
    "                        reasons_till_now,):\n",
    "        \n",
    "        code_fails = [f\"{i+1} \\n code: \\n{c}\\n error or output: \\n{e}\\n reason: \\n{r}\\n\" for i, (c,e,r) in enumerate(zip(codes_till_now, errors_or_outputs_till_now, reasons_till_now))]\n",
    "        prompt = self.suggest_prompt.format(context=context, plan=plan, full_code=full_code, \n",
    "                                    last_step_no=last_step_no, code_fails=code_fails, \n",
    "                                    tools=self.tools_info, suggestion_dict=self.suggestion_dict)\n",
    "        return callGpt.get_hard_call()(prompt, temperature=0.2)\n",
    "    \n",
    "    def is_doable(self):\n",
    "        pass\n",
    "    \n",
    "    def forward_one_step(self, context, plan, full_code, code_till_now, last_step_no, last_output_sample):\n",
    "        # Attempt till prompt size is too big for fitting all details of a step.\n",
    "        import traceback\n",
    "        retries = 0\n",
    "        errors = []\n",
    "        codes_till_now = []\n",
    "        reasons_till_now = []\n",
    "        recent_correction = None\n",
    "        recent_error = None\n",
    "        next_step = self.get_next_step_code(context, plan, full_code, code_till_now, last_step_no, last_output_sample)\n",
    "        next_step = eval(next_step)\n",
    "        more_steps_needed = next_step[\"more_steps_needed\"]\n",
    "        output = None\n",
    "        try:\n",
    "            while retries < 5:\n",
    "                \n",
    "                next_step_code = next_step[\"code\"]\n",
    "                try:\n",
    "                    print(next_step_code)\n",
    "                    exec(next_step_code)\n",
    "                    # TODO: truncate the output view for verification step since it may be too long.\n",
    "                    output = str(eval(next_step[\"final_variable\"]))\n",
    "                except Exception as e:\n",
    "                    output = str(e)\n",
    "\n",
    "                # We do below because self.get_next_step_code generates code for one step, but self.suggest_corrections generates full code from beginning\n",
    "                if retries == 0 and last_step_no > 0:\n",
    "                    code_for_now = \"\\n\".join([code_till_now, next_step_code])\n",
    "                else:\n",
    "                    code_for_now = next_step_code\n",
    "                verification = self.verify_step_code_output(context, plan, full_code, \n",
    "                                                            code_for_now, \n",
    "                                             last_step_no + 1, next_step_code, output)\n",
    "                verification = eval(verification)\n",
    "                if verification[\"Output as expected\"]:\n",
    "                    return {\"code_till_now\": code_for_now, \n",
    "                            \"last_step_no\": last_step_no, \n",
    "                            \"current_step_no\": last_step_no + 1, \n",
    "                            \"more_steps_needed\":more_steps_needed,\n",
    "                            \"reasons_till_now\": reasons_till_now,\n",
    "                \"codes_till_now\": codes_till_now,\n",
    "                \"errors\": errors,\n",
    "                            \"current_output_sample\": output,\n",
    "                            \"retried\": retries, \"success\": True}\n",
    "                errors.append(output)\n",
    "                codes_till_now.append(code_for_now)\n",
    "                verification[\"correction made in this version\"] = recent_correction\n",
    "                verification[\"suspected error in previous version\"] = recent_error\n",
    "                reasons_till_now.append(verification)\n",
    "                correction_dict = self.suggest_corrections(context, plan, full_code, last_step_no + 1, codes_till_now, \n",
    "                                errors,\n",
    "                                    reasons_till_now,)\n",
    "                correction_dict = eval(correction_dict)\n",
    "                assert correction_dict[\"more_steps_needed\"] == more_steps_needed\n",
    "                recent_correction = correction_dict[\"Correction in words we will make\"]\n",
    "                recent_error = correction_dict[\"Suspected Code Error\"]\n",
    "                next_step = {\"code\": correction_dict[\"corrected_code\"], \"final_variable\": correction_dict[\"final_variable\"]}\n",
    "                retries += 1\n",
    "        except Exception as e:\n",
    "            output = traceback.format_exc()\n",
    "        return {\"code_till_now\": code_till_now, \n",
    "                \"current_step_code\": next_step_code,\n",
    "                \"last_step_no\": last_step_no, \n",
    "                \"current_step_no\": last_step_no,\n",
    "                \"more_steps_needed\":True,\n",
    "                \"reasons_till_now\": reasons_till_now,\n",
    "                \"codes_till_now\": codes_till_now,\n",
    "                \"errors\": errors,\n",
    "                \"retried\": retries, \"success\": False, \"current_output_sample\": output}\n",
    "    \n",
    "    def forward_from_start_till_output(self, context, plan, full_code, ):\n",
    "        next_step = self.forward_one_step(context, plan, full_code, \"No steps executed\", 0, \"No output\")\n",
    "        more_steps_needed = next_step[\"more_steps_needed\"]\n",
    "        current_step_success = next_step[\"success\"]\n",
    "        if not current_step_success:\n",
    "            # re_plan and re_code\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        while more_steps_needed:\n",
    "            next_step = self.forward_one_step(context, plan, full_code, next_step['code_till_now'], \n",
    "                                          next_step['current_step_no'], \n",
    "                                          next_step['current_output_sample'])\n",
    "            more_steps_needed = next_step[\"more_steps_needed\"]\n",
    "            current_step_success = next_step[\"success\"]\n",
    "            if not current_step_success:\n",
    "                # re_plan and re_code\n",
    "                pass\n",
    "            \n",
    "        return next_step\n",
    "    \n",
    "    def re_plan_for_failure(self):\n",
    "        pass\n",
    "    \n",
    "    def re_plan_for_success(self):\n",
    "        pass\n",
    "    \n",
    "    def re_code(self):\n",
    "        pass\n",
    "    \n",
    "    def re_think_final_output(self,):\n",
    "        pass\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "\n",
    "class PredefinedPlans:\n",
    "    # Plan name, plan, code, when to use\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "095dac3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T17:48:26.712812Z",
     "start_time": "2023-05-20T17:48:26.655624Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def build_preamble():\n",
    "    planner_preamble = \"\"\"\n",
    "You are given a request task/context/query/instruction which specifies a task or set of instructions as below \\n\n",
    "{context}\n",
    "\n",
    "You are also given a set of tools ({tool_names}), which you can use (one or more times) to provide solution to the task/query/instruction/request. \n",
    "For most queries using one or more tools (python classes/functions/library) will help in providing better response to the user. Details about the tools and how to use them is as follows:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Some tools provide `List[str]` or other list types as outputs (i.e multiple outputs) while other tools only take `str` or other non-list-types as inputs. Use `Step Calls` for this.\n",
    "All tools which provide str output ensure that the length of output is within the handling size of the system. \n",
    "String lengths may exceed handling length if we concatenate them, in which case we summarise them before concatenation.\n",
    "You are requested to come up with a step-by-step plan/strategy on how to answer the query/instruction. \n",
    "Follow these rules for developing this plan.\n",
    "- Think about the request context/query/instruction and see what needs to be done. Is it asking a question, a multi-step question, request for code, request for short/long writing, information, requesting multiple items in same query, or something else.\n",
    "- Your plan must be composed of small atomic actionable steps which can be executed in code using python and the above tools. \n",
    "- Ensure to break the problem down to simple steps in your plan with each step having one goal or single responsibility.\n",
    "- Prefer using given tools in python code, rather than writing simple python code to do things.\n",
    "- You can change the query/request/instruction initially provided when passing it to the tools to better suit the tools input requirement.\n",
    "- In any steps, you might just want to execute python code without uisng any tools. You can just say `Step Action: Python` for that step. \n",
    "- For steps where any tool is not needed (python code is enough), just mention the logic or step in simple english.\n",
    "- If the request can be served by simple language generation, then just generate the language and `print` your output in code.\n",
    "\n",
    "Think a simple plan in english words as paragraph. \n",
    "Then, think a step wise plan in simple english using the tool names \n",
    "Next, think of each step to take from your above plan in the below \"planner format\".\n",
    "---\n",
    "Step: The order or rank of this step in the overall execution plan.\n",
    "    Calls: This has options `once`, `times_previous_output`,if current step accepts single input while previous step which is input to this provided multiple outputs, `N-times` where N is decided by you. \n",
    "    Reason: Why should we take this step or how this step can help us.\n",
    "    Action: Name of the Tool we can use in this step.\n",
    "    Input: Input to this step, a step can take input from any earlier step. Just write `Step N` (where is a number less than this step), if you want this step to take input from any previous step.\n",
    "    Expected Output: Expectations about the output of this step. Datatype and length, will it be a single word or number? or is it a long text output, or is it a list etc.\n",
    "---\n",
    "\n",
    "After writing the plan, think of python code to write using our given tools to execute this plan.\n",
    "Guidance/Rules for coding:\n",
    "- Plans are generally guidance on how to write code but they are not perfectly correct, you can use the plan as a weak guidance.\n",
    "- Perform exception handling and for cases where we expect exceptions or errors, handle them and create backup methods there.\n",
    "- You can modify the  request/query/instruction before feeding it to any tools in the code itself. You can also modify intermediate results to better suit the tools that you will use next.\n",
    "- Within your code use comments to describe which lines roughly belong to which step in the plan above, and what these lines intend to do.\n",
    "- Print results of important intermediate variables so that we can inspect and debug code easily. when printing a variable, print its name as well.\n",
    "- While writing code think, if this line of code fails what do I do? If this line throws any exception what do I do? Do this for each line of code you write.\n",
    "- In the `code` / `full_corrected_code` / `corrected_code_from_error_point_to_end` fields, just write code, no escape symbols or backslashes, pure python code there only.\n",
    "\n",
    "Few code example below:\n",
    "---\n",
    "A sample code for an unrelated plan looks as below.\n",
    "\n",
    "```python\n",
    "query=\"Why should I avoid overfitting in machine learning?\"\n",
    "query_response = Search(query)\n",
    "from tqdm import tqdm\n",
    "\n",
    "full_context = None\n",
    "for link in tqdm(query_response):\n",
    "    page_stuff = GetWebPage()(link, context=query)\n",
    "    page_stuff = ContextualReader()(query, page_stuff)\n",
    "    if full_context is not None:\n",
    "        \n",
    "        if TextLengthCheck(full_context + page_stuff):\n",
    "            full_context = full_context + \" \\n\\n \" + page_stuff\n",
    "            \n",
    "        else:\n",
    "            # Fuse information of two documents since they are too long to concatenate, shorten them as needed.\n",
    "            page_context = ContextualSummariser()(page_context)\n",
    "            if TextLengthCheck(full_context + page_stuff):\n",
    "                full_context = FuseInformation()(query, full_context, page_context)\n",
    "            else:\n",
    "                full_context = ContextualSummariser()(full_context)\n",
    "                full_context = FuseInformation()(query, full_context, page_context)\n",
    "    else:\n",
    "        full_context = page_stuff\n",
    "\n",
    "answer = ContextualAnswer()(query, full_context)\n",
    "print(answer)\n",
    "\n",
    "```\n",
    "\n",
    "Another example, which failed :\n",
    "```\n",
    "query = \"Nikki Minaj birth year\"\n",
    "query_response = Search(query)\n",
    "\n",
    "full_context = None\n",
    "for link in tqdm(query_response):\n",
    "    page_stuff = GetWebPage()(link, context=query)\n",
    "    page_stuff = ContextualReader()(query, page_stuff)\n",
    "    if full_context is not None:\n",
    "        \n",
    "        if TextLengthCheck(full_context + page_stuff):\n",
    "            full_context = full_context + \" \\n \" + page_stuff\n",
    "            \n",
    "        else:\n",
    "            # Fuse information of two documents since they are too long to concatenate, shorten them as needed.\n",
    "            page_context = ContextualSummariser()(page_context)\n",
    "            if TextLengthCheck(full_context + page_stuff):\n",
    "                full_context = FuseInformation()(query, full_context, page_context)\n",
    "            else:\n",
    "                full_context = ContextualSummariser()(full_context)\n",
    "                full_context = FuseInformation()(query, full_context, page_context)\n",
    "    else:\n",
    "        full_context = page_stuff\n",
    "\n",
    "birth_year = ContextualAnswer()(query, full_context)\n",
    "squared_birth_year = MathTool()(\"%s ** 2\" % (birth_year))\n",
    "print(squared_birth_year)\n",
    "```\n",
    "\n",
    "this example failed due to below line and reason. As can be seen in this example, you can modify the user query before feeding it to any tools in the code itself.\n",
    "\n",
    "`birth_year = ContextualAnswer()(query, full_context)` this line gave a string. Instead if the query was modified before this line such as \n",
    "```\n",
    "query = query + \"\\n\\n\" + \" Only provide birth year, nothing else, just a number only.\"\n",
    "birth_year = ContextualAnswer()(query, full_context)\n",
    "```\n",
    "\n",
    "Also think of any new tool that we need but we don't have yet which will help us solve this query or request better or easily.\n",
    "Finally, your output should only be a python dictionary (with keys as \"plan_in_simple_english\", \"stepwise_plan_with_toolnames\", \"plan_in_planner_format\", \"thoughts_before_writing_code\", \"code\", \"any_new_tool_that_we_need\") as below:\n",
    "\n",
    "{plan_and_code_dict}\n",
    "\n",
    "Return only a python dictionary in format above with the keys asked above, nothing else.\n",
    "\n",
    "---\n",
    "    \"\"\"\n",
    "    return planner_preamble\n",
    "\n",
    "def get_checker_preamble():\n",
    "    preamble = \"\"\"\n",
    "You were given a request task/context/query/instruction which specifies a task or set of instructions as below \\n\n",
    "{context}\n",
    "\n",
    "You were also given a set of tools ({tool_names}), which you can use (one or more times) to provide solution to the task/query/instruction/request. \n",
    "For most queries using one or more tools (python classes/functions/library) will help in providing better response to the user. Details about the tools and how to use them is as follows:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Some tools provide `List[str]` or other list types as outputs (i.e multiple outputs) while other tools only take `str` or other non-list-types as inputs. \n",
    "String lengths may exceed handling length if we concatenate them, in which case we summarise them before concatenation.\n",
    "You were requested to come up with a step-by-step plan/strategy on how to answer the query/instruction. \n",
    "Follow these rules for developing this plan.\n",
    "- Think about the request context/query/instruction and see what needs to be done. Is it asking a question, a multi-step question, request for code, request for short/long writing, information, requesting multiple items in same query, or something else.\n",
    "- Your plan must be composed of small atomic actionable steps which can be executed in code using python and the above tools. \n",
    "- Ensure to break the problem down to simple steps in your plan with each step having one goal or single responsibility.\n",
    "- Prefer using given tools in python code, rather than writing simple python code to do things.\n",
    "- You can change the query/request/instruction initially provided when passing it to the tools to better suit the tools input requirement.\n",
    "- In any steps, you might just want to execute python code without uisng any tools. You can just say `Step Action: Python` for that step. \n",
    "- For steps where any tool is not needed (python code is enough), just mention the logic or step in simple english.\n",
    "- If the request can be served by simple language generation, then just generate the language and `print` your output in code.\n",
    "\n",
    "You developed the below plan for this task:\n",
    "\n",
    "{plan}\n",
    "\n",
    "After writing the plan, You also came up with code to implement the plan and solve the context/query/instruction using the below rules:\n",
    "- Plans are generally guidance on how to write code but they are not perfectly correct, you can use the plan as a weak guidance.\n",
    "- Perform exception handling and for cases where we expect exceptions or errors, handle them and create backup methods there.\n",
    "- You can modify the  request/query/instruction before feeding it to any tools in the code itself. You can also modify intermediate results to better suit the tools that you will use next.\n",
    "- Within your code use comments to describe which lines roughly belong to which step in the plan above, and what these lines intend to do.\n",
    "- Print results of important intermediate variables so that we can inspect and debug code easily. when printing a variable, print its name as well.\n",
    "- While writing code think, if this line of code fails what do I do? If this line throws any exception what do I do? Do this for each line of code you write.\n",
    "- In the `code` / `full_corrected_code` / `corrected_code_from_error_point_to_end` fields, just write code, no escape symbols or backslashes, pure python code there only.\n",
    "\n",
    "Your previously written code is given below:\n",
    "\n",
    "{code}\n",
    "\n",
    "This code provided the following stdout on the console:\n",
    "\n",
    "{stdout}\n",
    "\n",
    "We also have possible exception stack trace from this code as below:\n",
    "\n",
    "{error_hint}\n",
    "\n",
    "Results of few intermediate variables of this code is also given:\n",
    "\n",
    "{intermediate_output}\n",
    "\n",
    "Further, Till now we have tried below codes which are given along with the outputs/errors, hypothesized failure reasons/attempted corrections.\n",
    "\n",
    "<code_notes>\n",
    "{code_fails}\n",
    "</code_notes>\n",
    "\n",
    "if <code_notes> is empty then this is our first attempt at correcting this code.\n",
    "\n",
    "{user_hints}\n",
    "\n",
    "\n",
    "Based on these, we need to address the below points and provide output in a python dictionary, \n",
    "1. Did the code and plan succeed `success`, if yes output the expected answer in `output`. (You can skip the rest of the fields if success==True, and just provide output in `output` field).\n",
    "2. If the code failed, then was it a failure of code, due to exception or wrong code usage, in `is_code_failure` and `code_failure_reason`.\n",
    "3. Was it a failure of our plan instead since the plan itself maybe too hard to execute or incorrect or other reasons, in `is_plan_failure` and `plan_failure_reason`. Only one of is_plan_failure , is_code_failure has to be True, if `success` is False. \n",
    "4. List down the suspected errors in simple english.\n",
    "5. List down any mistakes in our assumptions of using the tools or how python code would work, or any other assumptions we made.\n",
    "6. Mention corrective changes in simple language.\n",
    "7. Mention the corrected plan, if plan needs correction, on basis of `is_plan_failure` or `3. Was it a failure of our plan` above.\n",
    "8. For each line of previously written code, think if a particular line had mistake or bad usage or could be updated for better/correct results. Mention these in `line_by_line_errors_in_code`. \n",
    "9. Mention the corrected python code from the point of first error (or where we suspect first correction needs to be made), till the end of code in `corrected_code_from_error_point_to_end`.\n",
    "10. Mention the full corrected code from begginng till end in `full_corrected_code`\n",
    "11. If we could have used any other tool to make our work easier, Suggest the docstring for the new tool in similar manner as old tools.\n",
    "12. Remember for `success` (code and plan to succeed), both the plan/strategy and code needs to correctly solve the query/task given. Code running successfully may still mean that the plan itself was wrong. So check if both plan and code are correctly reaching the goal without any logical or commonsense flaws.\n",
    "\n",
    "Finally, your output should only be a python dictionary (with keys as \"success\", \"output\", \"is_code_failure\", \"code_failure_reason\", \"is_plan_failure\", \"plan_failure_reason\", \"errors_in_simple_english\", \"mistakes_in_assumptions\", \"correction_changes_in_simple_english\", \"corrected_plan\", \"line_by_line_errors_in_code\", \"corrected_code_from_error_point_to_end\", \"full_corrected_code\", \"any_new_tool_that_we_need\") as below:\n",
    "\n",
    "{judge_dict}\n",
    "\n",
    "    \"\"\"\n",
    "    return preamble\n",
    "\n",
    "class PlanAndCode:\n",
    "    def __init__(self, tools):\n",
    "        self.tools = tools\n",
    "        self.tools_info = \" \\n \".join([t.description for t in self.tools])\n",
    "        self.tool_names = \", \".join([t.name for t in self.tools])\n",
    "        self.plan_and_code_dict = \"\"\"\n",
    "{\n",
    "    \"plan_in_simple_english\": <str, plan in simple english>, \n",
    "    \"stepwise_plan_with_toolnames\": <str, plan with tool names>,\n",
    "    \"plan_in_planner_format\": <str, plan in planner format>, \n",
    "    \"thoughts_before_writing_code\": <str, any thoughts before we write code>,\n",
    "    \"code\": <python code>, \n",
    "    \"any_new_tool_that_we_need\": <new tool name and functionality in same format as tools documentation format>,\n",
    "    \n",
    "}\n",
    "\"\"\"\n",
    "        self.prompt = PromptTemplate(\n",
    "            input_variables=[\"context\", \"tools\", \"tool_names\", \"plan_and_code_dict\"],\n",
    "            template=build_preamble(),\n",
    "        )\n",
    "        self.judge_dict = \"\"\"\n",
    "{\n",
    "    \"success\": <bool True/False, if everything succeeded and output is as expected>, \n",
    "    \"output\": <final output if code succeeded>, \n",
    "    \"is_code_failure\": <bool True/False whether the problem was caused due to code failure>, \n",
    "    \"code_failure_reason\":\"possible reason of why code failed\", \n",
    "    \"is_plan_failure\": <bool True/False whether the problem was caused due to plan failure>,\n",
    "    \"plan_failure_reason\": \"why the plan failed and which step possibly failed or is responsible for failure\",\n",
    "    \"errors_in_simple_english\": <str, what errors were made in layman or eli5 terms>, \n",
    "    \"mistakes_in_assumptions\": <str, any mistakes we made in how we assumed our tools or python code would work>,\n",
    "    \"correction_changes_in_simple_english\": <str, corrections we will make to plan or code in layman or eli5 terms>,\n",
    "    \"corrected_plan\": <corrected plan in older plan's format, if plan needed correcting else empty>,\n",
    "    \"line_by_line_errors_in_code\": <list of code lines with suspected error in them>,\n",
    "    \"corrected_code_from_error_point_to_end\": <str, corrected code from first error point in code till end>,\n",
    "    \"full_corrected_code\": <full python code from start to end>, \n",
    "    \"any_new_tool_that_we_need\": <new tool name and functionality in same format as tools documentation format>,\n",
    "}\n",
    "\"\"\"\n",
    "        self.judge_prompt = PromptTemplate(\n",
    "            input_variables=[\"context\", \"plan\", \"tools\", \"tool_names\", \"code\", \"stdout\", \n",
    "                             \"error_hint\", \"intermediate_output\", \"judge_dict\", \"code_fails\", \"user_hints\"],\n",
    "            template=get_checker_preamble())\n",
    "        \n",
    "    \n",
    "    def create_plan_and_code(self, query):\n",
    "        prompt = self.prompt.format(context=query, tool_names=self.tool_names, tools=self.tools_info, plan_and_code_dict=self.plan_and_code_dict)\n",
    "        plan_and_code = callGpt.get_hard_call()(prompt, temperature=0.7)\n",
    "        return plan_and_code\n",
    "    \n",
    "    def check_output_and_correct_plan_and_code(self, query, plan, code, \n",
    "                                               stdout, error_hint, intermediate_output, \n",
    "                                               codes_till_now,  errors_or_outputs_till_now, reasons_till_now, user_hints=\"\"):\n",
    "        code_fails = [f\"{i+1} \\n code: \\n```{c}```\\n error or output: \\n{e}\\n reason: \\n{r}\\n\" for i, (c,e,r) in enumerate(zip(codes_till_now, errors_or_outputs_till_now, reasons_till_now))]\n",
    "        prompt = self.judge_prompt.format(context=query, plan=plan, tools=self.tools_info, tool_names=self.tool_names, stdout=stdout,\n",
    "                                               code=code, error_hint=error_hint, intermediate_output=intermediate_output, \n",
    "                                          judge_dict=self.judge_dict, code_fails=code_fails, user_hints=user_hints)\n",
    "        return callGpt.get_hard_call()(prompt, temperature=0.7)\n",
    "    \n",
    "    def __call__(self, query):\n",
    "        success = False\n",
    "        # TODO: use previous failures and their summary for creating better plan and code each time.\n",
    "        previous_plan_failures = []\n",
    "        previous_code_failures = []\n",
    "        \n",
    "        codes_till_now = [] \n",
    "        errors_or_outputs_till_now = []\n",
    "        reasons_till_now = [] # code and plan failure reasons, mistakes and errors\n",
    "\n",
    "\n",
    "        retries = 0\n",
    "        plan_and_code = self.create_plan_and_code(query)\n",
    "        print(plan_and_code)\n",
    "        print(\"=\"*80)\n",
    "        plan_and_code = eval(plan_and_code)\n",
    "\n",
    "        code = plan_and_code[\"code\"]\n",
    "        plan = plan_and_code[\"plan_in_planner_format\"]\n",
    "        while not success and retries < 2:\n",
    "            retries += 1\n",
    "\n",
    "            outputs = wrapped_exec(code)\n",
    "            print(outputs)\n",
    "            print(\"=\"*80)\n",
    "            intermediate_output = outputs[\"variables_and_values\"]\n",
    "            intermediate_output = {k: (v[:100] if isinstance(v, (str, list, tuple)) else v) for k, v in intermediate_output.items()}\n",
    "            print(intermediate_output)\n",
    "            print(\"=\"*80)\n",
    "            # Verify if we got expected output, if not, is the plan bad or the code bad?\n",
    "            \n",
    "            checker = self.check_output_and_correct_plan_and_code(query, plan, code, \n",
    "                                                                  stdout=outputs[\"output\"], error_hint=outputs[\"error\"], \n",
    "                                                                  intermediate_output=intermediate_output, \n",
    "                                                                  codes_till_now=codes_till_now,  \n",
    "                                                                  errors_or_outputs_till_now=errors_or_outputs_till_now, \n",
    "                                                                  reasons_till_now=reasons_till_now,)\n",
    "            print(checker)\n",
    "            print(\"=\"*80)\n",
    "            checker = eval(checker)\n",
    "            codes_till_now.append(code)\n",
    "            errors_or_outputs_till_now.append(f\"stdout: \\n{outputs['output']}\\n errors: \\n{outputs['error']}\\n\")\n",
    "            cr = copy(checker)\n",
    "            del cr[\"success\"]\n",
    "            del cr[\"output\"]\n",
    "            del cr[\"corrected_plan\"]\n",
    "            del cr[\"full_corrected_code\"]\n",
    "            del cr[\"any_new_tool_that_we_need\"]\n",
    "            del cr[\"corrected_code_from_error_point_to_end\"]\n",
    "            reasons_till_now.append(str(cr))\n",
    "            if checker[\"success\"]:\n",
    "                print(checker[\"output\"])\n",
    "                print(\"=\"*80)\n",
    "                success = True\n",
    "            else:\n",
    "                assert checker[\"is_code_failure\"] or checker[\"is_plan_failure\"]\n",
    "                plan = checker[\"corrected_plan\"] if len(str(checker[\"corrected_plan\"]).strip()) > 10 else plan\n",
    "                code = checker[\"full_corrected_code\"] if len(checker[\"full_corrected_code\"].strip()) > 10 else code\n",
    "                additional_code = checker[\"corrected_code_from_error_point_to_end\"]\n",
    "                print(checker[\"any_new_tool_that_we_need\"])\n",
    "                print(\"=\"*80)\n",
    "        return dict(query=query, plan=plan, code=code, \n",
    "                    stdout=outputs[\"output\"], error_hint=outputs[\"error\"], \n",
    "                    intermediate_output=intermediate_output, codes_till_now=codes_till_now,  \n",
    "                    errors_or_outputs_till_now=errors_or_outputs_till_now, reasons_till_now=reasons_till_now,)\n",
    "            \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e6cc0c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Round robin models [Done]\n",
    "# Make function param names same as documentation names\n",
    "# Lets try executing the code 3 times with 3 corrections before going for step by step approach.\n",
    "# Try to use tools that you haven't used yet.\n",
    "# Use Squad-v2 trained models for contextual answers where few word answers are ok.\n",
    "# Budget aware tool selection.\n",
    "# Tool dropout -> drop some tools out and then try again.\n",
    "# Two ways of improvement exist -> answer->critic->improve or answer->N-answers->rejection-sampling->select-best\n",
    "# Plan-critic-improve, plan-code-critic-improve, plan-code->execute->critic->improve-plan, \n",
    "# plan-code->execute-stepwise->critic->improve-plan\n",
    "# Make tools more easy to use and adapt multiple ways to call the tool.\n",
    "# Stratgy retrieval\n",
    "# Tool names with details of what they can do , when model decides to use a tool it should be able to look it up\n",
    "    # In simple terms, tool + usecase to planner\n",
    "    # Only selected tools from planner + documentation to codegen.\n",
    "    # Allow writing partial code, and request documentation, then can resume writing code from last step.\n",
    "\n",
    "# Provide retrieved code examples for coding part to plantocoder.\n",
    "# Add support for multiple errors and hints and corrections to plantocoder.\n",
    "# Can we use oasst rm model as our verifier?\n",
    "# There is no if-else planning\n",
    "# For it to write python code we only need tool interface details given to it.\n",
    "# Examples of good usage are needed\n",
    "# Ask it what other hypothetical tools it could use.\n",
    "\n",
    "# Check at intermediate steps if we are going in right direction and if plan needs to be changed.\n",
    "# Critic and correct, allow it to run a python repl which helps it understand and successfully run the code.\n",
    "# Ability to iterate over each critic point.\n",
    "# Limit completion length where it is known that answer will be small.\n",
    "# Model should state what it expects after a call.\n",
    "# Planner plan's fully but we also have a step by step runner which can do it in small steps, and then check and verify as well each step's expected output.\n",
    "# Contemplate\n",
    "# Think out of the box tool\n",
    "# fail-step -> replan, succeed -> replan\n",
    "# what if the error can't be corrected, the tools and LLM is simply not capable.\n",
    "# We are not telling the model,  the delta between this correction and previous correction, and the reasoning behind the correction.\n",
    "# Logging and retries -> Maybe Langchain\n",
    "\n",
    "# \"What is the temperature in Bangalore in degree celcius divided by 2\"\n",
    "# \"Is overfitting always a bad thing?\"\n",
    "# \"What is the birth year of nikki minaj squared?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "3cc72f6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T17:48:44.248152Z",
     "start_time": "2023-05-20T17:48:44.241493Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "available_tools = [ContextualAnswer(), FuseInformation(), \n",
    "                ContextualReader(), Search, \n",
    "                GetWebPage(), ChunkText, \n",
    "                TextLengthCheck, \n",
    "                ContextualSummarizer(), Summarizer(), \n",
    "                WikipediaTool, MathTool, ReduceRepeatTool(), \n",
    "                   CallLargeLanguageModelWithInstructionsTool(), \n",
    "                   ExtractInformationTool(), \n",
    "                   DecisionMakerTool()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2542dac7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d67334a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4725bdb9",
   "metadata": {},
   "source": [
    "# Test Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e71d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can you show code to generate gaussian neural field representation for an image in python  and plot the neural field for that image along with image ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "42935a11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T05:00:51.341482Z",
     "start_time": "2023-05-21T04:56:43.388063Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "894c4b6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T05:16:04.590690Z",
     "start_time": "2023-05-21T05:16:04.584439Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'I want to write an essay on a short monk who was starving in a famine. The story should focus on how he survived. Along with that, make the story long, over 10K words atleast.',\n",
       " 'plan': '\\nStep 1:\\n    Calls: once\\n    Reason: Generate the initial story about the short monk who was starving in a famine.\\n    Action: CallLargeLanguageModelWithInstructionsTool\\n    Input: instructions=\"Write a story about a short monk who was starving in a famine. The story should focus on how he survived.\"\\n    Expected Output: story (str, initial story)\\n    \\nStep 2:\\n    Calls: once\\n    Reason: Check if the story is long enough (at least 10,000 words).\\n    Action: TextLengthCheck\\n    Input: Step 1 (story)\\n    Expected Output: is_long_enough (bool, whether the story is at least 10,000 words)\\n    \\nStep 3:\\n    Calls: once\\n    Reason: If the story is not long enough, generate more story or use other strategies to increase the length.\\n    Action: Python (use a loop to call the language model, FuseInformation or other tools as needed)\\n    Input: Step 1 (story), Step 2 (is_long_enough)\\n    Expected Output: final_story (str, story of at least 10,000 words)\\n',\n",
       " 'code': '\\n# Step 1: Generate the initial story\\nstory = CallLargeLanguageModelWithInstructionsTool()(instructions=\"Write a story about a short monk who was starving in a famine. The story should focus on how he survived.\")\\n\\n# Step 2: Check if the story is long enough\\nis_long_enough = TextLengthCheck(text_document=story, threshold=10000)\\n\\n# Step 3: If the story is not long enough, generate more story or use other strategies to increase the length\\nwhile not is_long_enough:\\n    additional_story = CallLargeLanguageModelWithInstructionsTool()(instructions=\"Continue the story about the short monk who was starving in a famine. Focus on how he survived.\")\\n    story = FuseInformation()(context_user_query=\"Combine the stories of the short monk who was starving in a famine\", first_document=story, second_document=additional_story)\\n    is_long_enough = TextLengthCheck(text_document=story, threshold=10000)\\n\\nprint(\"Final story:\")\\nprint(story)\\n',\n",
       " 'stdout': 'Final story:\\nOnce upon a time, in a small village nestled in the mountains, there lived a short monk named Tenzin. Tenzin was known for his kind heart and his unwavering devotion to his faith. However, the village was hit by a severe famine, and Tenzin found himself struggling to survive.\\n\\nDespite his small stature, Tenzin was a hard worker. He spent his days tending to the village\\'s gardens and helping his fellow villagers in any way he could. But as the famine worsened, even the gardens began to wither, and Tenzin found himself growing weaker and weaker.\\n\\nOne day, as Tenzin was walking through the village, he stumbled upon a small patch of wild mushrooms growing in the shade of a nearby tree. Though he knew that some mushrooms could be poisonous, Tenzin was so hungry that he decided to take a chance and try them.\\n\\nTo his surprise, the mushrooms were not only safe to eat but also incredibly delicious. Tenzin quickly gathered as many as he could find and brought them back to his small hut.\\n\\nOver the next few days, Tenzin continued to search for more mushrooms, and soon he had enough to share with his fellow villagers. Though they were hesitant at first, they soon realized that the mushrooms were a safe and nutritious source of food.\\n\\nAs the days passed, Tenzin became known as the \"mushroom monk,\" and his small hut became a gathering place for those in need of food. Though the famine continued to rage on, Tenzin\\'s kindness and resourcefulness helped the village survive.\\n\\nYears later, when the famine finally ended, the villagers looked back on those difficult times and remembered the small monk who had saved them all. Tenzin may have been short in stature, but his heart was as big as the mountains that surrounded their village.\\n',\n",
       " 'error_hint': '',\n",
       " 'intermediate_output': {'story': 'Once upon a time, in a small village nestled in the mountains, there lived a short monk named Tenzin',\n",
       "  'is_long_enough': True},\n",
       " 'codes_till_now': ['\\n# Step 1: Generate the initial story\\nstory = CallLargeLanguageModelWithInstructionsTool()(instructions=\"Write a story about a short monk who was starving in a famine. The story should focus on how he survived.\")\\n\\n# Step 2: Check if the story is long enough\\nis_long_enough = TextLengthCheck(text_document=story, threshold=10000)\\n\\n# Step 3: If the story is not long enough, generate more story or use other strategies to increase the length\\nwhile not is_long_enough:\\n    additional_story = CallLargeLanguageModelWithInstructionsTool()(instructions=\"Continue the story about the short monk who was starving in a famine. Focus on how he survived.\")\\n    story = FuseInformation()(context_user_query=\"Combine the stories of the short monk who was starving in a famine\", first_document=story, second_document=additional_story)\\n    is_long_enough = TextLengthCheck(text_document=story, threshold=10000)\\n\\nprint(\"Final story:\")\\nprint(story)\\n'],\n",
       " 'errors_or_outputs_till_now': ['stdout: \\nFinal story:\\nOnce upon a time, in a small village nestled in the mountains, there lived a short monk named Tenzin. Tenzin was known for his kind heart and his unwavering devotion to his faith. However, the village was hit by a severe famine, and Tenzin found himself struggling to survive.\\n\\nDespite his small stature, Tenzin was a hard worker. He spent his days tending to the village\\'s gardens and helping his fellow villagers in any way he could. But as the famine worsened, even the gardens began to wither, and Tenzin found himself growing weaker and weaker.\\n\\nOne day, as Tenzin was walking through the village, he stumbled upon a small patch of wild mushrooms growing in the shade of a nearby tree. Though he knew that some mushrooms could be poisonous, Tenzin was so hungry that he decided to take a chance and try them.\\n\\nTo his surprise, the mushrooms were not only safe to eat but also incredibly delicious. Tenzin quickly gathered as many as he could find and brought them back to his small hut.\\n\\nOver the next few days, Tenzin continued to search for more mushrooms, and soon he had enough to share with his fellow villagers. Though they were hesitant at first, they soon realized that the mushrooms were a safe and nutritious source of food.\\n\\nAs the days passed, Tenzin became known as the \"mushroom monk,\" and his small hut became a gathering place for those in need of food. Though the famine continued to rage on, Tenzin\\'s kindness and resourcefulness helped the village survive.\\n\\nYears later, when the famine finally ended, the villagers looked back on those difficult times and remembered the small monk who had saved them all. Tenzin may have been short in stature, but his heart was as big as the mountains that surrounded their village.\\n\\n errors: \\n\\n'],\n",
       " 'reasons_till_now': [\"{'is_code_failure': False, 'code_failure_reason': '', 'is_plan_failure': False, 'plan_failure_reason': '', 'errors_in_simple_english': '', 'mistakes_in_assumptions': '', 'correction_changes_in_simple_english': '', 'line_by_line_errors_in_code': []}\"]}"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"I want to write an essay on a short monk who was starving in a famine. The story should focus on how he survived. Along with that, make the story long, over 10K words atleast.\"\n",
    "plan_outs = PlanAndCode(available_tools)(query)\n",
    "plan_outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "b7f2c7e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T05:57:31.071699Z",
     "start_time": "2023-05-21T05:57:31.059700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Step 1: Generate the initial story\\nstory = CallLargeLanguageModelWithInstructionsTool()(instructions=\"Write a story about a short monk who was starving in a famine. The story should focus on how he survived.\")\\n\\n# Step 2: Check if the story is long enough\\nis_long_enough = TextLengthCheck(text_document=story, threshold=10000)\\n\\n# Step 3: If the story is not long enough, generate more story and join the sections using normal python string concatenation. Use Summarizer tool for summaries.\\nstory_summary = Summarizer()(text_document=story)\\nwhile not is_long_enough:\\n    additional_story = CallLargeLanguageModelWithInstructionsTool()(instructions=\"Continue the story about the short monk who was starving in a famine. Focus on how he survived. Previously, {story_summary}.\", data={\"story_summary\": story_summary})\\n    story = story + \" \" + additional_story\\n    story_summary = Summarizer()(text_document=story_summary + \" \" + additional_story)\\n    is_long_enough = TextLengthCheck(text_document=story, threshold=10000)\\n\\nprint(\"Final story:\")\\nprint(story)\\n'"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'{\\n    \"success\": False, \\n    \"output\": \"\", \\n    \"is_code_failure\": True, \\n    \"code_failure_reason\":\"Exceeded the length limitations of the CallLargeLanguageModelWithInstructionsTool and FuseInformation tools\", \\n    \"is_plan_failure\": True,\\n    \"plan_failure_reason\": \"The plan relies on tools that cannot handle long text, and does not consider their limitations\",\\n    \"errors_in_simple_english\": \"Generated a story that exceeded the tool\\'s word limit, and tried to fuse stories without considering the tool\\'s word limit\", \\n    \"mistakes_in_assumptions\": \"Assumed that CallLargeLanguageModelWithInstructionsTool and FuseInformation tools can handle long text, but they have limitations\",\\n    \"correction_changes_in_simple_english\": \"Use a loop to call CallLargeLanguageModelWithInstructionsTool with a summary of the current story and last section, then join the sections using normal python string concatenation. For summary, pass the last summary and the current section to the Summarizer tool.\",\\n    \"corrected_plan\": [\\n        \"Step 1: Generate the initial story\",\\n        \"Step 2: Check if the story is long enough\",\\n        \"Step 3: If the story is not long enough, generate more story and join the sections using normal python string concatenation. Use Summarizer tool for summaries.\"\\n    ],\\n    \"line_by_line_errors_in_code\": [\\n        \"CallLargeLanguageModelWithInstructionsTool() in the loop\",\\n        \"FuseInformation() in the loop\"\\n    ],\\n    \"corrected_code_from_error_point_to_end\": \\'\\'\\'\\nwhile not is_long_enough:\\n    additional_story = CallLargeLanguageModelWithInstructionsTool()(instructions=\"Continue the story about the short monk who was starving in a famine. Focus on how he survived. Previously, {story_summary}.\", data={\"story_summary\": story_summary})\\n    story = story + \" \" + additional_story\\n    story_summary = Summarizer()(text_document=story_summary + \" \" + additional_story)\\n    is_long_enough = TextLengthCheck(text_document=story, threshold=10000)\\n\\'\\'\\',\\n    \"full_corrected_code\": \\'\\'\\'\\n# Step 1: Generate the initial story\\nstory = CallLargeLanguageModelWithInstructionsTool()(instructions=\"Write a story about a short monk who was starving in a famine. The story should focus on how he survived.\")\\n\\n# Step 2: Check if the story is long enough\\nis_long_enough = TextLengthCheck(text_document=story, threshold=10000)\\n\\n# Step 3: If the story is not long enough, generate more story and join the sections using normal python string concatenation. Use Summarizer tool for summaries.\\nstory_summary = Summarizer()(text_document=story)\\nwhile not is_long_enough:\\n    additional_story = CallLargeLanguageModelWithInstructionsTool()(instructions=\"Continue the story about the short monk who was starving in a famine. Focus on how he survived. Previously, {story_summary}.\", data={\"story_summary\": story_summary})\\n    story = story + \" \" + additional_story\\n    story_summary = Summarizer()(text_document=story_summary + \" \" + additional_story)\\n    is_long_enough = TextLengthCheck(text_document=story, threshold=10000)\\n\\nprint(\"Final story:\")\\nprint(story)\\n\\'\\'\\',\\n    \"any_new_tool_that_we_need\": \"\"\\n}'"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# new_plan = PlanAndCode(available_tools).check_output_and_correct_plan_and_code(**plan_outs, user_hints=\"\"\"\n",
    "# Current Code and Plan is wrong, due to wrong use of tools.\n",
    "# CallLargeLanguageModelWithInstructionsTool and FuseInformation tool cannot handle long text, you should use CallLargeLanguageModelWithInstructionsTool by passing a short summary of what happened before, last paragraph written, and the outline and ask it to write next section. \n",
    "# And don't use FuseInformation to join the sections, rather just join the strings in normal python. For summary also remember that the tool can't handle long text, so to summarize, pass the last summary and the current section to the summarize tool, not the entire story till now.\n",
    "# \"\"\")\n",
    "eval(new_plan)['full_corrected_code']\n",
    "new_plan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125d8af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can you show code to generate gaussian neural field representation for an image in python  and plot the neural field for that image along with image ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "64208eb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T15:18:42.529804Z",
     "start_time": "2023-05-20T15:15:24.687790Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"plan_in_simple_english\": \"First, we need to visit the provided webpage using GetWebPage tool and get its content. Then, we'll use the ContextualSummarizer tool to summarize the webpage content based on the given context about large language model training. Finally, we'll return the summarized content.\", \n",
      "    \"stepwise_plan_with_toolnames\": \"1. GetWebPage to get the webpage content. 2. ContextualSummarizer to summarize the content based on the context about large language model training.\",\n",
      "    \"plan_in_planner_format\": \"Step 1:\\n    Calls: once\\n    Reason: To get the content of the provided webpage.\\n    Action: GetWebPage\\n    Input: url='https://johanwind.github.io/2023/03/23/rwkv_overview.html', context='large language model training'\\n    Expected Output: webpage content\\n\\nStep 2:\\n    Calls: once\\n    Reason: To summarize the webpage content based on the given context.\\n    Action: ContextualSummarizer\\n    Input: context_user_query='large language model training', text_document=Step 1\\n    Expected Output: summarized content about large language model training\", \n",
      "    \"thoughts_before_writing_code\": \"We'll need to handle any exceptions that may occur while visiting the webpage or summarizing the content. We should also ensure the summarized content is within the acceptable length.\",\n",
      "\n",
      "    \"code\": '''\n",
      "# Step 1: Get the content of the provided webpage\n",
      "webpage_url = \"https://johanwind.github.io/2023/03/23/rwkv_overview.html\"\n",
      "context = \"large language model training\"\n",
      "try:\n",
      "    webpage_content = GetWebPage()(url=webpage_url, context=context)\n",
      "except Exception as e:\n",
      "    print(f\"Error while getting webpage content: {e}\")\n",
      "    webpage_content = \"\"\n",
      "\n",
      "# Step 2: Summarize the webpage content based on the given context\n",
      "if webpage_content:\n",
      "    try:\n",
      "        summarized_content = ContextualSummarizer()(context_user_query=context, text_document=webpage_content)\n",
      "    except Exception as e:\n",
      "        print(f\"Error while summarizing content: {e}\")\n",
      "        summarized_content = \"\"\n",
      "else:\n",
      "    summarized_content = \"\"\n",
      "\n",
      "# Print the summarized content\n",
      "print(\"Summarized content:\", summarized_content)\n",
      "''',\n",
      "\n",
      "    \"any_new_tool_that_we_need\": \"\"\n",
      "}\n",
      "================================================================================\n",
      "{'output': 'Summarized content: RWKV is a language model that combines RNNs and transformers to handle \"infinite\" context length. It scales linearly with the number of tokens and can handle longer context lengths without requiring more memory. The 14B model uses 50277 different tokens and was trained on 64 A100s in parallel for about 3 months. The cost estimate for training the model is around $100k, but with recent training code improvements, it could be reduced to $40k. For discussions on large language model training, contact johanswi@uio.no or join the Discord community.\\n', 'error': '', 'variables_and_values': {'webpage_url': 'https://johanwind.github.io/2023/03/23/rwkv_overview.html', 'context': 'large language model training', 'webpage_content': 'url: https://johanwind.github.io/2023/03/23/rwkv_overview.html, title: The RWKV language model: An RNN with the advantages of a transformer | The Good Minima, content: RWKV is a language model that combines RNNs and transformers. It can handle \"infinite\" context length and scales linearly with the number of tokens. It can be massively parallelized during training and can handle longer context lengths without requiring more memory. \\n\\nThe 14B RWKV model uses 50277 different tokens and was trained on 64 A100s in parallel, taking about 3 months. The cost estimate for training the model is around $100k, but with recent training code improvements, it could be reduced to $40k. \\n\\nFor large language model training, discussions can be held on Discord, email (johanswi@uio.no), or in the comments below.', 'summarized_content': 'RWKV is a language model that combines RNNs and transformers to handle \"infinite\" context length. It scales linearly with the number of tokens and can handle longer context lengths without requiring more memory. The 14B model uses 50277 different tokens and was trained on 64 A100s in parallel for about 3 months. The cost estimate for training the model is around $100k, but with recent training code improvements, it could be reduced to $40k. For discussions on large language model training, contact johanswi@uio.no or join the Discord community.'}}\n",
      "================================================================================\n",
      "{'webpage_url': 'https://johanwind.github.io/2023/03/23/rwkv_overview.html', 'context': 'large language model training', 'webpage_content': 'url: https://johanwind.github.io/2023/03/23/rwkv_overview.html, title: The RWKV language model: An R', 'summarized_content': 'RWKV is a language model that combines RNNs and transformers to handle \"infinite\" context length. It'}\n",
      "================================================================================\n",
      "{\n",
      "    \"success\": True, \n",
      "    \"output\": \"RWKV is a language model that combines RNNs and transformers to handle \\\"infinite\\\" context length. It scales linearly with the number of tokens and can handle longer context lengths without requiring more memory. The 14B model uses 50277 different tokens and was trained on 64 A100s in parallel for about 3 months. The cost estimate for training the model is around $100k, but with recent training code improvements, it could be reduced to $40k. For discussions on large language model training, contact johanswi@uio.no or join the Discord community.\", \n",
      "    \"is_code_failure\": False, \n",
      "    \"code_failure_reason\":\"\", \n",
      "    \"is_plan_failure\": False,\n",
      "    \"plan_failure_reason\": \"\",\n",
      "    \"errors_in_simple_english\": \"\", \n",
      "    \"mistakes_in_assumptions\": \"\",\n",
      "    \"correction_changes_in_simple_english\": \"\",\n",
      "    \"corrected_plan\": \"\",\n",
      "    \"line_by_line_errors_in_code\": [],\n",
      "    \"corrected_code_from_error_point_to_end\": \"\",\n",
      "    \"full_corrected_code\": \"\",\n",
      "    \"any_new_tool_that_we_need\": \"\",\n",
      "}\n",
      "================================================================================\n",
      "RWKV is a language model that combines RNNs and transformers to handle \"infinite\" context length. It scales linearly with the number of tokens and can handle longer context lengths without requiring more memory. The 14B model uses 50277 different tokens and was trained on 64 A100s in parallel for about 3 months. The cost estimate for training the model is around $100k, but with recent training code improvements, it could be reduced to $40k. For discussions on large language model training, contact johanswi@uio.no or join the Discord community.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "query = \"Can you summarize this webpage and tell me what it means for world of large language model training: https://johanwind.github.io/2023/03/23/rwkv_overview.html\"\n",
    "PlanAndCode(available_tools)(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fbd151",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06292f54",
   "metadata": {},
   "source": [
    "## solved Query\n",
    "\"I want to square the age that the current russian president was when russia invaded ukraine.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "cccfa2e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T14:33:08.889004Z",
     "start_time": "2023-05-20T14:23:49.053644Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"plan_in_simple_english\": \"First, find the age of the current Russian president in 2014 when Russia invaded Ukraine. Then, square that age.\", \n",
      "    \"stepwise_plan_with_toolnames\": \"1. Search for the age of the current Russian president in 2014. 2. Extract the age from the search results. 3. Use MathTool to square the age.\",\n",
      "    \"plan_in_planner_format\": \"\"\"\n",
      "    Step 1:\n",
      "        Calls: once\n",
      "        Reason: To find the age of the current Russian president in 2014 when Russia invaded Ukraine.\n",
      "        Action: Search\n",
      "        Input: \"age of current Russian president in 2014\"\n",
      "        Expected Output: List of webpage URLs\n",
      "        \n",
      "    Step 2:\n",
      "        Calls: times_previous_output\n",
      "        Reason: To extract the age of the current Russian president in 2014 from the search results.\n",
      "        Action: ExtractInformationTool\n",
      "        Input: Step 1\n",
      "        Expected Output: Age (integer)\n",
      "    \n",
      "    Step 3:\n",
      "        Calls: once\n",
      "        Reason: To square the age of the current Russian president in 2014.\n",
      "        Action: MathTool\n",
      "        Input: Step 2, with the format \"age ** 2\"\n",
      "        Expected Output: Squared age (integer)\n",
      "    \"\"\", \n",
      "    \"thoughts_before_writing_code\": \"We can modify the user query to make it more specific, like 'age of the current Russian president in 2014 when Russia invaded Ukraine'. Also, we need to handle cases when we don't find the age information.\",\n",
      "    \"code\": \"\"\"\n",
      "from tqdm import tqdm\n",
      "\n",
      "query = \"age of the current Russian president in 2014 when Russia invaded Ukraine\"\n",
      "query_response = Search(query)\n",
      "\n",
      "age = None\n",
      "for link in tqdm(query_response):\n",
      "    page_stuff = GetWebPage()(link, context=query)\n",
      "    page_stuff = ContextualReader()(query, page_stuff)\n",
      "    extracted_age = ExtractInformationTool()(context_user_query=\"age in 2014\", text_document=page_stuff)\n",
      "    if extracted_age != \"no answer\":\n",
      "        age = int(extracted_age)\n",
      "        break\n",
      "\n",
      "if age is not None:\n",
      "    squared_age = MathTool(num_expr=f\"{age} ** 2\")\n",
      "    print({\"squared_age\": squared_age})\n",
      "else:\n",
      "    print({\"error\": \"Age information not found\"})\n",
      "    \"\"\", \n",
      "    \"any_new_tool_that_we_need\": \"\"\n",
      "}\n",
      "================================================================================\n",
      "{'output': \"{'squared_age': '3844'}\\n\", 'error': '0%|          | 0/4 [00:00<?, ?it/s]\\r 25%|##5       | 1/4 [06:09<18:27, 369.23s/it]\\r 25%|##5       | 1/4 [07:03<21:11, 423.91s/it]', 'variables_and_values': {'query': 'age of the current Russian president in 2014 when Russia invaded Ukraine', 'query_response': ['https://en.wikipedia.org/wiki/Russo-Ukrainian_War', 'https://www.britannica.com/biography/Volodymyr-Zelensky', 'https://www.vox.com/2014/9/3/18088560/ukraine-everything-you-need-to-know', 'https://www.cbc.ca/news/ukraine-2014-euromaidan-1.6756384'], 'age': 62, 'page_stuff': \"The document lacks information on the age of the Russian president during the 2014 invasion of Ukraine. It does mention that Vladimir Putin, who was 62 years old at the time, was in office. The document primarily highlights Volodymyr Zelensky's background, including his early life, career as an entertainer, journey to the presidency, and current leadership of Ukraine.\", 'extracted_age': '62', 'squared_age': '3844'}}\n",
      "================================================================================\n",
      "{'query': 'age of the current Russian president in 2014 when Russia invaded Ukraine', 'query_response': ['https://en.wikipedia.org/wiki/Russo-Ukrainian_War', 'https://www.britannica.com/biography/Volodymyr-Zelensky', 'https://www.vox.com/2014/9/3/18088560/ukraine-everything-you-need-to-know', 'https://www.cbc.ca/news/ukraine-2014-euromaidan-1.6756384'], 'age': 62, 'page_stuff': 'The document lacks information on the age of the Russian president during the 2014 invasion of Ukrai', 'extracted_age': '62', 'squared_age': '3844'}\n",
      "================================================================================\n",
      "{\n",
      "    \"success\": True, \n",
      "    \"output\": {\"squared_age\": 3844}, \n",
      "    \"is_code_failure\": False, \n",
      "    \"code_failure_reason\":\"\", \n",
      "    \"is_plan_failure\": False,\n",
      "    \"plan_failure_reason\": \"\",\n",
      "    \"errors_in_simple_english\": \"\", \n",
      "    \"mistakes_in_assumptions\": \"\",\n",
      "    \"correction_changes_in_simple_english\": \"\",\n",
      "    \"corrected_plan\": \"\",\n",
      "    \"line_by_line_errors_in_code\": [],\n",
      "    \"corrected_code_from_error_point_to_end\": \"\",\n",
      "    \"full_corrected_code\": \"\",\n",
      "    \"any_new_tool_that_we_need\": \"\",\n",
      "}\n",
      "================================================================================\n",
      "{'squared_age': 3844}\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "query = \"I want to square the age that the current russian president was when russia invaded ukraine.\"\n",
    "PlanAndCode(available_tools)(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ed5bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Search for the Russian president's age when Russia invaded Ukraine\n",
    "search_query = \"age of russian president when russia invaded ukraine\"\n",
    "search_results = Search(search_query)\n",
    "\n",
    "# Step 2: Extract the age from the search results\n",
    "age = None\n",
    "for url in search_results:\n",
    "    page_content = GetWebPage()(url, context=search_query)\n",
    "    extracted_age = ExtractInformationTool()(context_user_query=\"age of the Russian president when Russia invaded Ukraine in numbers only\", text_document=page_content)\n",
    "    \n",
    "    if extracted_age != \"no answer\":\n",
    "        age = int(extracted_age)\n",
    "        break\n",
    "\n",
    "if age is None:\n",
    "    print(\"Unable to find the age of the Russian president when Russia invaded Ukraine.\")\n",
    "else:\n",
    "    # Step 3: Use MathTool to square the age\n",
    "    squared_age = MathTool(num_expr=f\"{age} ** 2\")\n",
    "    print(f\"The squared age of the Russian president when Russia invaded Ukraine is: {squared_age}\")\n",
    "\"\"\",\n",
    "    \"any_new_tool_that_we_need\": \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ea49a23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-18T08:47:22.078504Z",
     "start_time": "2023-05-18T08:42:00.562175Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved Plan:\n",
      "\n",
      "1. Search for the current Russian president's birth year and the year Russia invaded Ukraine separately.\n",
      "2. Extract the birth year of the current Russian president from a relevant web page.\n",
      "3. Extract the year Russia invaded Ukraine from a relevant web page.\n",
      "4. Calculate the age of the Russian president when Russia invaded Ukraine using MathTool.\n",
      "5. Square the age using MathTool.\n",
      "\n",
      "Step-by-step plan:\n",
      "\n",
      "Step Number: 1\n",
      "Step Calls: once\n",
      "Step Reason: To find information about the current Russian president's birth year.\n",
      "Step Action: Search\n",
      "Step Input: search_phrase=\"current Russian president birth year\"\n",
      "Step Expected Output: List[str]: web_url_list_president containing URLs related to the search query.\n",
      "\n",
      "Step Number: 2\n",
      "Step Calls: once\n",
      "Step Reason: To find information about the year Russia invaded Ukraine.\n",
      "Step Action: Search\n",
      "Step Input: search_phrase=\"year Russia invaded Ukraine\"\n",
      "Step Expected Output: List[str]: web_url_list_invasion containing URLs related to the search query.\n",
      "\n",
      "Step Number: 3\n",
      "Step Calls: once\n",
      "Step Reason: To extract the birth year of the current Russian president from the web page content.\n",
      "Step Action: GetWebPage\n",
      "Step Input: url=Step 1[0], context=\"birth year of the current Russian president\"\n",
      "Step Expected Output: str: birth_year_content containing the birth year of the current Russian president.\n",
      "\n",
      "Step Number: 4\n",
      "Step Calls: once\n",
      "Step Reason: To extract the year Russia invaded Ukraine from the web page content.\n",
      "Step Action: GetWebPage\n",
      "Step Input: url=Step 2[0], context=\"year Russia invaded Ukraine\"\n",
      "Step Expected Output: str: invasion_year_content containing the year Russia invaded Ukraine.\n",
      "\n",
      "Step Number: 5\n",
      "Step Calls: once\n",
      "Step Reason: To calculate the age of the Russian president when Russia invaded Ukraine.\n",
      "Step Action: MathTool\n",
      "Step Input: num_expr=\"Step 4 - Step 3\"\n",
      "Step Expected Output: int: age_of_president when Russia invaded Ukraine.\n",
      "\n",
      "Step Number: 6\n",
      "Step Calls: once\n",
      "Step Reason: To square the age of the Russian president when Russia invaded Ukraine.\n",
      "Step Action: MathTool\n",
      "Step Input: num_expr=\"Step 5 * Step 5\"\n",
      "Step Expected Output: int: squared_age_of_president when Russia invaded Ukraine.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "query = \"I want to square the age that the current russian president was when russia invaded ukraine.\"\n",
    "plan = Planner(available_tools)(query)\n",
    "print(plan)\n",
    "print(\"=\"*80)\n",
    "\n",
    "# new_plan = Planner(available_tools)._improve_plan(query, plan, \"\"\"\n",
    "# Use web-search to visit few pages about russian presidents, use web-search to get details of russia-ukraine war, then extract relevant details, then use the MathTool, you can also use WikipediaTool, CallLargeLanguageModelWithInstructionsTool (instructing this tool what you need to do)\n",
    "# \"\"\")\n",
    "# print(new_plan)\n",
    "# print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d1012a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-18T08:51:32.692412Z",
     "start_time": "2023-05-18T08:50:22.928099Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Step 1: Search for the current Russian president's birth year\n",
      "search_phrase_president = \"current Russian president birth year\"\n",
      "web_url_list_president = Search(search_phrase=search_phrase_president)\n",
      "\n",
      "# Step 2: Search for the year Russia invaded Ukraine\n",
      "search_phrase_invasion = \"year Russia invaded Ukraine\"\n",
      "web_url_list_invasion = Search(search_phrase=search_phrase_invasion)\n",
      "\n",
      "# Step 3: Extract the birth year of the current Russian president from the web page content\n",
      "birth_year_content = GetWebPage()(url=web_url_list_president[0], context=\"birth year of the current Russian president\")\n",
      "\n",
      "# Step 4: Extract the year Russia invaded Ukraine from the web page content\n",
      "invasion_year_content = GetWebPage()(url=web_url_list_invasion[0], context=\"year Russia invaded Ukraine\")\n",
      "\n",
      "# Step 5: Calculate the age of the Russian president when Russia invaded Ukraine\n",
      "age_of_president_expr = f\"{invasion_year_content} - {birth_year_content}\"\n",
      "age_of_president = MathTool(num_expr=age_of_president_expr)\n",
      "\n",
      "# Step 6: Square the age of the Russian president when Russia invaded Ukraine\n",
      "squared_age_expr = f\"{age_of_president} * {age_of_president}\"\n",
      "squared_age_of_president = MathTool(num_expr=squared_age_expr)\n",
      "\n",
      "# Print the squared age of the Russian president when Russia invaded Ukraine\n",
      "print(\"Squared age of the Russian president when Russia invaded Ukraine:\", squared_age_of_president)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "plantocoded = PlantoCode(available_tools)(query, plan)\n",
    "print(plantocoded)\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "# plantocoded = PlantoCode(available_tools)(query, new_plan)\n",
    "# print(plantocoded)\n",
    "\n",
    "# print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aa753c91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-18T08:53:38.646120Z",
     "start_time": "2023-05-18T08:51:51.483918Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-6:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ahemf/opt/anaconda3/lib/python3.9/threading.py\", line 973, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/var/folders/3j/ztkfg0cn00n8q91tdg5hkf7m0000gr/T/ipykernel_23912/1208049044.py\", line 17, in run\n",
      "  File \"/Users/ahemf/opt/anaconda3/lib/python3.9/asyncio/runners.py\", line 44, in run\n",
      "    return loop.run_until_complete(main)\n",
      "  File \"/Users/ahemf/opt/anaconda3/lib/python3.9/asyncio/base_events.py\", line 647, in run_until_complete\n",
      "    return future.result()\n",
      "  File \"/var/folders/3j/ztkfg0cn00n8q91tdg5hkf7m0000gr/T/ipykernel_23912/2191233018.py\", line 94, in get_url_content\n",
      "  File \"/Users/ahemf/opt/anaconda3/lib/python3.9/site-packages/playwright/async_api/_generated.py\", line 9173, in goto\n",
      "    await self._impl_obj.goto(\n",
      "  File \"/Users/ahemf/opt/anaconda3/lib/python3.9/site-packages/playwright/_impl/_page.py\", line 495, in goto\n",
      "    return await self._main_frame.goto(**locals_to_params(locals()))\n",
      "  File \"/Users/ahemf/opt/anaconda3/lib/python3.9/site-packages/playwright/_impl/_frame.py\", line 147, in goto\n",
      "    await self._channel.send(\"goto\", locals_to_params(locals()))\n",
      "  File \"/Users/ahemf/opt/anaconda3/lib/python3.9/site-packages/playwright/_impl/_connection.py\", line 61, in send\n",
      "    return await self._connection.wrap_api_call(\n",
      "  File \"/Users/ahemf/opt/anaconda3/lib/python3.9/site-packages/playwright/_impl/_connection.py\", line 461, in wrap_api_call\n",
      "    return await cb()\n",
      "  File \"/Users/ahemf/opt/anaconda3/lib/python3.9/site-packages/playwright/_impl/_connection.py\", line 96, in inner_send\n",
      "    result = next(iter(done)).result()\n",
      "playwright._impl._api_types.TimeoutError: Timeout 30000ms exceeded.\n",
      "=========================== logs ===========================\n",
      "navigating to \"https://www.reuters.com/world/europe/events-leading-up-russias-invasion-ukraine-2022-02-28/\", waiting until \"load\"\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url: https://www.reuters.com/world/europe/events-leading-up-russias-invasion-ukraine-2022-02-28/, title: None, content: None\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "LLMMathChain._evaluate(\"\nNone\n\") raised error: unsupported expression type: <class 'NoneType'>. Please try again with a valid numerical expression",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/langchain/chains/llm_math/base.py:80\u001b[0m, in \u001b[0;36mLLMMathChain._evaluate_expression\u001b[0;34m(self, expression)\u001b[0m\n\u001b[1;32m     78\u001b[0m     local_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpi\u001b[39m\u001b[38;5;124m\"\u001b[39m: math\u001b[38;5;241m.\u001b[39mpi, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124me\u001b[39m\u001b[38;5;124m\"\u001b[39m: math\u001b[38;5;241m.\u001b[39me}\n\u001b[1;32m     79\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\n\u001b[0;32m---> 80\u001b[0m         \u001b[43mnumexpr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexpression\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m            \u001b[49m\u001b[43mglobal_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# restrict access to globals\u001b[39;49;00m\n\u001b[1;32m     83\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# add common mathematical functions\u001b[39;49;00m\n\u001b[1;32m     84\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m     )\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/numexpr/necompiler.py:817\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(ex, local_dict, global_dict, out, order, casting, **kwargs)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m expr_key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _names_cache:\n\u001b[0;32m--> 817\u001b[0m     _names_cache[expr_key] \u001b[38;5;241m=\u001b[39m \u001b[43mgetExprNames\u001b[49m\u001b[43m(\u001b[49m\u001b[43mex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    818\u001b[0m names, ex_uses_vml \u001b[38;5;241m=\u001b[39m _names_cache[expr_key]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/numexpr/necompiler.py:704\u001b[0m, in \u001b[0;36mgetExprNames\u001b[0;34m(text, context)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetExprNames\u001b[39m(text, context):\n\u001b[0;32m--> 704\u001b[0m     ex \u001b[38;5;241m=\u001b[39m \u001b[43mstringToExpression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m     ast \u001b[38;5;241m=\u001b[39m expressionToAST(ex)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/numexpr/necompiler.py:293\u001b[0m, in \u001b[0;36mstringToExpression\u001b[0;34m(s, types, context)\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ex, expressions\u001b[38;5;241m.\u001b[39mExpressionNode):\n\u001b[0;32m--> 293\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsupported expression type: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(ex))\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported expression type: <class 'NoneType'>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Step 5: Calculate the age of the Russian president when Russia invaded Ukraine\u001b[39;00m\n\u001b[1;32m     15\u001b[0m age_of_president_expr \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minvasion_year_content\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbirth_year_content\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 16\u001b[0m age_of_president \u001b[38;5;241m=\u001b[39m \u001b[43mMathTool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_expr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mage_of_president_expr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Step 6: Square the age of the Russian president when Russia invaded Ukraine\u001b[39;00m\n\u001b[1;32m     19\u001b[0m squared_age_expr \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mage_of_president\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m * \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mage_of_president\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36mMathTool\u001b[0;34m(num_expr)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;129m@AddAttribute\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMathTool\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;129m@AddAttribute\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124mMathTool:\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mMathTool\u001b[39m(num_expr: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     17\u001b[0m     math_tool \u001b[38;5;241m=\u001b[39m load_tools([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm-math\u001b[39m\u001b[38;5;124m\"\u001b[39m], llm\u001b[38;5;241m=\u001b[39mllm)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmath_tool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_expr\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/langchain/tools/base.py:344\u001b[0m, in \u001b[0;36mTool._run\u001b[0;34m(self, run_manager, *args, **kwargs)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;124;03m\"\"\"Use the tool.\"\"\"\u001b[39;00m\n\u001b[1;32m    342\u001b[0m new_argument_supported \u001b[38;5;241m=\u001b[39m signature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 344\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m new_argument_supported\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    351\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/langchain/chains/base.py:236\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, *args, **kwargs)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supports only one positional argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 236\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_keys[\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks)[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_keys[\u001b[38;5;241m0\u001b[39m]]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/langchain/chains/base.py:140\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    139\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    141\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(inputs, outputs, return_only_outputs)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/langchain/chains/base.py:134\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks)\u001b[0m\n\u001b[1;32m    128\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    129\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    130\u001b[0m     inputs,\n\u001b[1;32m    131\u001b[0m )\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 134\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    137\u001b[0m     )\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    139\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/langchain/chains/llm_math/base.py:149\u001b[0m, in \u001b[0;36mLLMMathChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    143\u001b[0m _run_manager\u001b[38;5;241m.\u001b[39mon_text(inputs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_key])\n\u001b[1;32m    144\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_chain\u001b[38;5;241m.\u001b[39mpredict(\n\u001b[1;32m    145\u001b[0m     question\u001b[38;5;241m=\u001b[39minputs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_key],\n\u001b[1;32m    146\u001b[0m     stop\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```output\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    147\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m_run_manager\u001b[38;5;241m.\u001b[39mget_child(),\n\u001b[1;32m    148\u001b[0m )\n\u001b[0;32m--> 149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_llm_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_run_manager\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/langchain/chains/llm_math/base.py:103\u001b[0m, in \u001b[0;36mLLMMathChain._process_llm_result\u001b[0;34m(self, llm_output, run_manager)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_match:\n\u001b[1;32m    102\u001b[0m     expression \u001b[38;5;241m=\u001b[39m text_match\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 103\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate_expression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpression\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_text(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAnswer: \u001b[39m\u001b[38;5;124m\"\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n\u001b[1;32m    105\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_text(output, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myellow\u001b[39m\u001b[38;5;124m\"\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/langchain/chains/llm_math/base.py:87\u001b[0m, in \u001b[0;36mLLMMathChain._evaluate_expression\u001b[0;34m(self, expression)\u001b[0m\n\u001b[1;32m     79\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\n\u001b[1;32m     80\u001b[0m         numexpr\u001b[38;5;241m.\u001b[39mevaluate(\n\u001b[1;32m     81\u001b[0m             expression\u001b[38;5;241m.\u001b[39mstrip(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     84\u001b[0m         )\n\u001b[1;32m     85\u001b[0m     )\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 87\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     88\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLLMMathChain._evaluate(\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpression\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m) raised error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please try again with a valid numerical expression\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     90\u001b[0m     )\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# Remove any leading and trailing brackets from the output\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m[|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m]$\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, output)\n",
      "\u001b[0;31mValueError\u001b[0m: LLMMathChain._evaluate(\"\nNone\n\") raised error: unsupported expression type: <class 'NoneType'>. Please try again with a valid numerical expression"
     ]
    }
   ],
   "source": [
    "search_phrase_president = \"current Russian president birth year\"\n",
    "web_url_list_president = Search(search_phrase=search_phrase_president)\n",
    "\n",
    "# Step 2: Search for the year Russia invaded Ukraine\n",
    "search_phrase_invasion = \"year Russia invaded Ukraine\"\n",
    "web_url_list_invasion = Search(search_phrase=search_phrase_invasion)\n",
    "\n",
    "# Step 3: Extract the birth year of the current Russian president from the web page content\n",
    "birth_year_content = GetWebPage()(url=web_url_list_president[0], context=\"birth year of the current Russian president\")\n",
    "\n",
    "# Step 4: Extract the year Russia invaded Ukraine from the web page content\n",
    "invasion_year_content = GetWebPage()(url=web_url_list_invasion[0], context=\"year Russia invaded Ukraine\")\n",
    "\n",
    "# Step 5: Calculate the age of the Russian president when Russia invaded Ukraine\n",
    "age_of_president_expr = f\"{invasion_year_content} - {birth_year_content}\"\n",
    "age_of_president = MathTool(num_expr=age_of_president_expr)\n",
    "\n",
    "# Step 6: Square the age of the Russian president when Russia invaded Ukraine\n",
    "squared_age_expr = f\"{age_of_president} * {age_of_president}\"\n",
    "squared_age_of_president = MathTool(num_expr=squared_age_expr)\n",
    "\n",
    "# Print the squared age of the Russian president when Russia invaded Ukraine\n",
    "print(\"Squared age of the Russian president when Russia invaded Ukraine:\", squared_age_of_president)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "369451d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-18T09:41:27.843450Z",
     "start_time": "2023-05-18T09:41:27.810189Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [42]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;18;43m__file__\u001b[39;49m\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "de29faf0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-18T09:27:11.473652Z",
     "start_time": "2023-05-18T09:24:46.957265Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'search_phrase_president': 'current Russian president birth year',\n",
       " 'web_url_list_president': \"['https://en.wikipedia.org/wiki/List_of_presidents_of_Russia', 'https://en.wikipedia.org/wiki/Vladimir_Putin', 'https://en.wikipedia.org/wiki/President_of_Russia', 'https://www.britannica.com/biography/Vladimir-Putin']\",\n",
       " 'search_phrase_invasion': 'year Russia invaded Ukraine',\n",
       " 'web_url_list_invasion': \"['https://www.reuters.com/world/europe/events-leading-up-russias-invasion-ukraine-2022-02-28/', 'https://www.cnn.com/2022/02/24/europe/ukraine-russia-attack-timeline-intl/index.html', 'https://en.wikipedia.org/wiki/Russian_invasion_of_Ukraine', 'https://www.britannica.com/event/2022-Russian-invasion-of-Ukraine']\",\n",
       " 'birth_year_content': \"url: https://en.wikipedia.org/wiki/List_of_presidents_of_Russia, title: List of presidents of Russia - Wikipedia, content: The current Russian president is Vladimir Putin, born in 1952. The office of the president of Russia was introduced in 1991 after a referendum. The president is the head of state and has formal presidency over the State Council and is the commander in chief of the Russian Armed Forces. During the Soviet period, Russia was headed by collective bodies. The office of the President of the Soviet Union was introduced in 1990 during Mikhail Gorbachev's unsuccessful reforms. Gorbachev became the first and last president of the Union. His tenure was marked by legal and political confrontation with Russia and other republics of the USSR, which eventually led to their full independence in late 1991.\",\n",
       " 'invasion_year_content': 'url: https://www.reuters.com/world/europe/events-leading-up-russias-invasion-ukraine-2022-02-28/, title: None, content: None',\n",
       " 'age_of_president_expr': \"url: https://www.reuters.com/world/europe/events-leading-up-russias-invasion-ukraine-2022-02-28/, title: None, content: None - url: https://en.wikipedia.org/wiki/List_of_presidents_of_Russia, title: List of presidents of Russia - Wikipedia, content: The current Russian president is Vladimir Putin, born in 1952. The office of the president of Russia was introduced in 1991 after a referendum. The president is the head of state and has formal presidency over the State Council and is the commander in chief of the Russian Armed Forces. During the Soviet period, Russia was headed by collective bodies. The office of the President of the Soviet Union was introduced in 1990 during Mikhail Gorbachev's unsuccessful reforms. Gorbachev became the first and last president of the Union. His tenure was marked by legal and political confrontation with Russia and other republics of the USSR, which eventually led to their full independence in late 1991.\",\n",
       " 'age_of_president': 'NameError: name age_of_president is not defined',\n",
       " 'squared_age_expr': 'NameError: name squared_age_expr is not defined',\n",
       " 'squared_age_of_president': 'NameError: name squared_age_of_president is not defined'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Step 1: Search for the current Russian president's birth year\n",
      "search_phrase_president = \"current Russian president birth year\"\n",
      "web_url_list_president = Search(search_phrase=search_phrase_president)\n",
      "\n",
      "# Step 2: Search for the year Russia invaded Ukraine\n",
      "search_phrase_invasion = \"year Russia invaded Ukraine\"\n",
      "web_url_list_invasion = Search(search_phrase=search_phrase_invasion)\n",
      "\n",
      "# Step 3: Extract the birth year of the current Russian president from the web page content\n",
      "birth_year_content = GetWebPage()(url=web_url_list_president[1], context=\"birth year of the current Russian president\")\n",
      "birth_year = ExtractInformationTool()(context_user_query=\"year of birth\", text_document=birth_year_content)\n",
      "\n",
      "# Step 4: Extract the year Russia invaded Ukraine from the web page content\n",
      "invasion_year_content = GetWebPage()(url=web_url_list_invasion[0], context=\"year Russia invaded Ukraine\")\n",
      "invasion_year = ExtractInformationTool()(context_user_query=\"year of invasion\", text_document=invasion_year_content)\n",
      "\n",
      "# Step 5: Calculate the age of the Russian president when Russia invaded Ukraine\n",
      "age_of_president_expr = f\"{invasion_year} - {birth_year}\"\n",
      "age_of_president = MathTool(num_expr=age_of_president_expr)\n",
      "\n",
      "# Step 6: Square the age of the Russian president when Russia invaded Ukraine\n",
      "squared_age_expr = f\"{age_of_president} * {age_of_president}\"\n",
      "squared_age_of_president = MathTool(num_expr=squared_age_expr)\n",
      "\n",
      "# Print the squared age of the Russian president when Russia invaded Ukraine\n",
      "print(\"Squared age of the Russian president when Russia invaded Ukraine:\", squared_age_of_president)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "previous_output = get_variable_names_with_values(plantocoded)\n",
    "previous_output\n",
    "plantocoded = PlantoCode(available_tools).make_correction(query, plan, plantocoded, \"\"\"\n",
    "ValueError: LLMMathChain._evaluate(\"\n",
    "None\n",
    "\") raised error: unsupported expression type: <class 'NoneType'>. Please try again with a valid numerical expression\n",
    "\"\"\", previous_output)\n",
    "print(plantocoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "17755672",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-18T09:37:51.337640Z",
     "start_time": "2023-05-18T09:27:56.344914Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-9:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ahemf/opt/anaconda3/lib/python3.9/threading.py\", line 973, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/var/folders/3j/ztkfg0cn00n8q91tdg5hkf7m0000gr/T/ipykernel_23912/1208049044.py\", line 17, in run\n",
      "  File \"/Users/ahemf/opt/anaconda3/lib/python3.9/asyncio/runners.py\", line 44, in run\n",
      "    return loop.run_until_complete(main)\n",
      "  File \"/Users/ahemf/opt/anaconda3/lib/python3.9/asyncio/base_events.py\", line 647, in run_until_complete\n",
      "    return future.result()\n",
      "  File \"/var/folders/3j/ztkfg0cn00n8q91tdg5hkf7m0000gr/T/ipykernel_23912/2191233018.py\", line 94, in get_url_content\n",
      "  File \"/Users/ahemf/opt/anaconda3/lib/python3.9/site-packages/playwright/async_api/_generated.py\", line 9173, in goto\n",
      "    await self._impl_obj.goto(\n",
      "  File \"/Users/ahemf/opt/anaconda3/lib/python3.9/site-packages/playwright/_impl/_page.py\", line 495, in goto\n",
      "    return await self._main_frame.goto(**locals_to_params(locals()))\n",
      "  File \"/Users/ahemf/opt/anaconda3/lib/python3.9/site-packages/playwright/_impl/_frame.py\", line 147, in goto\n",
      "    await self._channel.send(\"goto\", locals_to_params(locals()))\n",
      "  File \"/Users/ahemf/opt/anaconda3/lib/python3.9/site-packages/playwright/_impl/_connection.py\", line 61, in send\n",
      "    return await self._connection.wrap_api_call(\n",
      "  File \"/Users/ahemf/opt/anaconda3/lib/python3.9/site-packages/playwright/_impl/_connection.py\", line 461, in wrap_api_call\n",
      "    return await cb()\n",
      "  File \"/Users/ahemf/opt/anaconda3/lib/python3.9/site-packages/playwright/_impl/_connection.py\", line 96, in inner_send\n",
      "    result = next(iter(done)).result()\n",
      "playwright._impl._api_types.TimeoutError: Timeout 30000ms exceeded.\n",
      "=========================== logs ===========================\n",
      "navigating to \"https://www.reuters.com/world/europe/events-leading-up-russias-invasion-ukraine-2022-02-28/\", waiting until \"load\"\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url: https://www.reuters.com/world/europe/events-leading-up-russias-invasion-ukraine-2022-02-28/, title: None, content: None\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "unknown format from LLM: This is not a valid math problem.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [41]\u001b[0m, in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Step 5: Calculate the age of the Russian president when Russia invaded Ukraine\u001b[39;00m\n\u001b[1;32m     18\u001b[0m age_of_president_expr \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minvasion_year\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbirth_year\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 19\u001b[0m age_of_president \u001b[38;5;241m=\u001b[39m \u001b[43mMathTool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_expr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mage_of_president_expr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Step 6: Square the age of the Russian president when Russia invaded Ukraine\u001b[39;00m\n\u001b[1;32m     22\u001b[0m squared_age_expr \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mage_of_president\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m * \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mage_of_president\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36mMathTool\u001b[0;34m(num_expr)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;129m@AddAttribute\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMathTool\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;129m@AddAttribute\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124mMathTool:\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mMathTool\u001b[39m(num_expr: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     17\u001b[0m     math_tool \u001b[38;5;241m=\u001b[39m load_tools([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm-math\u001b[39m\u001b[38;5;124m\"\u001b[39m], llm\u001b[38;5;241m=\u001b[39mllm)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmath_tool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_expr\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/langchain/tools/base.py:344\u001b[0m, in \u001b[0;36mTool._run\u001b[0;34m(self, run_manager, *args, **kwargs)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;124;03m\"\"\"Use the tool.\"\"\"\u001b[39;00m\n\u001b[1;32m    342\u001b[0m new_argument_supported \u001b[38;5;241m=\u001b[39m signature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 344\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m new_argument_supported\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    351\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/langchain/chains/base.py:236\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, *args, **kwargs)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supports only one positional argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 236\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_keys[\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks)[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_keys[\u001b[38;5;241m0\u001b[39m]]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/langchain/chains/base.py:140\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    139\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    141\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(inputs, outputs, return_only_outputs)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/langchain/chains/base.py:134\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks)\u001b[0m\n\u001b[1;32m    128\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    129\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    130\u001b[0m     inputs,\n\u001b[1;32m    131\u001b[0m )\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 134\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    137\u001b[0m     )\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    139\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/langchain/chains/llm_math/base.py:149\u001b[0m, in \u001b[0;36mLLMMathChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    143\u001b[0m _run_manager\u001b[38;5;241m.\u001b[39mon_text(inputs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_key])\n\u001b[1;32m    144\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_chain\u001b[38;5;241m.\u001b[39mpredict(\n\u001b[1;32m    145\u001b[0m     question\u001b[38;5;241m=\u001b[39minputs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_key],\n\u001b[1;32m    146\u001b[0m     stop\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```output\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    147\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m_run_manager\u001b[38;5;241m.\u001b[39mget_child(),\n\u001b[1;32m    148\u001b[0m )\n\u001b[0;32m--> 149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_llm_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_run_manager\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/langchain/chains/llm_math/base.py:112\u001b[0m, in \u001b[0;36mLLMMathChain._process_llm_result\u001b[0;34m(self, llm_output, run_manager)\u001b[0m\n\u001b[1;32m    110\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m llm_output\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer:\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 112\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munknown format from LLM: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mllm_output\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key: answer}\n",
      "\u001b[0;31mValueError\u001b[0m: unknown format from LLM: This is not a valid math problem."
     ]
    }
   ],
   "source": [
    "# Step 1: Search for the current Russian president's birth year\n",
    "search_phrase_president = \"current Russian president birth year\"\n",
    "web_url_list_president = Search(search_phrase=search_phrase_president)\n",
    "\n",
    "# Step 2: Search for the year Russia invaded Ukraine\n",
    "search_phrase_invasion = \"year Russia invaded Ukraine\"\n",
    "web_url_list_invasion = Search(search_phrase=search_phrase_invasion)\n",
    "\n",
    "# Step 3: Extract the birth year of the current Russian president from the web page content\n",
    "birth_year_content = GetWebPage()(url=web_url_list_president[1], context=\"birth year of the current Russian president\")\n",
    "birth_year = ExtractInformationTool()(context_user_query=\"year of birth\", text_document=birth_year_content)\n",
    "\n",
    "# Step 4: Extract the year Russia invaded Ukraine from the web page content\n",
    "invasion_year_content = GetWebPage()(url=web_url_list_invasion[0], context=\"year Russia invaded Ukraine\")\n",
    "invasion_year = ExtractInformationTool()(context_user_query=\"year of invasion\", text_document=invasion_year_content)\n",
    "\n",
    "# Step 5: Calculate the age of the Russian president when Russia invaded Ukraine\n",
    "age_of_president_expr = f\"{invasion_year} - {birth_year}\"\n",
    "age_of_president = MathTool(num_expr=age_of_president_expr)\n",
    "\n",
    "# Step 6: Square the age of the Russian president when Russia invaded Ukraine\n",
    "squared_age_expr = f\"{age_of_president} * {age_of_president}\"\n",
    "squared_age_of_president = MathTool(num_expr=squared_age_expr)\n",
    "\n",
    "# Print the squared age of the Russian president when Russia invaded Ukraine\n",
    "print(\"Squared age of the Russian president when Russia invaded Ukraine:\", squared_age_of_president)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7f5b8b83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-18T09:19:36.220523Z",
     "start_time": "2023-05-18T09:19:36.185826Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'yyzz' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [35]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43myyzz\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'yyzz' is not defined"
     ]
    }
   ],
   "source": [
    "yyzz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca9eea5",
   "metadata": {},
   "source": [
    "## Solved Query\n",
    "\"What is the temperature in Bangalore in degree celcius divided by 2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "36767585",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T10:21:59.642382Z",
     "start_time": "2023-05-17T10:20:44.064551Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step-by-step plan:\n",
      "1. Use the Search tool to find URLs related to the temperature in Bangalore.\n",
      "2. Use the GetWebPage tool to extract the content of the URLs.\n",
      "3. Use the ContextualReader tool to find the relevant temperature information from the extracted content.\n",
      "4. Use the MathTool to divide the temperature by 2.\n",
      "\n",
      "Enumerated plan:\n",
      "\n",
      "Step Number: 1\n",
      "Step Calls: once\n",
      "Step Reason: To find URLs related to the temperature in Bangalore.\n",
      "Step Action: Search\n",
      "Step Input: {\"search_phrase\": \"temperature in Bangalore\"}\n",
      "Step Expected Output: List of URLs related to the temperature in Bangalore.\n",
      "\n",
      "Step Number: 2\n",
      "Step Calls: times_previous_output\n",
      "Step Reason: To extract the content of the URLs found in step 1.\n",
      "Step Action: GetWebPage\n",
      "Step Input: Step 1\n",
      "Step Expected Output: List of text content from the URLs.\n",
      "\n",
      "Step Number: 3\n",
      "Step Calls: once\n",
      "Step Reason: To find the relevant temperature information from the extracted content.\n",
      "Step Action: ContextualReader\n",
      "Step Input: {\"context_user_query\": \"temperature in Bangalore in degree Celsius\", \"text_document\": Step 2}\n",
      "Step Expected Output: A string containing the temperature in Bangalore in degree Celsius.\n",
      "\n",
      "Step Number: 4\n",
      "Step Calls: once\n",
      "Step Reason: To divide the temperature by 2 as requested by the user.\n",
      "Step Action: MathTool\n",
      "Step Input: {\"num_expr\": \"Step 3 / 2\"}\n",
      "Step Expected Output: A string containing the temperature in Bangalore in degree Celsius divided by 2.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "plan = Planner(available_tools)(\"What is the temperature in Bangalore in degree celcius divided by 2\")\n",
    "print(plan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a722cb8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T10:27:58.645336Z",
     "start_time": "2023-05-17T10:24:30.780515Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "from typing import List\n",
      "from tqdm import tqdm\n",
      "\n",
      "# Step 1: Search for URLs related to temperature in Bangalore\n",
      "search_query = \"temperature in Bangalore\"\n",
      "search_tool = Search()\n",
      "urls = search_tool(search_query)\n",
      "\n",
      "# Step 2: Extract content from the URLs\n",
      "get_web_page_tool = GetWebPage()\n",
      "url_contents: List[str] = []\n",
      "for url in tqdm(urls):\n",
      "    content = get_web_page_tool(url)\n",
      "    url_contents.append(content)\n",
      "\n",
      "# Step 3: Find the relevant temperature information\n",
      "contextual_reader_tool = ContextualReader()\n",
      "context_query = \"temperature in Bangalore in degree Celsius\"\n",
      "temperature_text = \"\"\n",
      "for content in url_contents:\n",
      "    temperature_info = contextual_reader_tool(context_query, content)\n",
      "    if temperature_info:\n",
      "        temperature_text = temperature_info\n",
      "        break\n",
      "\n",
      "# Step 4: Divide the temperature by 2\n",
      "math_tool = MathTool()\n",
      "if temperature_text:\n",
      "    temperature_value = float(temperature_text.split()[0])\n",
      "    divided_temperature = math_tool(f\"{temperature_value} / 2\")\n",
      "    print(divided_temperature)\n",
      "else:\n",
      "    print(\"Temperature information not found.\")\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the temperature in Bangalore in degree celcius divided by 2\"\n",
    "plantocoded = PlantoCode(available_tools)(query, plan)\n",
    "print(plantocoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffcc6d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "48fc7c81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T12:15:42.242656Z",
     "start_time": "2023-05-17T12:13:49.847642Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from typing import List\n",
      "from tqdm import tqdm\n",
      "\n",
      "# Step 1: Search for URLs related to temperature in Bangalore\n",
      "search_query = \"temperature in Bangalore\"\n",
      "search_tool = Search()\n",
      "urls = search_tool(search_query)\n",
      "\n",
      "# Step 2: Extract content from the URLs\n",
      "get_web_page_tool = GetWebPage()\n",
      "url_contents: List[str] = []\n",
      "for url in tqdm(urls):\n",
      "    content = get_web_page_tool(url)\n",
      "    url_contents.append(content)\n",
      "\n",
      "# Step 3: Find the relevant temperature information\n",
      "contextual_reader_tool = ContextualReader()\n",
      "context_query = \"temperature in Bangalore in degree Celsius\"\n",
      "temperature_text = \"\"\n",
      "for content in url_contents:\n",
      "    temperature_info = contextual_reader_tool(context_query, content)\n",
      "    if temperature_info:\n",
      "        temperature_text = temperature_info\n",
      "        break\n",
      "\n",
      "# Step 4: Divide the temperature by 2\n",
      "if temperature_text:\n",
      "    # Modify the query for ContextualReader to provide only the numeric value\n",
      "    modified_query = context_query + \" Only provide the temperature value, nothing else, just a number only.\"\n",
      "    temperature_value = ContextualReader()(modified_query, temperature_text)\n",
      "    divided_temperature = MathTool()(f\"{temperature_value} / 2\")\n",
      "    print(divided_temperature)\n",
      "else:\n",
      "    print(\"Temperature information not found.\")\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the temperature in Bangalore in degree celcius divided by 2\"\n",
    "new_code = PlantoCode(available_tools).make_correction(query, plan, new_code, \"\"\"\n",
    "make more use of our tools and stick to the plan. And write valid python code only.\n",
    "\"\"\")\n",
    "print(new_code)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "e743c024",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T14:01:53.841035Z",
     "start_time": "2023-05-17T13:37:33.242984Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search_query = 'temperature in Bangalore'\n",
      "search_tool = Search()\n",
      "urls = search_tool(search_query)\n",
      "search_query = 'current temperature in Bangalore in degree Celsius'\n",
      "search_tool = Search()\n",
      "urls = search_tool(search_query)\n",
      "url_contents = []\n",
      "for url in urls:\n",
      "    content = get_web_page_tool(url)\n",
      "    url_contents.append(content)\n",
      "get_web_page_tool = GetWebPage()\n",
      "url_contents: List[str] = []\n",
      "for url in tqdm(urls):\n",
      "    content = get_web_page_tool(url)\n",
      "    url_contents.append(content)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                 | 0/4 [00:00<?, ?it/s]Exception in thread Thread-54:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ahemf/opt/anaconda3/lib/python3.9/threading.py\", line 973, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/var/folders/3j/ztkfg0cn00n8q91tdg5hkf7m0000gr/T/ipykernel_1675/1208049044.py\", line 17, in run\n",
      "  File \"/Users/ahemf/opt/anaconda3/lib/python3.9/asyncio/runners.py\", line 44, in run\n",
      "    return loop.run_until_complete(main)\n",
      "  File \"/Users/ahemf/opt/anaconda3/lib/python3.9/asyncio/base_events.py\", line 647, in run_until_complete\n",
      "    return future.result()\n",
      "  File \"/var/folders/3j/ztkfg0cn00n8q91tdg5hkf7m0000gr/T/ipykernel_1675/1949548304.py\", line 167, in get_url_content\n",
      "  File \"/Users/ahemf/opt/anaconda3/lib/python3.9/site-packages/playwright/async_api/_generated.py\", line 9173, in goto\n",
      "    await self._impl_obj.goto(\n",
      "  File \"/Users/ahemf/opt/anaconda3/lib/python3.9/site-packages/playwright/_impl/_page.py\", line 495, in goto\n",
      "    return await self._main_frame.goto(**locals_to_params(locals()))\n",
      "  File \"/Users/ahemf/opt/anaconda3/lib/python3.9/site-packages/playwright/_impl/_frame.py\", line 147, in goto\n",
      "    await self._channel.send(\"goto\", locals_to_params(locals()))\n",
      "  File \"/Users/ahemf/opt/anaconda3/lib/python3.9/site-packages/playwright/_impl/_connection.py\", line 61, in send\n",
      "    return await self._connection.wrap_api_call(\n",
      "  File \"/Users/ahemf/opt/anaconda3/lib/python3.9/site-packages/playwright/_impl/_connection.py\", line 461, in wrap_api_call\n",
      "    return await cb()\n",
      "  File \"/Users/ahemf/opt/anaconda3/lib/python3.9/site-packages/playwright/_impl/_connection.py\", line 96, in inner_send\n",
      "    result = next(iter(done)).result()\n",
      "playwright._impl._api_types.TimeoutError: Timeout 30000ms exceeded.\n",
      "=========================== logs ===========================\n",
      "navigating to \"https://www.timeanddate.com/weather/india/bangalore\", waiting until \"load\"\n",
      "============================================================\n",
      " 25%|██████████████████████████████████████▎                                                                                                                  | 1/4 [00:31<01:35, 31.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url: https://www.timeanddate.com/weather/india/bangalore, title: None, content: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████████████████████████████████████████████████████████████████████████████▌                                                                            | 2/4 [00:42<00:39, 19.59s/it]Exception in thread Thread-56:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ahemf/opt/anaconda3/lib/python3.9/threading.py\", line 973, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/var/folders/3j/ztkfg0cn00n8q91tdg5hkf7m0000gr/T/ipykernel_1675/1208049044.py\", line 17, in run\n",
      "  File \"/Users/ahemf/opt/anaconda3/lib/python3.9/asyncio/runners.py\", line 44, in run\n",
      "    return loop.run_until_complete(main)\n",
      "  File \"/Users/ahemf/opt/anaconda3/lib/python3.9/asyncio/base_events.py\", line 647, in run_until_complete\n",
      "    return future.result()\n",
      "  File \"/var/folders/3j/ztkfg0cn00n8q91tdg5hkf7m0000gr/T/ipykernel_1675/1949548304.py\", line 167, in get_url_content\n",
      "  File \"/Users/ahemf/opt/anaconda3/lib/python3.9/site-packages/playwright/async_api/_generated.py\", line 9173, in goto\n",
      "    await self._impl_obj.goto(\n",
      "  File \"/Users/ahemf/opt/anaconda3/lib/python3.9/site-packages/playwright/_impl/_page.py\", line 495, in goto\n",
      "    return await self._main_frame.goto(**locals_to_params(locals()))\n",
      "  File \"/Users/ahemf/opt/anaconda3/lib/python3.9/site-packages/playwright/_impl/_frame.py\", line 147, in goto\n",
      "    await self._channel.send(\"goto\", locals_to_params(locals()))\n",
      "  File \"/Users/ahemf/opt/anaconda3/lib/python3.9/site-packages/playwright/_impl/_connection.py\", line 61, in send\n",
      "    return await self._connection.wrap_api_call(\n",
      "  File \"/Users/ahemf/opt/anaconda3/lib/python3.9/site-packages/playwright/_impl/_connection.py\", line 461, in wrap_api_call\n",
      "    return await cb()\n",
      "  File \"/Users/ahemf/opt/anaconda3/lib/python3.9/site-packages/playwright/_impl/_connection.py\", line 96, in inner_send\n",
      "    result = next(iter(done)).result()\n",
      "playwright._impl._api_types.TimeoutError: Timeout 30000ms exceeded.\n",
      "=========================== logs ===========================\n",
      "navigating to \"https://www.timeanddate.com/weather/india/bengaluru/climate\", waiting until \"load\"\n",
      "============================================================\n",
      " 75%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                      | 3/4 [01:13<00:24, 24.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url: https://www.timeanddate.com/weather/india/bengaluru/climate, title: None, content: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [04:30<00:00, 67.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url_contents: List[str] = []\n",
      "for url in tqdm(urls):\n",
      "    content = get_web_page_tool(url)\n",
      "    if content and 'Access Denied' not in content:\n",
      "        url_contents.append(content)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [05:11<00:00, 77.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contextual_reader_tool = ContextualReader()\n",
      "context_query = 'temperature in Bangalore in degree Celsius'\n",
      "temperature_text = ''\n",
      "for content in url_contents:\n",
      "    temperature_info = contextual_reader_tool(context_query, content)\n",
      "    if temperature_info:\n",
      "        temperature_text = temperature_info\n",
      "        break\n",
      "modified_query = context_query + ' Only provide the temperature value, nothing else, just a number only.'\n",
      "temperature_value = contextual_reader_tool(modified_query, temperature_text)\n",
      "divided_temperature = MathTool()(f'{temperature_value} / 2')\n",
      "temperature_text = ''\n",
      "for content in url_contents:\n",
      "    temperature_info = contextual_reader_tool(context_query, content)\n",
      "    if temperature_info:\n",
      "        temperature_text = temperature_info\n",
      "        break\n",
      "if temperature_text:\n",
      "    modified_query = context_query + ' Only provide the temperature value, nothing else, just a number only.'\n",
      "    temperature_value = contextual_reader_tool(modified_query, temperature_text)\n",
      "    if temperature_value:\n",
      "        divided_temperature = MathTool()(f'{temperature_value} / 2')\n",
      "        print(divided_temperature)\n",
      "    else:\n",
      "        print('Temperature value not found.')\n",
      "else:\n",
      "    print('Temperature information not found.')\n",
      "Answer: 23.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'code_till_now': \"temperature_text = ''\\nfor content in url_contents:\\n    temperature_info = contextual_reader_tool(context_query, content)\\n    if temperature_info:\\n        temperature_text = temperature_info\\n        break\\nif temperature_text:\\n    modified_query = context_query + ' Only provide the temperature value, nothing else, just a number only.'\\n    temperature_value = contextual_reader_tool(modified_query, temperature_text)\\n    if temperature_value:\\n        divided_temperature = MathTool()(f'{temperature_value} / 2')\\n        print(divided_temperature)\\n    else:\\n        print('Temperature value not found.')\\nelse:\\n    print('Temperature information not found.')\",\n",
       " 'last_step_no': 3,\n",
       " 'current_step_no': 4,\n",
       " 'more_steps_needed': False,\n",
       " 'reasons_till_now': [{'step': 4,\n",
       "   'Output as expected': False,\n",
       "   'Reasoning': 'The output is an error message, which indicates that the temperature_value is None and cannot be used in a mathematical expression.',\n",
       "   'Suspected Code Error': 'The temperature_value might not have been extracted correctly by the ContextualReader tool, resulting in a None value.',\n",
       "   'Correction in words': \"We need to check the ContextualReader tool's extraction of the temperature_value and ensure that it returns a valid numerical value.\",\n",
       "   'Step to go back': 3,\n",
       "   'correction made in this version': None,\n",
       "   'suspected error in previous version': None}],\n",
       " 'codes_till_now': [\"url_contents: List[str] = []\\nfor url in tqdm(urls):\\n    content = get_web_page_tool(url)\\n    if content and 'Access Denied' not in content:\\n        url_contents.append(content)\\ncontextual_reader_tool = ContextualReader()\\ncontext_query = 'temperature in Bangalore in degree Celsius'\\ntemperature_text = ''\\nfor content in url_contents:\\n    temperature_info = contextual_reader_tool(context_query, content)\\n    if temperature_info:\\n        temperature_text = temperature_info\\n        break\\nmodified_query = context_query + ' Only provide the temperature value, nothing else, just a number only.'\\ntemperature_value = contextual_reader_tool(modified_query, temperature_text)\\ndivided_temperature = MathTool()(f'{temperature_value} / 2')\"],\n",
       " 'errors': ['LLMMathChain._evaluate(\"\\nNone\\n\") raised error: unsupported expression type: <class \\'NoneType\\'>. Please try again with a valid numerical expression'],\n",
       " 'current_output_sample': 'Answer: 23.5',\n",
       " 'retried': 1,\n",
       " 'success': True}"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is the temperature in Bangalore in degree celcius divided by 2\"\n",
    "\n",
    "\n",
    "StepbyStepPlanExecutor(tools).forward_from_start_till_output(query, plan, new_code)\n",
    "# TODO: Think of answer enhancement via critic as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd68e853",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the temperature in Bangalore in degree celcius divided by 2\"\n",
    "next_step = StepbyStepPlanExecutor(tools).forward_one_step(query, plan, new_code, \"No steps executed\", 0, \"No output\")\n",
    "next_step = StepbyStepPlanExecutor(tools).forward_one_step(query, plan, new_code, next_step['code_till_now'], \n",
    "                                                           next_step['current_step_no'], next_step['current_output_sample'])\n",
    "next_step = StepbyStepPlanExecutor(tools).forward_one_step(query, plan, new_code, next_step['code_till_now'], \n",
    "                                                           next_step['current_step_no'], next_step['current_output_sample'])\n",
    "next_step = StepbyStepPlanExecutor(tools).forward_one_step(query, plan, new_code, next_step['code_till_now'], \n",
    "                                                           next_step['current_step_no'], next_step['current_output_sample'])\n",
    "next_step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849f2e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to square the age that the current russian president was when russia invaded ukraine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "e5efbeea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-16T14:58:08.631724Z",
     "start_time": "2023-05-16T14:57:56.962174Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"step\": 1, \"code\": \"search_response = Search()(search_phrase)\", \"expectation\": \"List of URLs related to the current temperature in Bangalore.\", \"final_variable\": \"search_response\", \"more_steps_needed\": True}'"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is the temperature in Bangalore in degree celcius divided by 2\"\n",
    "next_step = StepbyStepPlanExecutor(tools).get_next_step_code(query, plan, plantocoded, \"No steps executed\", 0, \"No output\")\n",
    "next_step_code = eval(next_step)[\"code\"]\n",
    "exec(eval(next_step)[\"code\"])\n",
    "output = str(eval(eval(next_step)[\"final_variable\"]))\n",
    "next_step = StepbyStepPlanExecutor(tools).get_next_step_code(query, plan, plantocoded, next_step_code, 1, output)\n",
    "eval(eval(next_step)['final_variable'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "5cb8054b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-16T05:28:50.758499Z",
     "start_time": "2023-05-16T05:28:37.240360Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 2\n",
      "Code:\n",
      "```python\n",
      "from get_web_page_tool import GetWebPage\n",
      "\n",
      "try:\n",
      "    webpage_content = GetWebPage()(most_relevant_url, context=search_phrase)\n",
      "except Exception as e:\n",
      "    print(\"Error while fetching webpage content:\", e)\n",
      "```\n",
      "Expectation: The content of the most relevant webpage containing the current temperature in Bangalore.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the temperature in Bangalore in degree celcius divided by 2\"\n",
    "\n",
    "code_till_now = \"\"\"\n",
    "```python\n",
    "from search_tool import Search\n",
    "\n",
    "search_phrase = \"current temperature in Bangalore\"\n",
    "search_response = Search()(search_phrase)\n",
    "\n",
    "if not search_response:\n",
    "    print(\"No search results found.\")\n",
    "else:\n",
    "    most_relevant_url = search_response[0]\n",
    "```\n",
    "\"\"\"\n",
    "print(StepbyStepPlanExecutor(tools).get_next_step_code(query, plan, plantocoded, code_till_now, 1, 'https://www.accuweather.com/en/in/bengaluru/204108/current-weather/204108'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc60c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(StepbyStepPlanExecutor(tools).get_next_step_code(query, plan, plantocoded, code_till_now, 2, 'https://www.accuweather.com/en/in/bengaluru/204108/current-weather/204108'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "609ec821",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-16T10:25:30.471857Z",
     "start_time": "2023-05-16T10:25:02.214291Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"step\": 2,\n",
      "  \"Output as expected\": False,\n",
      "  \"Reasoning\": \"The output shows that the website is denying access to the requested URL, which means we cannot extract the temperature information from this source.\",\n",
      "  \"Suspected Code Error\": \"The GetWebPage tool might not be able to access the website due to restrictions or the website might require additional authentication.\",\n",
      "  \"Correction in words\": \"We can try using a different source or website to get the temperature information, or we can try using a different tool that can bypass the access restrictions.\",\n",
      "  \"Step to go back\": 1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the temperature in Bangalore in degree celcius divided by 2\"\n",
    "\n",
    "code_till_now = \"\"\"\n",
    "```python\n",
    "from search_tool import Search\n",
    "\n",
    "search_phrase = \"current temperature in Bangalore\"\n",
    "search_response = Search()(search_phrase)\n",
    "\n",
    "if not search_response:\n",
    "    print(\"No search results found.\")\n",
    "else:\n",
    "    most_relevant_url = search_response[0]\n",
    "    \n",
    "try:\n",
    "    webpage_content = GetWebPage()(most_relevant_url, context=search_phrase)\n",
    "except Exception as e:\n",
    "    print(\"Error while fetching webpage content:\", e)\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "most_recent_code = \"\"\"\n",
    "\n",
    "try:\n",
    "    webpage_content = GetWebPage()(most_relevant_url, context=search_phrase)\n",
    "except Exception as e:\n",
    "    print(\"Error while fetching webpage content:\", e)\n",
    "\"\"\"\n",
    "print(StepbyStepPlanExecutor(tools).verify_step_code_output(query, plan, plantocoded, code_till_now, 2, most_recent_code, 'url: https://www.accuweather.com/en/in/bengaluru/204108/current-weather/204108, title: Access Denied, content: The website is denying access to the requested URL. No information can be extracted without proper access.'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "a792556f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-16T11:58:45.905084Z",
     "start_time": "2023-05-16T11:58:14.536039Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"step\": 2, \"Suspected Code Error\": \"The GetWebPage tool might not be able to access the website due to restrictions or the website might require additional authentication.\", \"Correction in words we will make\": \"We can try using a different source or website to get the temperature information, or we can try using a different tool that can bypass the access restrictions.\", \"Step to go back\": 1, \"Corrected Code\": \"from search_tool import Search\\n\\nsearch_phrase = \\\"current temperature in Bangalore\\\"\\nsearch_response = Search()(search_phrase)\\n\\nalternative_url = search_response[1]\\nwebpage_content = GetWebPage()(alternative_url, context=search_phrase)\\n\"}\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the temperature in Bangalore in degree celcius divided by 2\"\n",
    "\n",
    "code_till_now = \"\"\"\n",
    "```python\n",
    "from search_tool import Search\n",
    "\n",
    "search_phrase = \"current temperature in Bangalore\"\n",
    "search_response = Search()(search_phrase)\n",
    "\n",
    "if not search_response:\n",
    "    print(\"No search results found.\")\n",
    "else:\n",
    "    most_relevant_url = search_response[0]\n",
    "    \n",
    "try:\n",
    "    webpage_content = GetWebPage()(most_relevant_url, context=search_phrase)\n",
    "except Exception as e:\n",
    "    print(\"Error while fetching webpage content:\", e)\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "most_recent_code = \"\"\"\n",
    "\n",
    "try:\n",
    "    webpage_content = GetWebPage()(most_relevant_url, context=search_phrase)\n",
    "except Exception as e:\n",
    "    print(\"Error while fetching webpage content:\", e)\n",
    "\"\"\"\n",
    "reasons_till_now=[{\n",
    "  \"step\": 2,\n",
    "  \"Output as expected\": False,\n",
    "  \"Reasoning\": \"The output shows that the website is denying access to the requested URL, which means we cannot extract the temperature information from this source.\",\n",
    "  \"Suspected Code Error\": \"The GetWebPage tool might not be able to access the website due to restrictions or the website might require additional authentication.\",\n",
    "  \"Correction in words\": \"We can try using a different source or website to get the temperature information, or we can try using a different tool that can bypass the access restrictions.\",\n",
    "  \"Step to go back\": 1\n",
    "}]\n",
    "errors = ['url: https://www.accuweather.com/en/in/bengaluru/204108/current-weather/204108, title: Access Denied, content: The website is denying access to the requested URL. No information can be extracted without proper access.']\n",
    "print(StepbyStepPlanExecutor(tools).suggest_corrections(query, plan, plantocoded, 2, [code_till_now], \n",
    "                            errors,\n",
    "                                reasons_till_now,))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "a961d58e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-16T12:05:52.409260Z",
     "start_time": "2023-05-16T12:05:52.403230Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "1+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b014b8",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Old stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "ebc9cfd5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-16T12:05:53.403723Z",
     "start_time": "2023-05-16T12:05:53.329105Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 7, got 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [315]\u001b[0m, in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m             all_code \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m cell[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m all_code\n\u001b[0;32m---> 31\u001b[0m \u001b[43mget_all_code_above\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [315]\u001b[0m, in \u001b[0;36mget_all_code_above\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis function must be run from a cell with `ast_node_interactivity` set to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m _, _, _, _, _, _, source \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39mgetouterframes(inspect\u001b[38;5;241m.\u001b[39mcurrentframe())[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     14\u001b[0m node \u001b[38;5;241m=\u001b[39m ast\u001b[38;5;241m.\u001b[39mparse(source)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Find the first cell in the notebook\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 7, got 6)"
     ]
    }
   ],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import inspect\n",
    "import ast\n",
    "\n",
    "def get_all_code_above():\n",
    "    \"\"\"\n",
    "    Returns all code above the current cell as a string.\n",
    "    \"\"\"\n",
    "    # Get the current cell's AST\n",
    "    current_cell = get_ipython().ast_node_interactivity == 'all'\n",
    "    if not current_cell:\n",
    "        print(\"This function must be run from a cell with `ast_node_interactivity` set to 'all'.\")\n",
    "        return None\n",
    "    _, _, _, _, _, _, source = inspect.getouterframes(inspect.currentframe())[1]\n",
    "    node = ast.parse(source)\n",
    "    \n",
    "    # Find the first cell in the notebook\n",
    "    for i, cell in enumerate(get_ipython().history_manager.get_range()):\n",
    "        if 'cell_type' in cell and cell['cell_type'] == 'code':\n",
    "            first_cell = i\n",
    "            break\n",
    "    \n",
    "    # Get all code above the current cell\n",
    "    all_code = ''\n",
    "    for i in range(first_cell, get_ipython().history_manager.get_tail()):\n",
    "        cell = get_ipython().history_manager.get_range(i, i+1)[0]\n",
    "        if 'cell_type' in cell and cell['cell_type'] == 'code':\n",
    "            all_code += cell['source']\n",
    "    \n",
    "    return all_code\n",
    "\n",
    "get_all_code_above()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "ee8da9c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-16T06:14:51.427951Z",
     "start_time": "2023-05-16T06:14:45.501940Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14]\n"
     ]
    }
   ],
   "source": [
    "search_phrase = \"current temperature in Bangalore\"\n",
    "search_response = Search()(search_phrase)\n",
    "\n",
    "if not search_response:\n",
    "    print(\"No search results found.\")\n",
    "else:\n",
    "    # Select a different URL from the search results\n",
    "    most_relevant_url = search_response[3]\n",
    "\n",
    "try:\n",
    "    webpage_content = GetWebPage()(most_relevant_url, context=search_phrase)\n",
    "except Exception as e:\n",
    "    print(\"Error while fetching webpage content:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "10dafbb7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-16T06:14:51.446809Z",
     "start_time": "2023-05-16T06:14:51.433599Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.accuweather.com/en/in/bengaluru/204108/current-weather/204108',\n",
       " 'https://www.accuweather.com/en/in/bengaluru/204108/weather-forecast/204108',\n",
       " 'https://mausam.imd.gov.in/bengaluru/',\n",
       " 'https://www.accuweather.com/en/in/bengaluru/204108/hourly-weather-forecast/204108']"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'url: https://www.accuweather.com/en/in/bengaluru/204108/hourly-weather-forecast/204108, title: Access Denied, content: The website is denying access to the requested URL. No information can be extracted without proper access.'"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_response\n",
    "webpage_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "77b9a07d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-16T06:07:50.392260Z",
     "start_time": "2023-05-16T06:07:39.360897Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[417]\n"
     ]
    }
   ],
   "source": [
    "search_phrase = \"current temperature in Bangalore site:weather.com\"\n",
    "search_response = Search()(search_phrase)\n",
    "\n",
    "if not search_response:\n",
    "    print(\"No search results found.\")\n",
    "else:\n",
    "    most_relevant_url = search_response[0]\n",
    "\n",
    "try:\n",
    "    webpage_content = GetWebPage()(most_relevant_url, context=search_phrase)\n",
    "except Exception as e:\n",
    "    print(\"Error while fetching webpage content:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "92c9232f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-16T06:07:55.611125Z",
     "start_time": "2023-05-16T06:07:55.604028Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url: https://weather.com/en-IN/weather/tenday/l/e5e5d9e304a310cd9f5387b75551e92f5e8cab07d343b649a100e4096931e227, title: Bangalore, Karnataka 10-Day Weather Forecast - The Weather Channel | Weather.com, content: The current temperature in Bangalore is 34°C.\n"
     ]
    }
   ],
   "source": [
    "print(webpage_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cfdb88",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6cbbf5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "709ead34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T18:14:26.243401Z",
     "start_time": "2023-05-15T18:14:08.610381Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14]\n",
      "I'm sorry, but the provided document does not contain any information about the temperature in Bangalore. I cannot answer your question based on the given context.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the temperature in Bangalore in degree Celsius divided by 2\"\n",
    "search_phrase = \"current temperature in Bangalore in degree Celsius\"\n",
    "urls = SearchQuery()(search_phrase)\n",
    "\n",
    "top_url = urls[0]\n",
    "page_content = GetWebPage()(top_url, context=\"temperature in Bangalore\")\n",
    "\n",
    "contextual_document = ContextualReader()(\"temperature in Bangalore in degree Celsius\", page_content)\n",
    "\n",
    "# Assuming the extracted temperature is a number in string format, e.g., \"30\"\n",
    "divided_temperature = MathTool()(\"{} / 2\".format(contextual_document))\n",
    "\n",
    "answer = ContextualAnswer()(\"temperature in Bangalore in degree Celsius divided by 2\", divided_temperature)\n",
    "print(answer)\n",
    "\n",
    "# \"NameError: name 'Contextual_Reader' is not defined\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "de106f51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T18:06:03.367641Z",
     "start_time": "2023-05-15T18:04:46.932763Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There could be several reasons why the code did not run properly or did not give the desired output:\n",
      "\n",
      "1. The tools (Search, GetWebPage, Contextual_Reader, MathTool, and Contextual_Answer) might not be implemented as functions with the same names as the tool names. Make sure to replace the function calls with the actual tool implementations.\n",
      "\n",
      "2. The Search tool might not return a list of URLs as expected. Ensure that the Search tool returns a list of URLs for the given search phrase.\n",
      "\n",
      "3. The GetWebPage tool might not return the cleaned text content of the web page as expected. Ensure that the GetWebPage tool returns the cleaned text content of the web page for the given URL and context.\n",
      "\n",
      "4. The Contextual_Reader tool might not return the contextual document as expected. Ensure that the Contextual_Reader tool returns the contextual document for the given context_user_query and text_document.\n",
      "\n",
      "5. The MathTool might not return the divided_temperature as expected. Ensure that the MathTool returns the result of the given numeric expression in string format.\n",
      "\n",
      "6. The Contextual_Answer tool might not return the final answer as expected. Ensure that the Contextual_Answer tool returns the answer for the given context_user_query and document.\n",
      "\n",
      "7. There might be an issue with the input data or the format of the input data. Ensure that the input data is in the correct format and is valid for the given tools.\n",
      "\n",
      "8. There might be an issue with the output data or the format of the output data. Ensure that the output data is in the correct format and is valid for the given tools.\n",
      "\n",
      "9. There might be an issue with the tools' execution order. Ensure that the tools are executed in the correct order as specified in the plan.\n",
      "\n",
      "10. There might be an issue with the tools' dependencies or libraries. Ensure that all the required dependencies and libraries are installed and imported correctly.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the temperature in Bangalore in degree celcius divided by 2\"\n",
    "error_hint = \"NameError: name 'Search' is not defined, please use tool names as provided and read their descriptions as well.\"\n",
    "critic = PlantoCode([ContextualAnswer(), FuseInformation(), \n",
    "                ContextualReader(), SearchQuery(), \n",
    "                GetWebPage(), ChunkText(), \n",
    "                TextLengthCheck(), ConditionalExecution(), \n",
    "                ContextualSummariser(), Summarizer(), \n",
    "                          WikipediaTool(), MathTool()]).critic_code(query, plan, plantocoded)\n",
    "print(critic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "c5736492",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T18:08:21.831292Z",
     "start_time": "2023-05-15T18:07:20.662880Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the Python code to execute the given plan for finding the temperature in Bangalore in degree Celsius divided by 2:\n",
      "\n",
      "```python\n",
      "from tqdm import tqdm\n",
      "\n",
      "# Step 1: Search for the current temperature in Bangalore\n",
      "search_phrase = \"current temperature in Bangalore in degree Celsius\"\n",
      "urls = Search({\"search_phrase\": search_phrase})\n",
      "\n",
      "# Step 2: Get the web page content of the top search result\n",
      "url = urls[0]\n",
      "context = \"temperature in Bangalore\"\n",
      "page_content = GetWebPage({\"url\": url, \"context\": context})\n",
      "\n",
      "# Step 3: Extract the temperature information from the web page content\n",
      "context_user_query = \"temperature in Bangalore in degree Celsius\"\n",
      "contextual_document = Contextual_Reader({\"context_user_query\": context_user_query, \"text_document\": page_content})\n",
      "\n",
      "# Step 4: Perform the mathematical operation of dividing the temperature by 2\n",
      "num_expr = f\"{contextual_document} / 2\"\n",
      "divided_temperature = MathTool({\"num_expr\": num_expr})\n",
      "\n",
      "# Step 5: Provide the final output\n",
      "context_user_query = \"temperature in Bangalore in degree Celsius divided by 2\"\n",
      "answer = Contextual_Answer({\"context_user_query\": context_user_query, \"document\": divided_temperature})\n",
      "\n",
      "print(answer)\n",
      "```\n",
      "\n",
      "Please note that you need to replace the function calls with the actual tool implementations. The code above assumes that the tools are implemented as functions with the same names as the tool names.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the temperature in Bangalore in degree celcius divided by 2\"\n",
    "error_hint = \"NameError: name 'Search' is not defined, please use tool names as provided and read their descriptions as well.\"\n",
    "new_code = PlantoCode([ContextualAnswer(), FuseInformation(), \n",
    "                ContextualReader(), SearchQuery(), \n",
    "                GetWebPage(), ChunkText(), \n",
    "                TextLengthCheck(), ConditionalExecution(), \n",
    "                ContextualSummariser(), Summarizer(), \n",
    "                          WikipediaTool(), MathTool()]).make_correction(query, plan, plantocoded, error_hint)\n",
    "print(new_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "7a27f9a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T17:33:18.473406Z",
     "start_time": "2023-05-15T17:33:18.427932Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Search' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [220]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Step 1: Search for the current temperature in Bangalore\u001b[39;00m\n\u001b[1;32m      4\u001b[0m search_phrase \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcurrent temperature in Bangalore in degree Celsius\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m urls \u001b[38;5;241m=\u001b[39m \u001b[43mSearch\u001b[49m(search_phrase)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Step 2: Get the web page content of the top search result\u001b[39;00m\n\u001b[1;32m      8\u001b[0m url \u001b[38;5;241m=\u001b[39m urls[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Search' is not defined"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Step 1: Search for the current temperature in Bangalore\n",
    "search_phrase = \"current temperature in Bangalore in degree Celsius\"\n",
    "urls = Search(search_phrase)\n",
    "\n",
    "# Step 2: Get the web page content of the top search result\n",
    "url = urls[0]\n",
    "context = \"temperature in Bangalore\"\n",
    "page_content = GetWebPage(url, context)\n",
    "\n",
    "# Step 3: Extract the temperature information from the web page content\n",
    "context_user_query = \"temperature in Bangalore in degree Celsius\"\n",
    "contextual_document = Contextual_Reader(context_user_query, page_content)\n",
    "\n",
    "# Step 4: Perform the mathematical operation of dividing the temperature by 2\n",
    "num_expr = f\"{contextual_document} / 2\"\n",
    "divided_temperature = MathTool(num_expr)\n",
    "\n",
    "# Step 5: Provide the final output\n",
    "context_user_query = \"temperature in Bangalore in degree Celsius divided by 2\"\n",
    "answer = Contextual_Answer(context_user_query, divided_temperature)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "090fe8d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T17:00:24.556620Z",
     "start_time": "2023-05-15T16:58:13.901652Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                                 | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2206, 2326, 2324, 2339, 2263, 1986, 1921, 1935, 1883, 1859, 1999, 1887, 1145]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██████████████████████████████████████▎                                                                                                                  | 1/4 [01:13<03:40, 73.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1482]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|████████████████████████████████████████████████████████████████████████████▌                                                                            | 2/4 [01:31<01:21, 40.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1484]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                      | 3/4 [01:47<00:29, 29.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[806]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [01:59<00:00, 29.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 3964\n"
     ]
    }
   ],
   "source": [
    "query = \"Nikki Minaj birth year\"\n",
    "query_response = SearchQuery()(query)\n",
    "\n",
    "full_context = None\n",
    "for link in tqdm(query_response):\n",
    "    page_stuff = GetWebPage()(link, context=query)\n",
    "    page_stuff = ContextualReader()(query, page_stuff)\n",
    "    if full_context is not None:\n",
    "        \n",
    "        if TextLengthCheck()(full_context + page_stuff):\n",
    "            full_context = full_context + \" \\n \" + page_stuff\n",
    "            \n",
    "        else:\n",
    "            # Fuse information of two documents since they are too long to concatenate, shorten them as needed.\n",
    "            page_context = ContextualSummariser()(page_context)\n",
    "            if TextLengthCheck()(full_context + page_stuff):\n",
    "                full_context = FuseInformation()(query, full_context, page_context)\n",
    "            else:\n",
    "                full_context = ContextualSummariser()(full_context)\n",
    "                full_context = FuseInformation()(query, full_context, page_context)\n",
    "    else:\n",
    "        full_context = page_stuff\n",
    "\n",
    "birth_year = ContextualAnswer()(query, full_context)\n",
    "squared_birth_year = MathTool()(f\"{birth_year} ** 2\")\n",
    "print(squared_birth_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "2fae7480",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T17:12:35.684413Z",
     "start_time": "2023-05-15T17:12:29.903755Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1982'"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 3928324\n"
     ]
    }
   ],
   "source": [
    "query = query + \"\\n\\n\" + \" Only provide birth year, nothing else, just a number only.\"\n",
    "birth_year = ContextualAnswer()(query, full_context)\n",
    "squared_birth_year = MathTool()(f\"{birth_year} ** 2\")\n",
    "birth_year\n",
    "print(squared_birth_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "c39b47b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T12:46:23.105154Z",
     "start_time": "2023-05-15T12:42:40.728312Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2315, 75]\n",
      "[5]\n",
      "[1277]\n",
      "[1563]\n",
      "From the second document, we gather the following information:\n",
      "\n",
      "1. Overfitting can occur when a model is too complex, and it captures noise in the training data that doesn't represent the underlying pattern.\n",
      "2. Overfitting can lead to a decrease in the model's ability to generalize to new, unseen data.\n",
      "3. In some cases, overfitting might not be a significant issue if the training data is representative of the entire population and the model's accuracy is high.\n",
      "4. To prevent overfitting, one can use techniques such as regularization, early stopping, and cross-validation.\n",
      "5. It is crucial to find the right balance between underfitting and overfitting to achieve the best possible model performance.\n",
      "6. Ensuring that the training data is representative and unbiased can help mitigate the negative effects of overfitting.\n",
      "7. Data augmentation and feature selection can also be used to address overfitting issues.\n",
      "\n",
      "In summary, overfitting is not always a bad thing, but it can lead to poor generalization on new data if the training data is biased or incomplete. Techniques such as regularization, early stopping, cross-validation, data augmentation, and feature selection can help prevent overfitting and improve model performance.\n"
     ]
    }
   ],
   "source": [
    "query = \"Is overfitting always a bad thing?\"\n",
    "query_response = SearchQuery()(query)\n",
    "\n",
    "full_context = None\n",
    "for link in query_response:\n",
    "    page_content = GetWebPage()(link, context=query)\n",
    "    contextual_document = ContextualReader()(query, page_content)\n",
    "    \n",
    "    if full_context is not None:\n",
    "        if TextLengthCheck()(full_context + contextual_document):\n",
    "            full_context = full_context + \"\\n\\n\" + contextual_document\n",
    "        else:\n",
    "            summarized_document = Summarizer()(contextual_document)\n",
    "            if TextLengthCheck()(full_context + summarized_document):\n",
    "                full_context = FuseInformation()(query, full_context, summarized_document)\n",
    "            else:\n",
    "                full_context = Summarizer()(full_context)\n",
    "                full_context = FuseInformation()(query, full_context, summarized_document)\n",
    "    else:\n",
    "        full_context = contextual_document\n",
    "\n",
    "answer = FuseInformation()(query, full_context, \"\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "d404ac39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T14:13:57.780536Z",
     "start_time": "2023-05-15T14:13:29.419694Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the document and my own knowledge, overfitting is not always a bad thing, but it can lead to poor generalization on new data. In some cases, overfitting can be useful, such as when the goal is to memorize a dataset or when the training data is representative of the entire population. However, it is generally considered undesirable as it leads to poor performance on new data. To prevent overfitting, techniques like regularization, early stopping, and cross-validation can be employed. It is essential to strike a balance between underfitting and overfitting to achieve optimal model performance.\n"
     ]
    }
   ],
   "source": [
    "print(ContextualAnswer()(query, full_context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "879385e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T14:12:58.475343Z",
     "start_time": "2023-05-15T14:12:58.471182Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From the document, we gather the following information:\n",
      "\n",
      "1. Overfitting is not always a bad thing, but it can lead to poor generalization on new data.\n",
      "2. The difference between training and testing scores can indicate overfitting, but it's not definitive.\n",
      "3. Increasing the number of iterations can improve the submission score even if it shows overfitting, as it may still be learning useful information.\n",
      "4. Overfitting can lead to better performance on the training data, which can be useful in some cases.\n",
      "5. Overfitting can help identify outliers or anomalies in the data.\n",
      "6. Overfitting can be a sign that the model is too complex and needs to be simplified.\n",
      "7. Overfitting can be reduced by using regularization techniques or increasing the amount of training data.\n",
      "8. Overfitting can lead to poor performance on new, unseen data, which is usually the main concern.\n",
      "\n",
      "After reviewing the document from the provided URL, I have gathered the following information:\n",
      "\n",
      "1. Overfitting occurs when a model learns the training data too well, capturing noise and patterns that do not generalize well to new, unseen data.\n",
      "2. Overfitting is generally considered undesirable as it leads to poor performance on new data.\n",
      "3. However, in some cases, overfitting can be useful, such as when the goal is to memorize a dataset or when the training data is representative of the entire population.\n",
      "4. Techniques like regularization, early stopping, and cross-validation can help prevent overfitting.\n",
      "5. It is essential to strike a balance between underfitting and overfitting to achieve optimal model performance.\n",
      "\n",
      "Overfitting is not always a bad thing, but can lead to harmful results if the training data is biased or incomplete. To tackle overfitting, one can inspect training data, collect data thoughtfully, augment the training data, and restrict the featureset if necessary.\n",
      "\n",
      "From the document, we gather that overfitting is a problem when the data reality is impossible to fit perfectly, and the model is built with a consideration of something not trained for. The accuracy of the model should be related to the nature of the data reality. In cases where the accuracy of the model is good and there is a good match between training and testing accuracy, overfitting might not be a major issue. To check for any sampling bias, one can perform a K-fold cross-validation with the same hyper-parameters.\n"
     ]
    }
   ],
   "source": [
    "print(full_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c890bb",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6239e9f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T08:39:18.597928Z",
     "start_time": "2023-05-14T08:36:21.745888Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'snippet': 'A <b>large</b> <b>language</b> <b>model</b>, or LLM, is a deep learning algorithm that can recognize, summarize, translate, predict and generate text and other content based on knowledge gained from massive datasets. <b>Large</b> <b>language</b> <b>models</b> are among the most successful applications of transformer <b>models</b>. They aren’t just for teaching AIs human languages, but for ...', 'title': 'What Are Large Language Models Used For? - NVIDIA Blog', 'link': 'https://blogs.nvidia.com/blog/2023/01/26/what-are-large-language-models-used-for/'}, {'snippet': 'A <b>large</b> <b>language</b> <b>model</b> (LLM) is a <b>language</b> <b>model</b> consisting of a neural network with many parameters (typically billions of weights or more), trained on <b>large</b> quantities of unlabeled text <b>using</b> self-supervised learning or semi-supervised learning. LLMs emerged around 2018 and perform well at a wide variety of tasks.', 'title': 'Large language model - Wikipedia', 'link': 'https://en.wikipedia.org/wiki/Large_language_model'}, {'snippet': '<b>Large</b> <b>language</b> <b>models</b> (LLMs) power ChatGPT, and these <b>models</b> are the topic of this post. Before considering LLMs more carefully, we would first like to establish what a <b>language</b> <b>model</b> does. A <b>language</b> <b>model</b> gives a probability distribution of a word being valid in a sequence of words. Essentially, the job of a <b>language</b> <b>model</b> is to predict which word is the best fit in a sentence. Figure 1 provides an example.', 'title': 'An Introduction to Large Language Models: Prompt Engineering and P ...', 'link': 'https://developer.nvidia.com/blog/an-introduction-to-large-language-models-prompt-engineering-and-p-tuning/'}, {'snippet': '<b>Large</b> <b>Language</b> <b>Models</b> (LLMs) are foundational machine learning <b>models</b> that <b>use</b> deep learning algorithms to process and understand natural <b>language</b>. These <b>models</b> are trained on massive amounts of text data to learn patterns and entity relationships in the <b>language</b>. LLMs can perform many types of <b>language</b> tasks, such as translating languages ...', 'title': 'An Introduction to Large Language Models (LLMs) - Analytics Vidhya', 'link': 'https://www.analyticsvidhya.com/blog/2023/03/an-introduction-to-large-language-models-llms/'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                      | 3/4 [01:59<00:40, 40.19s/it]Exception in thread Thread-34:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ahemf/opt/anaconda3/lib/python3.9/threading.py\", line 973, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/var/folders/3j/ztkfg0cn00n8q91tdg5hkf7m0000gr/T/ipykernel_79003/1208049044.py\", line 17, in run\n",
      "  File \"/Users/ahemf/opt/anaconda3/lib/python3.9/asyncio/runners.py\", line 44, in run\n",
      "    return loop.run_until_complete(main)\n",
      "  File \"/Users/ahemf/opt/anaconda3/lib/python3.9/asyncio/base_events.py\", line 647, in run_until_complete\n",
      "    return future.result()\n",
      "  File \"/var/folders/3j/ztkfg0cn00n8q91tdg5hkf7m0000gr/T/ipykernel_79003/912268750.py\", line 72, in get_url_content\n",
      "  File \"/Users/ahemf/opt/anaconda3/lib/python3.9/site-packages/playwright/async_api/_generated.py\", line 9173, in goto\n",
      "    await self._impl_obj.goto(\n",
      "  File \"/Users/ahemf/opt/anaconda3/lib/python3.9/site-packages/playwright/_impl/_page.py\", line 495, in goto\n",
      "    return await self._main_frame.goto(**locals_to_params(locals()))\n",
      "  File \"/Users/ahemf/opt/anaconda3/lib/python3.9/site-packages/playwright/_impl/_frame.py\", line 147, in goto\n",
      "    await self._channel.send(\"goto\", locals_to_params(locals()))\n",
      "  File \"/Users/ahemf/opt/anaconda3/lib/python3.9/site-packages/playwright/_impl/_connection.py\", line 61, in send\n",
      "    return await self._connection.wrap_api_call(\n",
      "  File \"/Users/ahemf/opt/anaconda3/lib/python3.9/site-packages/playwright/_impl/_connection.py\", line 461, in wrap_api_call\n",
      "    return await cb()\n",
      "  File \"/Users/ahemf/opt/anaconda3/lib/python3.9/site-packages/playwright/_impl/_connection.py\", line 96, in inner_send\n",
      "    result = next(iter(done)).result()\n",
      "playwright._impl._api_types.TimeoutError: Timeout 30000ms exceeded.\n",
      "=========================== logs ===========================\n",
      "navigating to \"https://www.analyticsvidhya.com/blog/2023/03/an-introduction-to-large-language-models-llms/\", waiting until \"load\"\n",
      "============================================================\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [02:31<00:00, 37.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url: https://www.analyticsvidhya.com/blog/2023/03/an-introduction-to-large-language-models-llms/, title: None, content: None\n",
      "Large language models (LLMs) should be used when:\n",
      "\n",
      "1. There is a need for natural language processing (NLP) tasks such as language translation, text summarization, question answering, sentiment analysis, and named-entity recognition.\n",
      "2. The task requires a high level of language understanding and reasoning.\n",
      "3. The task requires the ability to generate human-like text.\n",
      "4. The task requires the ability to handle large amounts of data.\n",
      "\n",
      "LLMs are useful in various applications, including chatbots, virtual assistants, content creation, improving search engine results, and speech recognition and synthesis. They can be customized for specific use cases and are capable of recognizing, summarizing, translating, predicting, and generating text based on knowledge gained from massive datasets.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query=\"When to use large language models?\"\n",
    "query_response = SearchQuery()(query)\n",
    "from tqdm import tqdm\n",
    "\n",
    "full_context = None\n",
    "for link in tqdm(query_response):\n",
    "    page_stuff = GetWebPage()(link, context=query)\n",
    "    page_stuff = ContextualReader()(query, page_stuff)\n",
    "    if full_context is not None:\n",
    "        \n",
    "        if TextLengthCheck()(full_context + page_stuff):\n",
    "            full_context = full_context + \" \\n\\n \" + page_stuff\n",
    "            \n",
    "        else:\n",
    "            # Fuse information of two documents since they are too long to concatenate, shorten them as needed.\n",
    "            page_context = ContextualSummariser()(page_context)\n",
    "            if TextLengthCheck()(full_context + page_stuff):\n",
    "                full_context = FuseInformation()(query, full_context, page_context)\n",
    "            else:\n",
    "                full_context = ContextualSummariser()(full_context)\n",
    "                full_context = FuseInformation()(query, full_context, page_context)\n",
    "    else:\n",
    "        full_context = page_stuff\n",
    "\n",
    "answer = ContextualAnswer()(query, full_context)\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "001acb67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T09:40:04.776614Z",
     "start_time": "2023-05-15T09:38:23.937064Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step-by-step plan:\n",
      "1. Use the Search tool to find relevant web pages about overfitting.\n",
      "2. Use the GetWebPage tool to extract the content of the found web pages.\n",
      "3. Use the Contextual_Reader tool to extract relevant information about overfitting from the extracted content.\n",
      "4. If the output from step 3 is too long, use the Fuse_Information tool to shorten the text.\n",
      "5. Use the Contextual_Answer tool to answer the user's question based on the extracted information or shortened text.\n",
      "\n",
      "Enumerated steps:\n",
      "\n",
      "Step Number: 1\n",
      "Step Calls: once\n",
      "Step Reason: To find relevant web pages about overfitting.\n",
      "Step Action: Search\n",
      "Step Input: {\"search_phrase\": \"Is overfitting always a bad thing?\"}\n",
      "Step Output: urls\n",
      "\n",
      "Step Number: 2\n",
      "Step Calls: times_previous_output\n",
      "Step Reason: To extract the content of the found web pages.\n",
      "Step Action: GetWebPage\n",
      "Step Input: {\"url\": \"Step 1\"}\n",
      "Step Output: page_contents\n",
      "\n",
      "Step Number: 3\n",
      "Step Calls: times_previous_output\n",
      "Step Reason: To extract relevant information about overfitting from the extracted content.\n",
      "Step Action: Contextual_Reader\n",
      "Step Input: {\"context_user_query\": \"Is overfitting always a bad thing?\", \"text_document\": \"Step 2\"}\n",
      "Step Output: contextual_documents\n",
      "\n",
      "Step Number: 4\n",
      "Step Calls: once\n",
      "Step Reason: To shorten the text if the output from step 3 is too long.\n",
      "Step Action: Fuse_Information\n",
      "Step Input: {\"context_user_query\": \"Is overfitting always a bad thing?\", \"first_document\": \"Step 3\", \"second_document\": \"\"}\n",
      "Step Output: shortened_contextual_document\n",
      "\n",
      "Step Number: 5\n",
      "Step Calls: once\n",
      "Step Reason: To answer the user's question based on the extracted information or shortened text.\n",
      "Step Action: Contextual_Answer\n",
      "Step Input: {\"context_user_query\": \"Is overfitting always a bad thing?\", \"document\": \"Step 4\"}\n",
      "Step Output: answer\n",
      "\n",
      "New tool suggestion:\n",
      "Tool Name: Text_Length_Checker\n",
      "Tool Description: This tool takes a text document and checks if its length is greater than a specified threshold. If the length is greater than the threshold, it returns True, otherwise, it returns False.\n",
      "Tool Input params: text_document: str, threshold: int\n",
      "Tool Output: is_too_long: bool\n",
      "\n",
      "To execute this tool write in your output\n",
      "Tool Name: Text_Length_Checker\n",
      "Tool Input {\"text_document\": <text_document|replace by actual document>, \"threshold\": <threshold|replace by desired threshold>}\n"
     ]
    }
   ],
   "source": [
    "query = \"Is overfitting always a bad thing?\"\n",
    "plan2 = Planner([ContextualAnswer(), FuseInformation(), ContextualReader(), SearchQuery(), GetWebPage()])(f\"\"\"\n",
    "User query was: {query}\n",
    "FuseInformationor this you developed a step by step strategy to find solution using below tools. The solution was: {plan} \\n\\n\n",
    "Now this plan doesn't take into account, what to do if text output from step 3 is too long, could we have used fuse information after step 3 to shorten the text before Contextual_Answer step?\n",
    "Based on these insights and any other possible failures that you can see come up with a new better improved plan.\n",
    "\n",
    "\"\"\")\n",
    "print(plan2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b6451bcc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T08:59:24.266806Z",
     "start_time": "2023-05-14T08:59:24.222298Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2523"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "len(enc.encode(full_context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2bc3dd5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T09:00:01.892262Z",
     "start_time": "2023-05-14T08:59:33.212010Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large language models (LLMs) should be used when:\n",
      "\n",
      "1. There is a need for natural language processing (NLP) tasks such as language translation, text summarization, question answering, sentiment analysis, named-entity recognition, or part-of-speech tagging.\n",
      "2. The task requires a high level of language understanding and reasoning.\n",
      "3. The task requires the ability to generate human-like text.\n",
      "4. The task requires the ability to handle large amounts of data.\n",
      "\n",
      "LLMs are used in various applications, including chatbots, virtual assistants, content generation, improving search engine results, and analyzing large datasets of text for research or business purposes. They can be customized for specific use cases and are known for their flexibility in generation capabilities and the ability to handle a wide variety of tasks.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "answer = ContextualAnswer()(query, full_context)\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6195c358",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# answer length control and detail level control.\n",
    "# if we have token space then make model generate longer in various steps\n",
    "# Speed up\n",
    "# Use gpt4 mostly for planning and Fusion.\n",
    "\n",
    "# Use hf, use multiple models from openai and parallelise, compromise on quality output, make simpler tasks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "990d69cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T09:01:34.751793Z",
     "start_time": "2023-05-14T09:00:50.111211Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It seems that the document you mentioned was not provided. However, I can still provide some general information about when to use large language models.\n",
      "\n",
      "Large language models, such as OpenAI's GPT-3, are useful in various scenarios, including:\n",
      "\n",
      "1. Natural Language Understanding (NLU): These models can be used to understand and extract information from text, such as sentiment analysis, summarization, and question-answering systems.\n",
      "\n",
      "2. Natural Language Generation (NLG): Large language models can generate human-like text, which can be used for tasks like content creation, chatbot responses, and creative writing prompts.\n",
      "\n",
      "3. Machine Translation: These models can be employed for translating text between different languages with high accuracy.\n",
      "\n",
      "4. Code Generation: Large language models can generate code snippets based on natural language descriptions, which can be helpful for developers.\n",
      "\n",
      "5. Knowledge Retrieval: These models can be used to retrieve specific information from a large corpus of text or answer questions based on the information available in the text.\n",
      "\n",
      "It's important to note that large language models may not always be the best choice for every task. They can be computationally expensive and may require significant resources to run. Smaller models or task-specific models might be more suitable for certain applications. Additionally, large language models may sometimes generate incorrect or biased information, so it's essential to evaluate their output carefully.\n"
     ]
    }
   ],
   "source": [
    "answer = ContextualAnswer()(query, \"\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135e953f",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Code Graveyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b10171c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T16:47:48.176404Z",
     "start_time": "2023-05-22T16:47:45.485422Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    verbose=True, \n",
    "    retriever_mode=\"embedding\"\n",
    ")\n",
    "response = query_engine.query(\"what is pruning?\")\n",
    "print(response)\n",
    "\n",
    "# llama index retriever\n",
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    verbose=True, \n",
    "    retriever_mode=\"embedding\"\n",
    ")\n",
    "response = query_engine.retrieve(\"what is pruning?\")\n",
    "print(response)\n",
    "\n",
    "query_engine = index.as_retriever(\n",
    "    verbose=True, \n",
    "    retriever_mode=\"embedding\"\n",
    ")\n",
    "response = query_engine.retrieve(\"what are their results\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "c972bc3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T14:21:26.319118Z",
     "start_time": "2023-05-20T14:21:26.307401Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_variable_names_with_values(code):\n",
    "    import ast\n",
    "    tree = ast.parse(code)\n",
    "    variables = []\n",
    "    for node in ast.walk(tree):\n",
    "        if isinstance(node, ast.Assign):\n",
    "            for target in node.targets:\n",
    "                if isinstance(target, ast.Name):\n",
    "    #                 print(f\"{target.id}: {ast.dump(target.ctx)}\")\n",
    "                    if ast.dump(target.ctx) == \"Store()\":\n",
    "                        variables.append(f\"{target.id}\")\n",
    "    variable_values = [str(locals()[name] if name in locals() else ( globals()[name] if name in globals() else f\"NameError: name {name} is not defined\")) for name in variables]\n",
    "    return dict(zip(variables, variable_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f479f8fa",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MathTool:\n",
    "    def __init__(self):\n",
    "        self.math_tool = load_tools([\"llm-math\"], llm=llm)[0]\n",
    "        self.name = \"MathTool\"\n",
    "        self.description = \"\"\"\n",
    "Tool Name: MathTool\n",
    "Tool Description: This tool takes a numeric expression as a string and provides the output for it in the format 'Answer: <answer>'\n",
    "Tool Input params: num_expr: str\n",
    "Tool Output: answer: str\n",
    "\n",
    "To execute this tool write in your output\n",
    "Tool Name: MathTool\n",
    "Tool Input {\"num_expr\": <numeric expression in string type>}\n",
    "    \"\"\"\n",
    "    def __call__(self, num_expr):\n",
    "        return self.math_tool._run(num_expr).replace(\"Answer: \", \"\")\n",
    "    \n",
    "class WikipediaTool:\n",
    "    def __init__(self,):\n",
    "        self.tool = load_tools([\"wikipedia\"], llm=llm)[0]\n",
    "        self.name = \"WikipediaTool\"\n",
    "        self.description = \"\"\"\n",
    "Tool Name: WikipediaTool\n",
    "Tool Description: This tool takes a phrase or key words and searches them over wikipedia, returns results from wikipedia as a str.\n",
    "Tool Input params: search_phrase: str\n",
    "Tool Output: answer: str\n",
    "\n",
    "To execute this tool write in your output\n",
    "Tool Name: WikipediaTool\n",
    "Tool Input {\"search_phrase\": <text to search over wikipedia>}\n",
    "    \"\"\"\n",
    "    def __call__(self, search_phrase):\n",
    "        return self.tool._run(search_phrase)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749dccfc",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class ChunkText:\n",
    "    def __init__(self, chunk_size=3400, chunk_overlap=100):\n",
    "        self.name = \"ChunkText\"\n",
    "        self.description = \"\"\"    \n",
    "\n",
    "Tool Name: ChunkText\n",
    "Tool Description: This tool takes a text document and chunks it into given chunk size lengths, then returns a list of strings as chunked sub-documents\n",
    "Tool Input params: text_document: str\n",
    "Tool Output: text_chunks: List[str]\n",
    "\n",
    "To execute this tool write in your output\n",
    "Tool Name: ChunkText\n",
    "Tool Input {\"text_document\": <text_document|replace by actual document>}\n",
    "\n",
    "        \"\"\"\n",
    "        self.text_splitter = TokenTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    \n",
    "    def __call__(self, text_document):\n",
    "        return self.text_splitter.split_text(text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12896168",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class TextLengthCheck:\n",
    "    def __init__(self, max_permitted_length=3400):\n",
    "        self.name = \"TextLengthCheck\"\n",
    "        self.description = \"\"\"\n",
    "Tool Name: TextLengthCheck\n",
    "Tool Description: This tool takes a text document and checks if its length is greater than a specified threshold. If the length is greater than the threshold, it returns True, otherwise, it returns False.\n",
    "Tool Input params: text_document: str\n",
    "Tool Output: is_too_long: bool\n",
    "\n",
    "To execute this tool write in your output\n",
    "Tool Name: TextLengthCheck\n",
    "Tool Input {\"text_document\": <text_document|replace by actual document>}\n",
    "    \"\"\"\n",
    "        self.max_permitted_length = max_permitted_length\n",
    "        \n",
    "        self.enc = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "    def __call__(self, text_document):\n",
    "        return len(self.enc.encode(text_document)) < self.max_permitted_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57146b86",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size):\n",
    "    words = text.split()\n",
    "    for i in range(0, len(words), chunk_size):\n",
    "        yield ' '.join(words[i:i+chunk_size])\n",
    "        \n",
    "\n",
    "def chunk_text(text, chunk_size):\n",
    "    # Split the text by spaces, newlines, and HTML tags\n",
    "    chunks = re.split(r'( |\\n|<[^>]+>)', text)\n",
    "    \n",
    "    # Group the chunks into larger chunks of size chunk_size\n",
    "    for i in range(0, len(chunks), chunk_size):\n",
    "        yield ''.join(chunks[i:i+chunk_size])\n",
    "\n",
    "def process_text(text, chunk_size, my_function):\n",
    "    # Split the text into chunks\n",
    "    chunks = list(chunk_text_langchain(text, chunk_size))\n",
    "\n",
    "    # Create a ThreadPoolExecutor\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:\n",
    "        # Use the executor to apply my_function to each chunk\n",
    "        futures = [executor.submit(my_function, chunk) for chunk in chunks]\n",
    "\n",
    "    # Get the results from the futures\n",
    "    results = [future.result() for future in futures]\n",
    "\n",
    "    # Concatenate the results\n",
    "    \n",
    "    # TODO: what if after concatenation the returned result is too long.\n",
    "    return ' '.join(results)\n",
    "\n",
    "\n",
    "def process_text_round_robin(text, chunk_size, my_function_rr):\n",
    "    # Split the text into chunks\n",
    "    my_function = lambda x: next(my_function_rr)(x)\n",
    "    chunks = list(chunk_text_langchain(text, chunk_size))\n",
    "#     print([len(c.split()) for c in chunks])\n",
    "#     # Create a ThreadPoolExecutor\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        # Use the executor to apply my_function to each chunk\n",
    "        futures = [executor.submit(my_function, chunk) for chunk in chunks]\n",
    "\n",
    "    # Get the results from the futures\n",
    "    results = [future.result() for future in futures]\n",
    "    tlc = TextLengthCheck()\n",
    "    summariser = Summarizer()\n",
    "    while len(results) > 1:\n",
    "        results = [r if tlc(r) else summariser(r) for r in results]\n",
    "        results = combine_array_two_at_a_time(results)\n",
    "    results = [r if tlc(r) else summariser(r) for r in results]\n",
    "    # Concatenate the results\n",
    "    # TODO: what if after concatenation the returned result is too long.\n",
    "    \n",
    "    return ' '.join(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d03b11",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class ChunkText:\n",
    "    def __init__(self, chunk_size=3400, chunk_overlap=100):\n",
    "        self.name = \"ChunkText\"\n",
    "        self.description = \"\"\"    \n",
    "\n",
    "Tool Name: ChunkText\n",
    "Tool Description: This tool takes a text document and chunks it into given chunk size lengths, then returns a list of strings as chunked sub-documents\n",
    "Tool Input params: text_document: str\n",
    "Tool Output: text_chunks: List[str]\n",
    "\n",
    "To execute this tool write in your output\n",
    "Tool Name: ChunkText\n",
    "Tool Input {\"text_document\": <text_document|replace by actual document>}\n",
    "\n",
    "        \"\"\"\n",
    "        self.text_splitter = TokenTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    \n",
    "    def __call__(self, text_document):\n",
    "        return self.text_splitter.split_text(text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684c2b2a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Search:\n",
    "    def __init__(self, k=5):\n",
    "        self.k = k\n",
    "        self.search = BingSearchAPIWrapper()\n",
    "        self.name = \"Search\"\n",
    "        self.description = \"\"\"\n",
    "\n",
    "Tool Name: Search\n",
    "Tool Description: This tool takes a search phrase, performs search over a web search engine and returns a list of urls for the search\n",
    "Tool Input params: search_phrase: str\n",
    "Tool Output: urls: List[str]\n",
    "\n",
    "To execute this tool write in your output\n",
    "Tool Name: Search\n",
    "Tool Input {\"search_phrase\": <search_phrase|replace by actual phrase>}\n",
    "    \"\"\"\n",
    "    def __call__(self, search_phrase):\n",
    "        return [r[\"link\"] for r in self.search.results(query, self.k)]\n",
    "    def _run(self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None) -> str:\n",
    "        \"\"\"Use the tool.\"\"\"\n",
    "        return self.__call__(query)\n",
    "    async def _arun(self, query: str, run_manager: Optional[AsyncCallbackManagerForToolRun] = None) -> str:\n",
    "        \"\"\"Use the tool asynchronously.\"\"\"\n",
    "        raise NotImplementedError(\"custom_search does not support async\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dddda36",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "    \n",
    "class ConditionalExecution:\n",
    "    def __init__(self,):\n",
    "        self.name = \"ConditionalExecution\"\n",
    "        self.description = \"\"\"\n",
    "Tool Name: ConditionalExecution\n",
    "Tool Description: This tool conditionally executes one of the two tools given to it. Useful when you need to conditionally execute a tool instead of always executing it.\n",
    "Tool Input params: condition: bool, tool_to_execute_if_true: Tool, tool_to_execute_if_false: Tool, true_tool_inputs: Any, false_tool_inputs: Any\n",
    "Tool Output: answer: Any\n",
    "\n",
    "To execute this tool write in your output\n",
    "Tool Name: ConditionalExecution\n",
    "Tool Input {\"condition\": <actual true or false condition or an expression to get the condition>, \"tool_to_execute_if_true\": <Tool>, \"tool_to_execute_if_false\": <Tool>, \"true_tool_inputs\": <input to true tool (true tool was first param)>, \"false_tool_inputs\": <input to false tool (false tool was second param)>}\n",
    "    \"\"\"\n",
    "    def __call__(self, condition, tool_to_execute_if_true, tool_to_execute_if_false, true_tool_inputs, false_tool_inputs):\n",
    "        if condition:\n",
    "            return tool_to_execute_if_true(true_tool_inputs)\n",
    "        else:\n",
    "            return tool_to_execute_if_false(false_tool_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5301c81b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class CleanerHF:\n",
    "    def __init__(self, model=\"gpt-3.5-turbo\", prompt=None, context=None):\n",
    "        self.instruction = \"\"\"\n",
    "You are given text from a document and a user query or instruction. \n",
    "Your goal is to extract relevant content from the document that can help answer the user query.\n",
    "Document metadata and user query / instructions given as follows: \n",
    "        \"\"\" if prompt is None else prompt\n",
    "        self.clean_now_follows = \"\\nActual given document follows: \\n\"\n",
    "        self.prompt = (self.instruction + \" \" + (context if context is not None else \"\") + \" \" + self.clean_now_follows) if prompt is None else prompt\n",
    "        self.model = model\n",
    "        self.models = [\"text-davinci-003\", \"gpt-3.5-turbo\", \"gpt-3.5-turbo-0301\", \"text-davinci-002\"]\n",
    "        \n",
    "    def clean_one(self, string, model=None):\n",
    "        return hf_query_flan(self.prompt + string)\n",
    "    \n",
    "\n",
    "    def clean_one_with_exception(self, string, model=None):\n",
    "        try:\n",
    "            cleaned_text = self.clean_one(string)\n",
    "            return cleaned_text\n",
    "        except Exception as e:\n",
    "            exp_str = str(e)\n",
    "            too_long = \"maximum context length\" in exp_str and \"your messages resulted in\" in exp_str\n",
    "            if too_long:\n",
    "                return \" \".join([self.clean_one_with_exception(st) for st in split_text(string)])\n",
    "            raise e\n",
    "            \n",
    "    def __call__(self, string, chunk_size=768):\n",
    "        from functools import partial\n",
    "        return process_text(string, chunk_size, self.clean_one_with_exception)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
